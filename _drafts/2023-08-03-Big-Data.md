---
layout: page
title: "Big Data Concepts"
date: 2023-08-02
tags: Azure
key: "AZ0208" 
comment: true
mathjax: true
--- 
## What is Big Data?
* data that contains greater variety arriving in increasing volumes and with more velocity
* These data sets are so voluminous that traditional data processing software just can’t manage them
    * **Volume**: The amount of data matters. With big data, you’ll have to process high volumes of low-density, unstructured data. This can be data of unknown value, such as Twitter data feeds, clickstreams on a web page or a mobile app, or sensor-enabled equipment. For some organizations, this might be tens of terabytes of data. For others, it may be hundreds of petabytes.
    * **Velocity**: Velocity is the fast rate at which data is received and (perhaps) acted on. Normally, the highest velocity of data streams directly into memory versus being written to disk. Some internet-enabled smart products operate in real time or near real time and will require real-time evaluation and action.
    * **Variety** Variety refers to the many types of data that are available. Traditional data types were **structured** and fit neatly in a **relational database**. With the rise of big data, data comes in new unstructured data types. Unstructured and semistructured data types, such as text, audio, and video, require additional preprocessing to derive meaning and support metadata.
* Also, two more Vs: **Value and Veracity (truth)**
    * Derive value by analyzing the data with right questions, finding patterns
    * A large part of the **value** they (big companies) offer comes from their data, which they’re constantly analyzing to produce more efficiency and develop new products.
* Technological breakthroughs have exponentially reduced the cost of data storage and compute
* Techniques and tools:
    * **MapReduce and Spark** are parallel programming framework that uses 100s and 1000s of CPUs effectively
    * Both make use of divide and conquer strategy
    * Use sorting techniques like quick sort (small scale), merge sort (large scale).**Hashing == Sorting**. For a small scale, use a hash table and for a large scale use a partitioned hash table. Sorting and hashing are dual!
* **The Process**:
    * **Integrate**: Extract, transform,Load (ETL) is not sufficient for a large scale. Remains challenging to handle for terabytes/petabytes of data. During integration, you need to bring in the data, process it, and make sure it’s formatted and available in a form that your business analysts can get started with.
    * **Manage**: Where do you store? how do you store? We have separate management systems in place.
    * **Analysze**: Visualize, Statistical tools, modeling using ML and DL.

## Data Sources
    * Based on source, the format changes
* Data Base Backends (RDBMS, NoSQL)
* API endpoints (RESTAPI) (Ex Format:JSON)
* Messages/Streams (item by item)
* Batches

## Data Management Systems
* practice of collecting, keeping, and using data securely, efficiently, and cost-effectively
* data management platforms and can include databases, data lakes and data warehouses, big data management systems, data analytics, and more.
### DO I need Data Lakes or Data warehouses?
* Organizations use both data lakes and data warehouses for large volumes of data from various sources.
* **Data Lakes**: 
    * store an abundance of disparate, unfiltered data to be used later for a particular purpose. 
    * Data from line-of-business applications, mobile apps, social media, IoT devices, and more is captured as raw data in a data lake. The structure, integrity, selection, and format of the various datasets is derived at the time of analysis by the person doing the analysis.
    *  When organizations need **low-cost storage** for unformatted, unstructured data from multiple sources that they intend to **use** for some purpose **in the future**, a data lake might be the right choice.
* **Data warehouses**
    * are specifically intended to **analyze data**. Analytical processing within a data warehouse is performed on data that has been readied for analysis—gathered, contextualized, and transformed—with the purpose of generating analysis-based insights. 
    * Data warehouses are also adept at handling large quantities of data from various sources. 
    * When organizations need advanced data analytics or analysis that draws on historical data from multiple sources across their enterprise, a data warehouse is likely the right choice.

### What is a relational Database?
* A relational database is a type of database that stores and provides access to data points that are related to one another. Relational databases are based on the relational model, an intuitive, straightforward way of representing data in tables. 
* In a relational database, each row in the table is a record with a **unique ID** called the key. The columns of the table hold attributes of the data, and **each record usually has a value for each attribute**, making it easy to establish the relationships among data points.
* **Example**: Here’s a simple example of two tables a small business might use to process orders for its products. The first table is a customer info table, so each record includes a customer’s name, address, shipping and billing information, phone number, and other contact information. Each bit of information (each attribute) is in its own column, and the database assigns a unique ID (a key) to each row. In the second table—a customer order table—each record includes the ID of the customer that placed the order, the product ordered, the quantity, the selected size and color, and so on—but not the customer’s name or contact information.
* These two tables have **only one thing in common**: the ID column (the key). But because of that common column, the relational database can create a relationship between the two tables. Then, when the company’s order processing application submits an order to the database, the database can go to the customer order table, pull the correct information about the product order, and use the customer ID from that table to look up the customer’s billing and shipping information in the customer info table. The warehouse can then pull the correct product, the customer can receive timely delivery of the order, and the company can get paid.
* The relational model means that the logical data structures—the data tables, views, and indexes—are separate from the physical storage structures.
*  Relational databases are used to track inventories, process ecommerce transactions, manage huge amounts of mission-critical customer information, and much more. A relational database can be considered for any information need in which data points relate to each other and must be managed in a secure, rules-based, consistent way.
* For example, when a customer deposits money at an ATM and then looks at the account balance on a mobile phone, the customer expects to see that deposit reflected immediately in an updated account balance. **Relational databases excel** at this kind of data consistency, ensuring that multiple instances of a database have the same data all the time.

### Continuous Integration (CI)
*  CI is a development practice where developers commit their code changes (usually small and incremental) to a centralized source repository, which kicks off a set of automated builds and tests. 
* This repository allows developers to capture the bugs early and automatically before passing them on to production.
* Continuous Integration pipeline usually involves a series of steps, starting from code commit to performing basic automated linting/static analysis, capturing dependencies, and finally building the software and performing some basic unit tests before creating a build artifact
* Source code management systems like Github, Gitlab, etc., offer webhooks integration to which CI tools like Jenkins can subscribe to start running automated builds and tests after each code check-in.

## Who is a Data Scientist?
* **Duties:** developing strategies for analyzing data, preparing data for analysis, exploring, analyzing, and visualizing data, building models with data using programming languages, such as Python and R, and deploying models into applications.
* Data scientist rarely works alone. 
* The data science team comprises of, Business Analyst, Data Engineer (to prepare data) and IT architect
* Relay on IT Managers to deliver required data (Cloud alleviates this prob)
* **Data Science Platforms** resolve most challenges faced by data science teams (especially in accessing voluminous data)