<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-08-11T11:11:44+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">Arun’s Blog</title><subtitle>Teaching and writing are great ways of learning anything in detail. 
</subtitle><author><name>Arun Prakash A</name></author><entry><title type="html">All About nn.Modules in Pytorch</title><link href="http://localhost:4000/2024/08/10/All_About_nn_Module.html" rel="alternate" type="text/html" title="All About nn.Modules in Pytorch" /><published>2024-08-10T00:00:00+05:30</published><updated>2024-08-10T00:00:00+05:30</updated><id>http://localhost:4000/2024/08/10/All_About_nn_Module</id><content type="html" xml:base="http://localhost:4000/2024/08/10/All_About_nn_Module.html"><![CDATA[<p>If you are a researcher or someone who builds/tweaks the deep learning models regularly using the Pytorch framework or any other high-level frameworks that are built on top of Pytorch such as Huggingface, then it is extremely important to understand Pytorch’s <code class="language-plaintext highlighter-rouge">nn.Modules</code>. This is because your model could run without displaying any symptoms even if you are training the model <code class="language-plaintext error highlighter-rouge">INCORRECTLY</code>! What do I mean by that?</p>

<p>For example, often we need to freeze the parameters of a few layers or a layer (say embeddings in transformer), add a new layer with learnable parameters, change the initialization scheme and train the model from scratch, do some fancy way of updating parameters (like LORA, QLORA). All these tasks require us to change the code carefully (that is, you should know what you are doing)! Having a partial understanding of <code class="language-plaintext highlighter-rouge">nn.Modules</code> will create a false impression (because it won’t raise an error) that your model is working as you expected.</p>

<p>The beginners usefully confuse the various ways of making a tensor learnable in Pytorch such as setting <code class="language-plaintext highlighter-rouge">requires_grad=True</code> or using <code class="language-plaintext highlighter-rouge">nn.Parameter(tensor)</code> or directly using <code class="language-plaintext highlighter-rouge">nn.Linear, nn.Conv2d..</code>.</p>

<p>In this post, let us use simple examples to understand the <code class="language-plaintext highlighter-rouge">nn.Modules</code> better and nuances of using <code class="language-plaintext highlighter-rouge">requires_grad=True</code>, <code class="language-plaintext highlighter-rouge">nn.Parameter(tensor)</code> and <code class="language-plaintext highlighter-rouge">nn.Linear, nn.Conv2d..</code>. You should look into the documentation even if you have a slight doubt about the usage</p>

<p>If you are new to Pytorch, then take a look at this <a href="https://github.com/Arunprakash-A/DL-Pytorch-Workshop">Github repo</a> to learn Pytorch modules and come back here!</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h2 id="mlp-module">MLP Module</h2>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="code"><pre><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x_dim</span><span class="p">,</span><span class="n">h1_dim</span><span class="p">,</span><span class="n">h2_dim</span><span class="p">,</span><span class="n">out_dim</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">w1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">x_dim</span><span class="p">,</span><span class="n">h1_dim</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">w2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h1_dim</span><span class="p">,</span><span class="n">h2_dim</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">w3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h2_dim</span><span class="p">,</span><span class="n">out_dim</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">w1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">w2</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">w3</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>    
    <span class="k">return</span> <span class="n">out</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#instance
</span><span class="n">mlp</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">])</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#print output
</span><span class="nf">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([0.4890], grad_fn=&lt;ReluBackward0&gt;)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="n">mlp</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>MLP(
  (w1): Linear(in_features=1, out_features=3, bias=True)
  (w2): Linear(in_features=3, out_features=3, bias=True)
  (w3): Linear(in_features=3, out_features=1, bias=True)
)
</code></pre></div></div>

<ul>
  <li>Just define a dummy <strong>HEAD</strong> Module to understand different terminologies like children, and modules..</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="code"><pre><span class="k">class</span> <span class="nc">HEAD</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># add mlp module here
</span>        <span class="n">self</span><span class="p">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># using nn.Parameter instead of linear layer
</span>        <span class="n">self</span><span class="p">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># using nn.Parameter instead of linear layer
</span>    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="c1"># do multiplication of x with w instead of passing x as self.w(x) as in MLP module as w is a param
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">w</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="n">self</span><span class="p">.</span><span class="n">b</span>         
        <span class="k">return</span> <span class="n">out</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p><strong>Some useful methods</strong> that help us initialize parameters, add hooks and anything we wish to do with the model.</p>

<h2 id="children">Children</h2>
<ul>
  <li>Return an iterator over <strong>immediate</strong> children modules.</li>
  <li>Immediate children here are: all three <code class="language-plaintext highlighter-rouge">Linear</code> layers</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre><span class="k">for</span> <span class="n">child_name</span><span class="p">,</span><span class="n">layer</span> <span class="ow">in</span> <span class="n">mlp</span><span class="p">.</span><span class="nf">named_children</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">child_name:</span><span class="si">{</span><span class="n">child_name</span><span class="si">}</span><span class="s">,</span><span class="se">\t</span><span class="s"> layer:</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="nf">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="nf">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span><span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>child_name:w1,	 layer:Linear(in_features=1, out_features=3, bias=True)
True
False
child_name:w2,	 layer:Linear(in_features=3, out_features=3, bias=True)
True
False
child_name:w3,	 layer:Linear(in_features=3, out_features=1, bias=True)
True
False
</code></pre></div></div>

<p>Now let’s look at the children of the head module</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre><span class="n">head</span> <span class="o">=</span> <span class="nc">HEAD</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">head</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([-0.8912], grad_fn=&lt;AddBackward0&gt;)
</code></pre></div></div>

<p>Immediate children of the head module are the <code class="language-plaintext highlighter-rouge">MLP</code> module alone, What? Why?</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre><span class="k">for</span> <span class="n">child_name</span><span class="p">,</span><span class="n">layer</span> <span class="ow">in</span> <span class="n">head</span><span class="p">.</span><span class="nf">named_children</span><span class="p">():</span> <span class="c1">#head module as root
</span>    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">child_name:</span><span class="si">{</span><span class="n">child_name</span><span class="si">}</span><span class="s">,</span><span class="se">\t</span><span class="s"> layer:</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="nf">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="nf">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span><span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>child_name:mlp,	 layer:MLP(
  (w1): Linear(in_features=1, out_features=3, bias=True)
  (w2): Linear(in_features=3, out_features=3, bias=True)
  (w3): Linear(in_features=3, out_features=1, bias=True)
)
False
False
</code></pre></div></div>

<p><strong>Observations</strong></p>
<ul>
  <li>Adding parameters via <code class="language-plaintext highlighter-rouge">nn.Parameters()</code> is <strong>not considered as child</strong> (obvious)</li>
  <li>Only the Modules are considered as child</li>
  <li>Let’s replace <code class="language-plaintext highlighter-rouge">nn.Parameter</code> by <code class="language-plaintext highlighter-rouge">nn.Linear</code> and see what happens</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="code"><pre><span class="k">class</span> <span class="nc">HEAD</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># add mlp here
</span>        <span class="n">self</span><span class="p">.</span><span class="n">w</span> <span class="o">=</span>  <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># using linear layer        
</span>    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>       
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>       
        <span class="k">return</span> <span class="n">out</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">head</span> <span class="o">=</span> <span class="nc">HEAD</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">head</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([0.4823], grad_fn=&lt;AddBackward0&gt;)
</code></pre></div></div>

<p><strong>Immediate</strong> children of the head module are <code class="language-plaintext highlighter-rouge">MLP</code> and <code class="language-plaintext highlighter-rouge">Linear modules</code></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre><span class="k">for</span> <span class="n">child_name</span><span class="p">,</span><span class="n">layer</span> <span class="ow">in</span> <span class="n">head</span><span class="p">.</span><span class="nf">named_children</span><span class="p">():</span> <span class="c1"># head module as root
</span>    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">child_name:</span><span class="si">{</span><span class="n">child_name</span><span class="si">}</span><span class="s">,</span><span class="se">\t</span><span class="s"> layer:</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="nf">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="nf">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span><span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>child_name:mlp,	 layer:MLP(
  (w1): Linear(in_features=1, out_features=3, bias=True)
  (w2): Linear(in_features=3, out_features=3, bias=True)
  (w3): Linear(in_features=3, out_features=1, bias=True)
)
False
False
child_name:w,	 layer:Linear(in_features=1, out_features=1, bias=True)
True
False
</code></pre></div></div>

<ul>
  <li>We can use <code class="language-plaintext highlighter-rouge">named_children</code> as well if we want the name of the child module.</li>
</ul>

<h2 id="named-modules">Named modules</h2>
<ul>
  <li>What does the <code class="language-plaintext highlighter-rouge">named_modules</code> return? How does it differes from children?</li>
  <li>It returns <strong>all the nn.modules</strong> in the tree recursively by default <strong>(again, nn.Parameters will be ignored)</strong></li>
  <li>The name for the root module is ‘’ and all the submodules will have their respective names</li>
  <li>If we do not want the name of the module, just use the method <code class="language-plaintext highlighter-rouge">.modules()</code></li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre><span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">mlp</span><span class="p">.</span><span class="nf">named_modules</span><span class="p">():</span> <span class="c1">#returns tuple
</span>    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Module:</span><span class="si">{</span><span class="n">module</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Module:('', MLP(
  (w1): Linear(in_features=1, out_features=3, bias=True)
  (w2): Linear(in_features=3, out_features=3, bias=True)
  (w3): Linear(in_features=3, out_features=1, bias=True)
))
Module:('w1', Linear(in_features=1, out_features=3, bias=True))
Module:('w2', Linear(in_features=3, out_features=3, bias=True))
Module:('w3', Linear(in_features=3, out_features=1, bias=True))
</code></pre></div></div>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre><span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">head</span><span class="p">.</span><span class="nf">named_modules</span><span class="p">():</span> <span class="c1">#returns tuple
</span>    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Module:</span><span class="si">{</span><span class="n">module</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Module:('', HEAD(
  (mlp): MLP(
    (w1): Linear(in_features=1, out_features=3, bias=True)
    (w2): Linear(in_features=3, out_features=3, bias=True)
    (w3): Linear(in_features=3, out_features=1, bias=True)
  )
  (w): Linear(in_features=1, out_features=1, bias=True)
))
Module:('mlp', MLP(
  (w1): Linear(in_features=1, out_features=3, bias=True)
  (w2): Linear(in_features=3, out_features=3, bias=True)
  (w3): Linear(in_features=3, out_features=1, bias=True)
))
Module:('mlp.w1', Linear(in_features=1, out_features=3, bias=True))
Module:('mlp.w2', Linear(in_features=3, out_features=3, bias=True))
Module:('mlp.w3', Linear(in_features=3, out_features=1, bias=True))
Module:('w', Linear(in_features=1, out_features=1, bias=True))
</code></pre></div></div>

<ul>
  <li>MLP is a module we created by deriving from <code class="language-plaintext highlighter-rouge">nn.Module</code></li>
  <li>Linear is also a module created by deriving from <code class="language-plaintext highlighter-rouge">nn.Module</code></li>
  <li>Therefore, if we want all the modules in the model with their names, then we need to call <code class="language-plaintext highlighter-rouge">named_modules</code></li>
</ul>

<h2 id="parameters-and-named_parameters">parameters and named_parameters</h2>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre><span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">mlp</span><span class="p">.</span><span class="nf">named_parameters</span><span class="p">():</span> <span class="c1"># returns tuple
</span>    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">name: </span><span class="si">{</span><span class="n">parameter</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s">  </span><span class="se">\n</span><span class="s"> type: </span><span class="si">{</span><span class="nf">type</span><span class="p">(</span><span class="n">parameter</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s"> param:</span><span class="si">{</span><span class="n">parameter</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s"> type:</span><span class="si">{</span><span class="nf">type</span><span class="p">(</span><span class="n">parameter</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="si">}</span><span class="s"> </span><span class="sh">'</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>name: w1.weight  
 type: &lt;class 'str'&gt; 
 param:Parameter containing:
tensor([[-0.2805],
        [-0.1376],
        [ 0.0760]], requires_grad=True) 
 type:&lt;class 'torch.nn.parameter.Parameter'&gt; 
------------------------------------------------------------------------------------------------------------------------
name: w1.bias  
 type: &lt;class 'str'&gt; 
 param:Parameter containing:
tensor([0.6878, 0.7096, 0.1408], requires_grad=True) 
 type:&lt;class 'torch.nn.parameter.Parameter'&gt; 
------------------------------------------------------------------------------------------------------------------------
name: w2.weight  
 type: &lt;class 'str'&gt; 
 param:Parameter containing:
tensor([[ 0.3649, -0.4209, -0.5017],
        [-0.3077, -0.0009, -0.5688],
        [-0.4915, -0.4050, -0.5599]], requires_grad=True) 
 type:&lt;class 'torch.nn.parameter.Parameter'&gt; 
------------------------------------------------------------------------------------------------------------------------
name: w2.bias  
 type: &lt;class 'str'&gt; 
 param:Parameter containing:
tensor([-0.1146,  0.0343,  0.1846], requires_grad=True) 
 type:&lt;class 'torch.nn.parameter.Parameter'&gt; 
------------------------------------------------------------------------------------------------------------------------
name: w3.weight  
 type: &lt;class 'str'&gt; 
 param:Parameter containing:
tensor([[-0.4755,  0.1015,  0.5086]], requires_grad=True) 
 type:&lt;class 'torch.nn.parameter.Parameter'&gt; 
------------------------------------------------------------------------------------------------------------------------
name: w3.bias  
 type: &lt;class 'str'&gt; 
 param:Parameter containing:
tensor([0.4890], requires_grad=True) 
 type:&lt;class 'torch.nn.parameter.Parameter'&gt; 
------------------------------------------------------------------------------------------------------------------------
</code></pre></div></div>

<ul>
  <li>We can also get the parameters by its name</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mlp</span><span class="p">.</span><span class="nf">get_parameter</span><span class="p">(</span><span class="sh">'</span><span class="s">w1.weight</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Parameter containing:
tensor([[-0.1526],
        [ 0.2645],
        [-0.2916]], requires_grad=True)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">mlp</span><span class="p">.</span><span class="nf">named_buffers</span><span class="p">():</span> <span class="c1"># we do not have any buffers
</span>    <span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>Parameters without name</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre><span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">mlp</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">shape:</span><span class="si">{</span><span class="n">parameter</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">, </span><span class="se">\t</span><span class="s"> num_ele: </span><span class="si">{</span><span class="n">parameter</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span><span class="si">}</span><span class="s">, </span><span class="se">\t</span><span class="s"> type:</span><span class="si">{</span><span class="nf">type</span><span class="p">(</span><span class="n">parameter</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="o">*</span><span class="mi">40</span><span class="p">)</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>shape:torch.Size([3, 1]), 	 num_ele: 3, 	 type:&lt;class 'torch.nn.parameter.Parameter'&gt;
--------------------------------------------------------------------------------
shape:torch.Size([3]), 	 num_ele: 3, 	 type:&lt;class 'torch.nn.parameter.Parameter'&gt;
--------------------------------------------------------------------------------
shape:torch.Size([3, 3]), 	 num_ele: 9, 	 type:&lt;class 'torch.nn.parameter.Parameter'&gt;
--------------------------------------------------------------------------------
shape:torch.Size([3]), 	 num_ele: 3, 	 type:&lt;class 'torch.nn.parameter.Parameter'&gt;
--------------------------------------------------------------------------------
shape:torch.Size([1, 3]), 	 num_ele: 3, 	 type:&lt;class 'torch.nn.parameter.Parameter'&gt;
--------------------------------------------------------------------------------
shape:torch.Size([1]), 	 num_ele: 1, 	 type:&lt;class 'torch.nn.parameter.Parameter'&gt;
--------------------------------------------------------------------------------
</code></pre></div></div>

<p><strong>Important</strong>:</p>
<ul>
  <li>If we want to modify or initialize the weights, then use <code class="language-plaintext highlighter-rouge">module.weight = nn.Parameter()</code>. Avoid using <code class="language-plaintext highlighter-rouge">module.weight = torch.Tensor([],requires_grad=True)</code></li>
</ul>

<h2 id="initialize-the-weights-randomly">Initialize the weights randomly</h2>
<ul>
  <li>Weights of Linear layers are by default initialized randomly using uniform distribution (Kaiming) with $U(-\sqrt{k},\sqrt{k})$, where $k=\frac{1}{in_features}$ <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">doc</a></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">w1</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="nf">type</span><span class="p">(</span><span class="n">w1</span><span class="p">.</span><span class="n">weight</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Parameter containing:
tensor([[ 0.0492, -0.0842,  0.0332,  0.2264, -0.3632],
        [ 0.0654,  0.0897,  0.3019, -0.3227,  0.1919],
        [ 0.0218, -0.3282,  0.4164, -0.3662,  0.4460],
        [-0.2443,  0.0830,  0.2533, -0.3738,  0.0574]], requires_grad=True) &lt;class 'torch.nn.parameter.Parameter'&gt;
</code></pre></div></div>

<ul>
  <li>We can manually initialize using <code class="language-plaintext highlighter-rouge">nn.Parameter()</code> or define an initialize function and apply it to all the parameters</li>
  <li>The latter is helpful if we have a really big model and changing an initialization strategy going layer by layer is painful</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w1</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">w1</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Parameter containing:
tensor([[ 0.4229, -0.6291, -0.4947,  1.7361],
        [-0.6693, -1.5704, -0.3898, -0.0944],
        [-0.3906, -0.8454,  0.2299, -1.0408],
        [ 0.9672,  1.3886, -1.1870, -0.8637],
        [-0.0612,  0.8623, -1.3237, -0.1126]], requires_grad=True)
</code></pre></div></div>

<ul>
  <li>Is that legal? Does that work?</li>
  <li>We answered it already</li>
  <li>Anyhow, let’s look inside the source code</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="code"><pre><span class="n">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">((</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">),</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">out_features</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_parameter</span><span class="p">(</span><span class="sh">'</span><span class="s">bias</span><span class="sh">'</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">reset_parameters</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="c1"># Setting a=sqrt(5) in kaiming_uniform is the same as initializing with
</span>    <span class="c1"># uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see
</span>    <span class="c1"># https://github.com/pytorch/pytorch/issues/57109
</span>    <span class="n">init</span><span class="p">.</span><span class="nf">kaiming_uniform_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">fan_in</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">init</span><span class="p">.</span><span class="nf">_calculate_fan_in_and_fan_out</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">bound</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span> <span class="k">if</span> <span class="n">fan_in</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">init</span><span class="p">.</span><span class="nf">uniform_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">bias</span><span class="p">,</span> <span class="o">-</span><span class="n">bound</span><span class="p">,</span> <span class="n">bound</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<ul>
  <li>Finally, <code class="language-plaintext highlighter-rouge">nn.functional.linear(x,self.weight,self.bias)</code> is called in the forward method.</li>
  <li>Therefore, <code class="language-plaintext highlighter-rouge">w1.weight</code> modifies <code class="language-plaintext highlighter-rouge">self.weight</code> before calling the forward method</li>
</ul>

<p>What if we want to initialize the parameters of <strong>all linear layers of the model</strong> using normal distribution?</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre><span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
    <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">):</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="n">module</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span><span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span><span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>    
</pre></td></tr></tbody></table></code></pre></figure>

<p>Apply <code class="language-plaintext highlighter-rouge">fn</code> <strong>recursively</strong> to every submodule (as returned by .children()) as well as self.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">head</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>HEAD(
  (mlp): MLP(
    (w1): Linear(in_features=1, out_features=3, bias=True)
    (w2): Linear(in_features=3, out_features=3, bias=True)
    (w3): Linear(in_features=3, out_features=1, bias=True)
  )
  (w): Linear(in_features=1, out_features=1, bias=True)
)
</code></pre></div></div>

<ul>
  <li>Children first return MLP</li>
  <li>GO into MLP and its children..start recursing!</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stack</span> <span class="o">=</span> <span class="p">[]</span>
</code></pre></div></div>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre><span class="k">def</span> <span class="nf">recursive_call</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">module</span><span class="p">.</span><span class="nf">children</span><span class="p">():</span>
        <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span><span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">):</span>            
            <span class="k">return</span> <span class="mi">1</span>
        <span class="nf">recursive_call</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">recursive_call</span><span class="p">(</span><span class="n">head</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Linear(in_features=1, out_features=3, bias=True)
Linear(in_features=3, out_features=3, bias=True)
Linear(in_features=3, out_features=1, bias=True)
Linear(in_features=3, out_features=1, bias=True)
Linear(in_features=1, out_features=1, bias=True)
Linear(in_features=1, out_features=1, bias=True)
</code></pre></div></div>

<h2 id="hook-for-tracking">Hook for tracking</h2>

<ul>
  <li>Hooks are mostly used for debugging purposes (such as tracking input, gradient, output…)</li>
</ul>

<h3 id="forward_hook">forward_hook</h3>
<ul>
  <li>Modifies the output (if needed)</li>
  <li>Look at the signature of various hooks</li>
  <li><strong>We can register more than one hook for the same Module</strong></li>
</ul>

<p><strong>Signature:</strong> <code class="language-plaintext highlighter-rouge">hook(module, input, output) -&gt; None or modified output</code> <a href="https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html">doc</a></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="code"><pre><span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">def</span> <span class="nf">forward_hook_1</span><span class="p">(</span><span class="n">module</span><span class="p">,</span><span class="nb">input</span><span class="p">,</span><span class="n">output</span><span class="p">):</span>
    <span class="c1">## we can do whatever we want like storing, printing, tracking..
</span>    <span class="nf">print</span><span class="p">(</span><span class="n">module</span><span class="p">,</span><span class="nb">input</span><span class="p">,</span><span class="n">output</span><span class="p">)</span>    

<span class="k">def</span> <span class="nf">forward_hook_2</span><span class="p">(</span><span class="n">module</span><span class="p">,</span><span class="nb">input</span><span class="p">,</span><span class="n">output</span><span class="p">):</span>
    <span class="n">out</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>  
</pre></td></tr></tbody></table></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre><span class="n">handle_hook_1</span> <span class="o">=</span> <span class="n">mlp</span><span class="p">.</span><span class="nf">register_forward_hook</span><span class="p">(</span><span class="n">forward_hook_1</span><span class="p">)</span>
<span class="n">handle_hook_2</span> <span class="o">=</span> <span class="n">mlp</span><span class="p">.</span><span class="nf">register_forward_hook</span><span class="p">(</span><span class="n">forward_hook_2</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#execute three times to coolect the output from forward the "out"
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="nf">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>MLP(
  (w1): Linear(in_features=1, out_features=3, bias=True)
  (w2): Linear(in_features=3, out_features=3, bias=True)
  (w3): Linear(in_features=3, out_features=1, bias=True)
) (tensor([1.]),) tensor([0.0551], grad_fn=&lt;ReluBackward0&gt;)
MLP(
  (w1): Linear(in_features=1, out_features=3, bias=True)
  (w2): Linear(in_features=3, out_features=3, bias=True)
  (w3): Linear(in_features=3, out_features=1, bias=True)
) (tensor([1.]),) tensor([0.0551], grad_fn=&lt;ReluBackward0&gt;)
MLP(
  (w1): Linear(in_features=1, out_features=3, bias=True)
  (w2): Linear(in_features=3, out_features=3, bias=True)
  (w3): Linear(in_features=3, out_features=1, bias=True)
) (tensor([1.]),) tensor([0.0551], grad_fn=&lt;ReluBackward0&gt;)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[tensor([0.0551], grad_fn=&lt;ReluBackward0&gt;), tensor([0.0551], grad_fn=&lt;ReluBackward0&gt;), tensor([0.0551], grad_fn=&lt;ReluBackward0&gt;), tensor([0.0551], grad_fn=&lt;ReluBackward0&gt;), tensor([0.0551], grad_fn=&lt;ReluBackward0&gt;)]
</code></pre></div></div>

<ul>
  <li>How do we remove the hook?</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre><span class="n">handle_hook_1</span><span class="p">.</span><span class="nf">remove</span><span class="p">()</span>
<span class="n">handle_hook_2</span><span class="p">.</span><span class="nf">remove</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="nf">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p>Modify the output</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">out</span> <span class="o">=</span> <span class="nf">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([0.0551], grad_fn=&lt;ReluBackward0&gt;)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">fwd_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span><span class="n">args</span><span class="p">,</span><span class="n">output</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span><span class="o">*</span><span class="n">output</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">handle_1</span> <span class="o">=</span> <span class="n">mlp</span><span class="p">.</span><span class="nf">register_forward_hook</span><span class="p">(</span><span class="n">fwd_hook</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">out</span> <span class="o">=</span> <span class="nf">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([0.1102], grad_fn=&lt;MulBackward0&gt;)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">handle_1</span><span class="p">.</span><span class="nf">remove</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="forward-pre_hook">forward Pre_hook</h3>

<ul>
  <li>signature: <code class="language-plaintext highlighter-rouge">hook(module, input) -&gt; None or modified input</code></li>
  <li>To (optionally) modify the <strong>input</strong></li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre><span class="k">def</span> <span class="nf">fwd_pre_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span><span class="nb">input</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="n">handle</span> <span class="o">=</span> <span class="n">mlp</span><span class="p">.</span><span class="nf">register_forward_pre_hook</span><span class="p">(</span><span class="n">fwd_pre_hook</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(tensor([1.]),)





tensor([0.0551], grad_fn=&lt;ReluBackward0&gt;)
</code></pre></div></div>

<h3 id="backward_hook">backward_hook</h3>

<h3 id="backward_pre_hook">backward_pre_hook</h3>

<h3 id="saveload-state_dict">save/load State_dict</h3>
<ul>
  <li>Both parameters and persistent buffers (e.g. running averages) are included</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mlp</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">().</span><span class="nf">keys</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>odict_keys(['w1.weight', 'w1.bias', 'w2.weight', 'w2.bias', 'w3.weight', 'w3.bias'])
</code></pre></div></div>

<p>Hope you enjoyed reading the article and learned something important!</p>]]></content><author><name>Arun Prakash A</name></author><category term="DL" /><summary type="html"><![CDATA[If you are a researcher or someone who builds/tweaks the deep learning models regularly using the Pytorch framework or any other high-level frameworks that are built on top of Pytorch such as Huggingface, then it is extremely important to understand Pytorch’s nn.Modules. This is because your model could run without displaying any symptoms even if you are training the model INCORRECTLY! What do I mean by that? For example, often we need to freeze the parameters of a few layers or a layer (say embeddings in transformer), add a new layer with learnable parameters, change the initialization scheme and train the model from scratch, do some fancy way of updating parameters (like LORA, QLORA). All these tasks require us to change the code carefully (that is, you should know what you are doing)! Having a partial understanding of nn.Modules will create a false impression (because it won’t raise an error) that your model is working as you expected. The beginners usefully confuse the various ways of making a tensor learnable in Pytorch such as setting requires_grad=True or using nn.Parameter(tensor) or directly using nn.Linear, nn.Conv2d... In this post, let us use simple examples to understand the nn.Modules better and nuances of using requires_grad=True, nn.Parameter(tensor) and nn.Linear, nn.Conv2d... You should look into the documentation even if you have a slight doubt about the usage If you are new to Pytorch, then take a look at this Github repo to learn Pytorch modules and come back here! 1 2 import torch import torch.nn as nn MLP Module 1 2 3 4 5 6 7 8 9 10 11 12 13 class MLP(nn.Module): def __init__(self,x_dim,h1_dim,h2_dim,out_dim): super().__init__() self.w1 = nn.Linear(x_dim,h1_dim,bias=True) self.w2 = nn.Linear(h1_dim,h2_dim,bias=True) self.w3 = nn.Linear(h2_dim,out_dim,bias=True) def forward(self,x): out = torch.relu(self.w1(x)) out = torch.relu(self.w2(out)) out = torch.relu(self.w3(out)) return out #instance mlp = MLP(1,3,3,1) 1 2 x = torch.tensor([1.0]) y = torch.tensor([0.0]) #print output mlp(x) tensor([0.4890], grad_fn=&lt;ReluBackward0&gt;) print(mlp) MLP( (w1): Linear(in_features=1, out_features=3, bias=True) (w2): Linear(in_features=3, out_features=3, bias=True) (w3): Linear(in_features=3, out_features=1, bias=True) ) Just define a dummy HEAD Module to understand different terminologies like children, and modules.. 1 2 3 4 5 6 7 8 9 10 11 12 13 class HEAD(nn.Module): def __init__(self): super().__init__() self.mlp = MLP(1,3,3,1) # add mlp module here self.w = nn.Parameter(torch.randn(1)) # using nn.Parameter instead of linear layer self.b = nn.Parameter(torch.randn(1)) # using nn.Parameter instead of linear layer def forward(self,x): # do multiplication of x with w instead of passing x as self.w(x) as in MLP module as w is a param out = self.mlp(x) out = self.w*x+self.b return out Some useful methods that help us initialize parameters, add hooks and anything we wish to do with the model. Children Return an iterator over immediate children modules. Immediate children here are: all three Linear layers 1 2 3 4 for child_name,layer in mlp.named_children(): print(f'child_name:{child_name},\t layer:{layer}') print(isinstance(layer,nn.Linear)) print(isinstance(layer,nn.Embedding)) child_name:w1, layer:Linear(in_features=1, out_features=3, bias=True) True False child_name:w2, layer:Linear(in_features=3, out_features=3, bias=True) True False child_name:w3, layer:Linear(in_features=3, out_features=1, bias=True) True False Now let’s look at the children of the head module 1 2 head = HEAD() print(head(x)) tensor([-0.8912], grad_fn=&lt;AddBackward0&gt;) Immediate children of the head module are the MLP module alone, What? Why? 1 2 3 4 for child_name,layer in head.named_children(): #head module as root print(f'child_name:{child_name},\t layer:{layer}') print(isinstance(layer,nn.Linear)) print(isinstance(layer,nn.Embedding)) child_name:mlp, layer:MLP( (w1): Linear(in_features=1, out_features=3, bias=True) (w2): Linear(in_features=3, out_features=3, bias=True) (w3): Linear(in_features=3, out_features=1, bias=True) ) False False Observations Adding parameters via nn.Parameters() is not considered as child (obvious) Only the Modules are considered as child Let’s replace nn.Parameter by nn.Linear and see what happens 1 2 3 4 5 6 7 8 9 10 11 class HEAD(nn.Module): def __init__(self): super().__init__() self.mlp = MLP(1,3,3,1) # add mlp here self.w = nn.Linear(1,1,bias=True) # using linear layer def forward(self,x): out = self.mlp(x) out = self.w(x) return out head = HEAD() print(head(x)) tensor([0.4823], grad_fn=&lt;AddBackward0&gt;) Immediate children of the head module are MLP and Linear modules 1 2 3 4 for child_name,layer in head.named_children(): # head module as root print(f'child_name:{child_name},\t layer:{layer}') print(isinstance(layer,nn.Linear)) print(isinstance(layer,nn.Embedding)) child_name:mlp, layer:MLP( (w1): Linear(in_features=1, out_features=3, bias=True) (w2): Linear(in_features=3, out_features=3, bias=True) (w3): Linear(in_features=3, out_features=1, bias=True) ) False False child_name:w, layer:Linear(in_features=1, out_features=1, bias=True) True False We can use named_children as well if we want the name of the child module. Named modules What does the named_modules return? How does it differes from children? It returns all the nn.modules in the tree recursively by default (again, nn.Parameters will be ignored) The name for the root module is ‘’ and all the submodules will have their respective names If we do not want the name of the module, just use the method .modules() 1 2 for module in mlp.named_modules(): #returns tuple print(f'Module:{module}') Module:('', MLP( (w1): Linear(in_features=1, out_features=3, bias=True) (w2): Linear(in_features=3, out_features=3, bias=True) (w3): Linear(in_features=3, out_features=1, bias=True) )) Module:('w1', Linear(in_features=1, out_features=3, bias=True)) Module:('w2', Linear(in_features=3, out_features=3, bias=True)) Module:('w3', Linear(in_features=3, out_features=1, bias=True)) 1 2 for module in head.named_modules(): #returns tuple print(f'Module:{module}') Module:('', HEAD( (mlp): MLP( (w1): Linear(in_features=1, out_features=3, bias=True) (w2): Linear(in_features=3, out_features=3, bias=True) (w3): Linear(in_features=3, out_features=1, bias=True) ) (w): Linear(in_features=1, out_features=1, bias=True) )) Module:('mlp', MLP( (w1): Linear(in_features=1, out_features=3, bias=True) (w2): Linear(in_features=3, out_features=3, bias=True) (w3): Linear(in_features=3, out_features=1, bias=True) )) Module:('mlp.w1', Linear(in_features=1, out_features=3, bias=True)) Module:('mlp.w2', Linear(in_features=3, out_features=3, bias=True)) Module:('mlp.w3', Linear(in_features=3, out_features=1, bias=True)) Module:('w', Linear(in_features=1, out_features=1, bias=True)) MLP is a module we created by deriving from nn.Module Linear is also a module created by deriving from nn.Module Therefore, if we want all the modules in the model with their names, then we need to call named_modules parameters and named_parameters 1 2 3 for parameter in mlp.named_parameters(): # returns tuple print(f'name: {parameter[0]} \n type: {type(parameter[0])} \n param:{parameter[1]} \n type:{type(parameter[1])} ') print('--'*60) name: w1.weight type: &lt;class 'str'&gt; param:Parameter containing: tensor([[-0.2805], [-0.1376], [ 0.0760]], requires_grad=True) type:&lt;class 'torch.nn.parameter.Parameter'&gt; ------------------------------------------------------------------------------------------------------------------------ name: w1.bias type: &lt;class 'str'&gt; param:Parameter containing: tensor([0.6878, 0.7096, 0.1408], requires_grad=True) type:&lt;class 'torch.nn.parameter.Parameter'&gt; ------------------------------------------------------------------------------------------------------------------------ name: w2.weight type: &lt;class 'str'&gt; param:Parameter containing: tensor([[ 0.3649, -0.4209, -0.5017], [-0.3077, -0.0009, -0.5688], [-0.4915, -0.4050, -0.5599]], requires_grad=True) type:&lt;class 'torch.nn.parameter.Parameter'&gt; ------------------------------------------------------------------------------------------------------------------------ name: w2.bias type: &lt;class 'str'&gt; param:Parameter containing: tensor([-0.1146, 0.0343, 0.1846], requires_grad=True) type:&lt;class 'torch.nn.parameter.Parameter'&gt; ------------------------------------------------------------------------------------------------------------------------ name: w3.weight type: &lt;class 'str'&gt; param:Parameter containing: tensor([[-0.4755, 0.1015, 0.5086]], requires_grad=True) type:&lt;class 'torch.nn.parameter.Parameter'&gt; ------------------------------------------------------------------------------------------------------------------------ name: w3.bias type: &lt;class 'str'&gt; param:Parameter containing: tensor([0.4890], requires_grad=True) type:&lt;class 'torch.nn.parameter.Parameter'&gt; ------------------------------------------------------------------------------------------------------------------------ We can also get the parameters by its name mlp.get_parameter('w1.weight') Parameter containing: tensor([[-0.1526], [ 0.2645], [-0.2916]], requires_grad=True) for x in mlp.named_buffers(): # we do not have any buffers print(x) Parameters without name 1 2 3 4 for parameter in mlp.parameters(): print(f'shape:{parameter.shape}, \t num_ele: {parameter.numel()}, \t type:{type(parameter)}') print('--'*40) shape:torch.Size([3, 1]), num_ele: 3, type:&lt;class 'torch.nn.parameter.Parameter'&gt; -------------------------------------------------------------------------------- shape:torch.Size([3]), num_ele: 3, type:&lt;class 'torch.nn.parameter.Parameter'&gt; -------------------------------------------------------------------------------- shape:torch.Size([3, 3]), num_ele: 9, type:&lt;class 'torch.nn.parameter.Parameter'&gt; -------------------------------------------------------------------------------- shape:torch.Size([3]), num_ele: 3, type:&lt;class 'torch.nn.parameter.Parameter'&gt; -------------------------------------------------------------------------------- shape:torch.Size([1, 3]), num_ele: 3, type:&lt;class 'torch.nn.parameter.Parameter'&gt; -------------------------------------------------------------------------------- shape:torch.Size([1]), num_ele: 1, type:&lt;class 'torch.nn.parameter.Parameter'&gt; -------------------------------------------------------------------------------- Important: If we want to modify or initialize the weights, then use module.weight = nn.Parameter(). Avoid using module.weight = torch.Tensor([],requires_grad=True) Initialize the weights randomly Weights of Linear layers are by default initialized randomly using uniform distribution (Kaiming) with $U(-\sqrt{k},\sqrt{k})$, where $k=\frac{1}{in_features}$ doc w1 = nn.Linear(5,4,bias=True) print(w1.weight, type(w1.weight)) Parameter containing: tensor([[ 0.0492, -0.0842, 0.0332, 0.2264, -0.3632], [ 0.0654, 0.0897, 0.3019, -0.3227, 0.1919], [ 0.0218, -0.3282, 0.4164, -0.3662, 0.4460], [-0.2443, 0.0830, 0.2533, -0.3738, 0.0574]], requires_grad=True) &lt;class 'torch.nn.parameter.Parameter'&gt; We can manually initialize using nn.Parameter() or define an initialize function and apply it to all the parameters The latter is helpful if we have a really big model and changing an initialization strategy going layer by layer is painful w1.weight = nn.Parameter(torch.randn(5,4)) print(w1.weight) Parameter containing: tensor([[ 0.4229, -0.6291, -0.4947, 1.7361], [-0.6693, -1.5704, -0.3898, -0.0944], [-0.3906, -0.8454, 0.2299, -1.0408], [ 0.9672, 1.3886, -1.1870, -0.8637], [-0.0612, 0.8623, -1.3237, -0.1126]], requires_grad=True) Is that legal? Does that work? We answered it already Anyhow, let’s look inside the source code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs)) if bias: self.bias = Parameter(torch.empty(out_features, **factory_kwargs)) else: self.register_parameter('bias', None) self.reset_parameters() def reset_parameters(self) -&gt; None: # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with # uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see # https://github.com/pytorch/pytorch/issues/57109 init.kaiming_uniform_(self.weight, a=math.sqrt(5)) if self.bias is not None: fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight) bound = 1 / math.sqrt(fan_in) if fan_in &gt; 0 else 0 init.uniform_(self.bias, -bound, bound) Finally, nn.functional.linear(x,self.weight,self.bias) is called in the forward method. Therefore, w1.weight modifies self.weight before calling the forward method What if we want to initialize the parameters of all linear layers of the model using normal distribution? 1 2 3 def init_weights(module): if isinstance(module,nn.Linear): torch.nn.init.normal_(module.weight,mean=0.0,std=0.02) Apply fn recursively to every submodule (as returned by .children()) as well as self. head.apply(init_weights) HEAD( (mlp): MLP( (w1): Linear(in_features=1, out_features=3, bias=True) (w2): Linear(in_features=3, out_features=3, bias=True) (w3): Linear(in_features=3, out_features=1, bias=True) ) (w): Linear(in_features=1, out_features=1, bias=True) ) Children first return MLP GO into MLP and its children..start recursing! stack = [] 1 2 3 4 5 6 def recursive_call(module): for module in module.children(): if isinstance(module,nn.Parameter): return 1 recursive_call(module) print(module) recursive_call(head) Linear(in_features=1, out_features=3, bias=True) Linear(in_features=3, out_features=3, bias=True) Linear(in_features=3, out_features=1, bias=True) Linear(in_features=3, out_features=1, bias=True) Linear(in_features=1, out_features=1, bias=True) Linear(in_features=1, out_features=1, bias=True) Hook for tracking Hooks are mostly used for debugging purposes (such as tracking input, gradient, output…) forward_hook Modifies the output (if needed) Look at the signature of various hooks We can register more than one hook for the same Module Signature: hook(module, input, output) -&gt; None or modified output doc 1 2 3 4 5 6 7 out = [] def forward_hook_1(module,input,output): ## we can do whatever we want like storing, printing, tracking.. print(module,input,output) def forward_hook_2(module,input,output): out.append(output) 1 2 handle_hook_1 = mlp.register_forward_hook(forward_hook_1) handle_hook_2 = mlp.register_forward_hook(forward_hook_2) #execute three times to coolect the output from forward the "out" for i in range(3): mlp(x) MLP( (w1): Linear(in_features=1, out_features=3, bias=True) (w2): Linear(in_features=3, out_features=3, bias=True) (w3): Linear(in_features=3, out_features=1, bias=True) ) (tensor([1.]),) tensor([0.0551], grad_fn=&lt;ReluBackward0&gt;) MLP( (w1): Linear(in_features=1, out_features=3, bias=True) (w2): Linear(in_features=3, out_features=3, bias=True) (w3): Linear(in_features=3, out_features=1, bias=True) ) (tensor([1.]),) tensor([0.0551], grad_fn=&lt;ReluBackward0&gt;) MLP( (w1): Linear(in_features=1, out_features=3, bias=True) (w2): Linear(in_features=3, out_features=3, bias=True) (w3): Linear(in_features=3, out_features=1, bias=True) ) (tensor([1.]),) tensor([0.0551], grad_fn=&lt;ReluBackward0&gt;) print(out) [tensor([0.0551], grad_fn=&lt;ReluBackward0&gt;), tensor([0.0551], grad_fn=&lt;ReluBackward0&gt;), tensor([0.0551], grad_fn=&lt;ReluBackward0&gt;), tensor([0.0551], grad_fn=&lt;ReluBackward0&gt;), tensor([0.0551], grad_fn=&lt;ReluBackward0&gt;)] How do we remove the hook? 1 2 handle_hook_1.remove() handle_hook_2.remove() for i in range(3): mlp(x) Modify the output out = mlp(x) print(out) tensor([0.0551], grad_fn=&lt;ReluBackward0&gt;) def fwd_hook(module,args,output): return 2*output handle_1 = mlp.register_forward_hook(fwd_hook) out = mlp(x) print(out) tensor([0.1102], grad_fn=&lt;MulBackward0&gt;) handle_1.remove() forward Pre_hook signature: hook(module, input) -&gt; None or modified input To (optionally) modify the input 1 2 3 4 def fwd_pre_hook(module,input): print(input) handle = mlp.register_forward_pre_hook(fwd_pre_hook) mlp(x) (tensor([1.]),) tensor([0.0551], grad_fn=&lt;ReluBackward0&gt;) backward_hook backward_pre_hook save/load State_dict Both parameters and persistent buffers (e.g. running averages) are included mlp.state_dict().keys() odict_keys(['w1.weight', 'w1.bias', 'w2.weight', 'w2.bias', 'w3.weight', 'w3.bias']) Hope you enjoyed reading the article and learned something important!]]></summary></entry><entry><title type="html">Model Training Strategies</title><link href="http://localhost:4000/2024/07/19/Model-Training-Strategies.html" rel="alternate" type="text/html" title="Model Training Strategies" /><published>2024-07-19T00:00:00+05:30</published><updated>2024-07-19T00:00:00+05:30</updated><id>http://localhost:4000/2024/07/19/Model-Training-Strategies</id><content type="html" xml:base="http://localhost:4000/2024/07/19/Model-Training-Strategies.html"><![CDATA[<p>Training large language models from scratch is the job of tech giants. Often, the pre-trained proprietary models are adapted to downstream tasks using instruction fine-tuning. However, doing full fine-tuning of model parameters increases the model performance. Of course, full fine- tuning of large models with Billions of parameters requires a good amount of compute resources.</p>

<p>Anyhow, we typically want to train the model as quickly as possible. This requires passing a larger batch of samples (increased memory) and faster computation (increased FLOPs (Floating Point Operations per Second). There is also an important hidden factor: communication between memory and compute unit. Its role is important in distributed training on multiple nodes.</p>

<p>Broadly, we can tailor the strategies based on the available compute resources.</p>

<ul>
  <li>Access to single-gpu</li>
  <li>Access to Multi-GPU (in a single node or clusters)</li>
</ul>

<h1 id="single-gpu-setting">Single GPU setting</h1>

<p>Assume that we have a GPU with 16 GB of memory (Say, Nvidia T4). Suppose we have a model that has 335 million parameters (for example, BERT large). What is the memory requirement to store the model?</p>

<ul>
  <li>It requires 1.35 GB of memory in FP32</li>
  <li>Moreover, if we want to place the model on a GPU memory <code class="language-plaintext highlighter-rouge">(model.to(device))</code>, loading the kernel (A function that is meant to be executed in parallel on an attached GPU is called a kernel <a href="https://cvw.cac.cornell.edu/gpu-architecture/gpu-characteristics/kernel_sm">Ref</a>) takes additional (400MB to 2 GB) memory</li>
</ul>

<p>So we need almost 2GB of GPU memory just to load the model. As we move into training, the required amount of memory goes up drastically, roughly calculated as follows</p>

<ul>
  <li>Gradient requires the same 1.35 GB</li>
  <li>Optimizer states (like $\beta_1$, $\beta_2$ in Adam) require 1.35 GB each</li>
  <li>Finally, storing the output activation values takes a significant amount of memory</li>
</ul>

<p>Put together, naively training the model requires at least (ignoring activation) $1.35+ 2*(1.35)+1=5GB$ of memory per sample. When the batch size increases, the amount of memory for storing the gradients and output activation values increases proportionally (memory requirements for optimizer states won’t change as they can be accumulated across samples). So we will be running out of memory (Cuda:OOM) if the batch size goes beyond 6. How do we overcome this?</p>

<p>We have to make a tradeoff with computation time. Is making the tradeoff worthy?</p>

<h2 id="gradient-accumulation">Gradient Accumulation</h2>

<p>So, how do we increase the effective batch size to 36? In the previous setting, we were able to load at max 6 samples into the given GPU memory. One simple idea is instead of storing gradients for each sample separately, just accumulate them (we assume there is enough space to store the output activation values for the extra samples during forward). In this way, we can effectively do the update for a batch of size 36. Here is the demonstration of the concept in the colab</p>

<script src="https://gist.github.com/Arunprakash-A/c27ebe06e6c8fbd21263fc54013bbf49.js"></script>

<p>The tradeoff here is the training time. Suppose we have used mini-batch gradient descent with a batch of size 36 with a suitable GPU, then in 100 iterations, we would have made 100 updates (collectively used 3600 samples). In gradient accumulation, each iteration can take only 6 samples. Therefore, one weight update happens after every 6 iterations (steps). So only at the end of 600 iterations, we would have the same weight values given by mini-batch gradient descent. This is the trade-off we have to make. It is fine as it generally gives better test performance. Note that gradient accumulation is not possible if we use optimizers like GaLore, and AdaFactor as the factorize the gradients!</p>

<h2 id="activation-check-pointing">Activation Check Pointing</h2>

<p>We can save a significant amount of memory by recomputing the activation values instead of storing those for large networks. The animation below shows how activation values that are stored during forward prop are consumed during backpropagation</p>
<p align="center">
  <img align="center" src="/images/Training-Strategies/1.webp" />
</p>

<p>The memory requirement scales linearly if we increase the number of nodes. During backpropagation, the activation at the $n-th$ node is consumed first (so it is independent of values in previous nodes). So we can drop storing the activation values for the previous nodes to compute the gradients for $n-th$ node. What about the gradients for $n-1$ the node? Well, we need to recompute it! That’s the trade-off we make</p>

<p>It is beautifully illustrated in the animation below.</p>
<p align="center">
  <img align="center" src="/images/Training-Strategies/2.webp" />
</p>

<p>In total,  for every batch of samples, this approach requires $O(n^2)$ computation and $O(1)$ memory. To reduce the quadratic computational complexity, instead of dropping activation values of <strong>all</strong> previous nodes, we checkpoint some of them as shown below to bring down $O(n^2)$</p>

<p align="center">
  <img align="center" src="/images/Training-Strategies/3.webp" />
</p>

<p>In this case, we have chosen the second node for checkpointing. Therefore, activation values for 3rd node is obtained by forward passing the values from the second node (no need to do a full forward pass from the inputs). This is illustrated in the animation below</p>

<p align="center">
  <img align="center" src="/images/Training-Strategies/4.webp" />
</p>

<p>We can combine Gradient Accumulation and Activation Check-pointing to reduce memory requirements significantly. What is the interval of dropping nodes? Well, it is recommended to drop $\sqrt{n}$ nodes if we have $n$ nodes in the network. To see the relation between node, memory and computation time, see the original <a href="https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9">article</a> (all images were taken from there). Do remember that if we combine these approaches, it will take still more time for training than training with gradient accumulation alone.</p>

<p>In HF, we can simply turn it on by setting <code class="language-plaintext highlighter-rouge">gradient_checkpointing=True</code> while instantiating <code class="language-plaintext highlighter-rouge">TrainingArguments</code>.</p>

<h2 id="mixed-precision-training">Mixed-Precision Training</h2>

<ul>
  <li>Not all variables require FP32</li>
  <li>Store activation  FP16 and gradient in FP32</li>
  <li>However, for training, load gradients in FP16.</li>
  <li>Modern GPUs support BF16</li>
</ul>

<p>Once again, in HF, we can simply turn it on by setting <code class="language-plaintext highlighter-rouge">fp16=True</code> while instantiating <code class="language-plaintext highlighter-rouge">TrainingArguments</code>. Moreover, we can combine this with the above techniques!</p>

<h2 id="memory-efficient-optimizers">Memory Efficient Optimizers</h2>

<ul>
  <li>Use memory-efficient optimizers such as AdaFactor, LION, 8-bit Adam, GaLore</li>
  <li>However, if our model gets big, we need to use distributed training using multiple GPUs!</li>
</ul>

<p><a href="https://huggingface.co/docs/transformers/v4.18.0/en/performance">Ref:</a> (lengthy but well-written article from HF). I just added a running example.</p>]]></content><author><name>Arun Prakash A</name></author><category term="LLM" /><category term="DL" /><summary type="html"><![CDATA[Training large language models from scratch is the job of tech giants. Often, the pre-trained proprietary models are adapted to downstream tasks using instruction fine-tuning. However, doing full fine-tuning of model parameters increases the model performance. Of course, full fine- tuning of large models with Billions of parameters requires a good amount of compute resources. Anyhow, we typically want to train the model as quickly as possible. This requires passing a larger batch of samples (increased memory) and faster computation (increased FLOPs (Floating Point Operations per Second). There is also an important hidden factor: communication between memory and compute unit. Its role is important in distributed training on multiple nodes. Broadly, we can tailor the strategies based on the available compute resources. Access to single-gpu Access to Multi-GPU (in a single node or clusters) Single GPU setting Assume that we have a GPU with 16 GB of memory (Say, Nvidia T4). Suppose we have a model that has 335 million parameters (for example, BERT large). What is the memory requirement to store the model? It requires 1.35 GB of memory in FP32 Moreover, if we want to place the model on a GPU memory (model.to(device)), loading the kernel (A function that is meant to be executed in parallel on an attached GPU is called a kernel Ref) takes additional (400MB to 2 GB) memory So we need almost 2GB of GPU memory just to load the model. As we move into training, the required amount of memory goes up drastically, roughly calculated as follows Gradient requires the same 1.35 GB Optimizer states (like $\beta_1$, $\beta_2$ in Adam) require 1.35 GB each Finally, storing the output activation values takes a significant amount of memory Put together, naively training the model requires at least (ignoring activation) $1.35+ 2*(1.35)+1=5GB$ of memory per sample. When the batch size increases, the amount of memory for storing the gradients and output activation values increases proportionally (memory requirements for optimizer states won’t change as they can be accumulated across samples). So we will be running out of memory (Cuda:OOM) if the batch size goes beyond 6. How do we overcome this? We have to make a tradeoff with computation time. Is making the tradeoff worthy? Gradient Accumulation So, how do we increase the effective batch size to 36? In the previous setting, we were able to load at max 6 samples into the given GPU memory. One simple idea is instead of storing gradients for each sample separately, just accumulate them (we assume there is enough space to store the output activation values for the extra samples during forward). In this way, we can effectively do the update for a batch of size 36. Here is the demonstration of the concept in the colab The tradeoff here is the training time. Suppose we have used mini-batch gradient descent with a batch of size 36 with a suitable GPU, then in 100 iterations, we would have made 100 updates (collectively used 3600 samples). In gradient accumulation, each iteration can take only 6 samples. Therefore, one weight update happens after every 6 iterations (steps). So only at the end of 600 iterations, we would have the same weight values given by mini-batch gradient descent. This is the trade-off we have to make. It is fine as it generally gives better test performance. Note that gradient accumulation is not possible if we use optimizers like GaLore, and AdaFactor as the factorize the gradients! Activation Check Pointing We can save a significant amount of memory by recomputing the activation values instead of storing those for large networks. The animation below shows how activation values that are stored during forward prop are consumed during backpropagation The memory requirement scales linearly if we increase the number of nodes. During backpropagation, the activation at the $n-th$ node is consumed first (so it is independent of values in previous nodes). So we can drop storing the activation values for the previous nodes to compute the gradients for $n-th$ node. What about the gradients for $n-1$ the node? Well, we need to recompute it! That’s the trade-off we make It is beautifully illustrated in the animation below. In total, for every batch of samples, this approach requires $O(n^2)$ computation and $O(1)$ memory. To reduce the quadratic computational complexity, instead of dropping activation values of all previous nodes, we checkpoint some of them as shown below to bring down $O(n^2)$ In this case, we have chosen the second node for checkpointing. Therefore, activation values for 3rd node is obtained by forward passing the values from the second node (no need to do a full forward pass from the inputs). This is illustrated in the animation below We can combine Gradient Accumulation and Activation Check-pointing to reduce memory requirements significantly. What is the interval of dropping nodes? Well, it is recommended to drop $\sqrt{n}$ nodes if we have $n$ nodes in the network. To see the relation between node, memory and computation time, see the original article (all images were taken from there). Do remember that if we combine these approaches, it will take still more time for training than training with gradient accumulation alone. In HF, we can simply turn it on by setting gradient_checkpointing=True while instantiating TrainingArguments. Mixed-Precision Training Not all variables require FP32 Store activation FP16 and gradient in FP32 However, for training, load gradients in FP16. Modern GPUs support BF16 Once again, in HF, we can simply turn it on by setting fp16=True while instantiating TrainingArguments. Moreover, we can combine this with the above techniques! Memory Efficient Optimizers Use memory-efficient optimizers such as AdaFactor, LION, 8-bit Adam, GaLore However, if our model gets big, we need to use distributed training using multiple GPUs! Ref: (lengthy but well-written article from HF). I just added a running example.]]></summary></entry><entry><title type="html">Introduction to Large Language Models - Course</title><link href="http://localhost:4000/2024/06/15/Intro-to-LLM-course.html" rel="alternate" type="text/html" title="Introduction to Large Language Models - Course" /><published>2024-06-15T00:00:00+05:30</published><updated>2024-06-15T00:00:00+05:30</updated><id>http://localhost:4000/2024/06/15/Intro-to-LLM-course</id><content type="html" xml:base="http://localhost:4000/2024/06/15/Intro-to-LLM-course.html"><![CDATA[<p>For the past three months, I have been quite busy building course materials (lecture slides, graded assignments and coding assignments) for the first offering of the course Introduction to Large Language Models by Prof.Mitesh Khapra. It has been challenging work as we have committed to offer the course in the JAN 2024 term. Every challenge is an opportunity. I have gone through a lot of research papers (a few of those listed in the previous blog posts) carefully. No other opportunities could enable us to dive deep into various concepts in the world of transformers. In the process of building materials, you will get to know all the details of the concept you learn.</p>

<p>Though I created materials, they were thoroughly reviewed/re-shaped by the Prof., a team of research scholars and domain experts at AI4Bharat. It always motivated me to even be more mindful of the words that I put into the lecture slides. Still a lot more to read (of course, it will never end!). Finally, we have built a beautiful course (will keep adding more lectures).</p>

<ul>
  <li>
    <p>You can access all the lecture slides <a href="https://iitm-pod.slides.com/arunprakash_ai/decks/llms">here</a></p>
  </li>
  <li>
    <p>The respective lecture videos <a href="https://youtube.com/playlist?list=PLZ2ps__7DhBbaMNZoyW2Hizl8DG6ikkjo&amp;si=QJeUJBh37ZvP9-fv">here</a>.</p>
  </li>
  <li>
    <p>The course page <a href="http://www.cse.iitm.ac.in/~miteshk/llm-course.html">here</a></p>
  </li>
  <li>
    <p>Programming assignments <a href="https://github.com/Arunprakash-A/LLM-from-scratch-PyTorch">here</a></p>
  </li>
</ul>]]></content><author><name>Arun Prakash A</name></author><category term="LLM" /><summary type="html"><![CDATA[For the past three months, I have been quite busy building course materials (lecture slides, graded assignments and coding assignments) for the first offering of the course Introduction to Large Language Models by Prof.Mitesh Khapra. It has been challenging work as we have committed to offer the course in the JAN 2024 term. Every challenge is an opportunity. I have gone through a lot of research papers (a few of those listed in the previous blog posts) carefully. No other opportunities could enable us to dive deep into various concepts in the world of transformers. In the process of building materials, you will get to know all the details of the concept you learn. Though I created materials, they were thoroughly reviewed/re-shaped by the Prof., a team of research scholars and domain experts at AI4Bharat. It always motivated me to even be more mindful of the words that I put into the lecture slides. Still a lot more to read (of course, it will never end!). Finally, we have built a beautiful course (will keep adding more lectures). You can access all the lecture slides here The respective lecture videos here. The course page here Programming assignments here]]></summary></entry><entry><title type="html">Positional Encoding In Transformers</title><link href="http://localhost:4000/2024/02/02/PositionalEncodings.html" rel="alternate" type="text/html" title="Positional Encoding In Transformers" /><published>2024-02-02T00:00:00+05:30</published><updated>2024-02-02T00:00:00+05:30</updated><id>http://localhost:4000/2024/02/02/PositionalEncodings</id><content type="html" xml:base="http://localhost:4000/2024/02/02/PositionalEncodings.html"><![CDATA[<h1 id="introduction">Introduction</h1>

<p>One question that bothers our mind when we read positional encoding is whether it helps the model or not. It is observed that adding position embeddings only helps marginally in a convolutional neural network as CNN uses relative position embedding (implicitly). It is not the case for transformers as the model is <strong>permutation invariant</strong> without positional information added to the input embeddings. It is also empirically observed that removing positional embeddings (in the transformer architecture) altogether hurts the performance (with certain exceptions like NoPE).</p>

<p>Whenever one reads about positional encoding scheme, one may ask the following questions to understand it better</p>
<ol>
  <li>Absolute or relative?</li>
  <li>Fixed (deterministic) or learnable?</li>
  <li>Injected only at the input block (layer) or all the blocks?</li>
  <li>Inductive or not-inductive (BERT, RoBERTa)?</li>
</ol>

<h2 id="notations">Notations</h2>
<p>We will closely follow the standard notations:</p>
<ul>
  <li>Embedding matrix: $ E \in \mathbb{R}^{len(vocab)} \times d_{model}$</li>
  <li>Token embeddings : $ X \in \mathbb{R}^{n \times d_{model}}$</li>
  <li>Absolute Position embeddings: $P \in \mathbb{R}^{n \times d_{model}}$</li>
  <li>Input to Transformer: $X+P \in \mathbb{R}^{n \times d_{model}}$, $n$ is the length of the sequence</li>
  <li>Projection matrices: $W_Q,W_K,W_V \in \mathbb{R}^{d_{model} \times d_k}, \quad d_q=d_k=d_v$</li>
  <li>Context window size: $n_{max}$</li>
</ul>

<h1 id="absolute-position-encoding">Absolute Position Encoding</h1>
<p>In absolute position encoding, we add the position information to the input embeddings. The position is also a sequence [1,2,3,4,…,n]. Mathematically, we say it is simply a progression (arithmetic or geometric). However, to find the $t$-th position in the progression, we need the first position. That is why it is called absolute position encoding (ape). The embeddings of the positions can be fixed or learnable.</p>

<p>We have the input matrix $X$ and the position matrix $P$ (fixed or learnable) of the same size. Then the input to the transformer is an elementwise addition of these two matrices. Decomposing the pre-attention $A_{pre}$ matrix provides a better insight into how the positional information is being used by the model</p>
<p align="center">
 $$
 \begin{aligned}
 A_{pre} = QK^T &amp;= (X+P)W_Q W_K^T(X+P)^T=(XW_Q+PW_Q)(W_K^TX^T+W_K^TP^T) \\
                &amp; = XW_QW_K^TX^T+PW_QW_K^TX^T+XW_QW_K^TP^T+PW_QW_K^TP^T
 \end{aligned}
 $$
 </p>

<p>The pre-attention score is the sum of 4 terms all involving dot products between the words (the first term), between the positions (the last term) and between the words and positions (2nd and third term). It is reasonable as we expect it to take the position into account while attending to words in a sentence. Therefore, the model is no longer <strong>permutation-invariant</strong>.</p>

<h2 id="fixed-position-encoding">Fixed Position Encoding</h2>

<p>The original transformer architecture followed the fixed sinusoidal embedding. Suppose the embedding dimension $d_{model}=512$, then $j=0,1,2,\cdots,255$. Then each position embedding is a geometric progression of the wavelength of the sinusoid starting from $2 \pi$ (for $j=0$) to $10000 \cdot 2 \pi$ (for $j=255$),</p>
<p align="center">
$$P(pos,2j)=sin(\frac{pos}{10000^{2j/d_{model}}})$$ 
$$P(pos,2j+1)=cos(\frac{pos}{10000^{2j/d_{model}}})$$
</p>
<p>Either one can concatenate or interleave these positions to get the final position embedding. This particular formulation has many possible interpretations. I have written a detailed article on <a href="https://medium.com/p/7e98f32a925f">medium</a>. As mentioned in the paper, for any fixed offset $k$, $P(pos+k)$ can be represented as a linear function of $P(pos)$.For example,</p>

<p>Consider a sentence: “A cat is smaller than an elephant”</p>

<p>Shift the sentence by $k=2$ units: “ You know, a cat is smaller than an elephant”</p>

<p>Suppose the $P_4 \in \mathbb{R}^{1 \times 512}$ be a positional embedding for the second position (for the word “smaller”) in the first sentence, then $P_{4}=P_1W$ or $P_{4}=P_3W$ or $P_{4}=P_iW$  (<a href="https://blog.timodenk.com/linear-relationships-in-the-transformers-positional-encoding/">Proof</a>). That is, one can get the positional embedding for a particular position as a linear function of other position embedding. Therefore, the model learns to attend to the <strong>relative positions</strong> as well. However, how effectively it learns the relative position is a different question (answer: not effective) and it is addressed by Shaw in their  <a href="https://arxiv.org/pdf/1803.02155.pdf">paper on relative positions</a></p>

<h2 id="learnable">Learnable</h2>
<p>The same transformer paper also used <strong>learned absolute position embeddings</strong> by randomly initializing the position embeddings. However, in terms of performance, both fixed and learned embeddings gave the same level of performance. So going with the fixed embedding saves some computational budget.</p>

<p><strong>Improvements</strong>: <a href="https://arxiv.org/pdf/2003.09229.pdf">FLOATER</a> transformer generalizes the sinusoidal PE under the continuous dynamic models. They argued that any PE scheme should be <strong>Inductive</strong> ( handle sequences longer than any sequence seen in the training time), <strong>Data-Driven</strong>(learnable from data) and <strong>Parameter-efficient</strong></p>

<h2 id="drawbacks">Drawbacks</h2>
<ol>
  <li>Despite having the ability to get the embedding for a new position, the performance degrades during inference if it goes beyond the context length (because the model was not trained on the embedding of the new position).</li>
</ol>

<h2 id="shortformer">Shortformer</h2>

<p>One important observation from the above decomposition is that one can add positional information <strong>directly</strong> in the attention layer as well!. The advantages are two-fold</p>
<ol>
  <li>Adding it directly to the input embedding is non-trivial due to $n$ possible encodings for each token</li>
  <li>The dimension of embedding should of $d_{model}$ if added directly, whereas adding it in attention is $d_k$.</li>
</ol>

<p>Shortformer follows this approach by adding positional information (learnable) at the queries and keys but not in values as shown below</p>
<p align="center">
 $$
 \begin{aligned}
 A_{pre} = QK^T &amp;= (X+P)W_Q \quad W_K^T(X+P)^T \\
            A &amp;= Softmax(A_{pre})V 
 \end{aligned}
 $$
 </p>

<h1 id="relative-position-encoding">Relative Position Encoding</h1>

<p>The relative position plays a role when the relative positions in a sentence matter and makes sense in the real world. For example, “the cat eats cheese” makes more sense than “the cheese eats cat” (though grammatically correct). We expect the model to learn relative positions just from the absolute positions is good but may not be efficient Often in practice, a relative position encoding scheme is used <strong>in addition</strong> to the absolute position encoding. However, the relative position of a word in a given sentence of length $n$ has $n$ possible values. It is not necessary to measure the relative position with respect to all other words in a sentence. We may limit it to $k$ words. How do we add a learnable relative position embedding?</p>

<p>Let us re-write the $A_pre$ decomposition equation for a single element in the pre-attention matrix and <strong>add a learnable vector</strong> $r_{j-i} \in \mathbb{R}^{d_k}$  that learns the relative position $j-i$.</p>

<p align="center">
 $$
 \begin{aligned}
 A_{pre}[i,j] = \frac{((x_i+p_i)W_Q)((x_j+p_j)W_K+r_{j-i})^T}{\sqrt{d_k}}                
 \end{aligned}
 $$
 </p>

<p align="center">
  <img align="center" src="/images/PositionEncoding/1_rpe.PNG" />
</p>

<p>This essentially translates to learning the relative position between the key vector and the query vector (it is also ok to add it to the value vector). One can think of $r_{j-i}$ as adding the <strong>inductive bias</strong>. In practice, we do not need to compute the relative position of a word to all other words in the sentence. We may limit it to $\pm k$ words. Therefore, the embedding matrix for the relative position is also fixed.</p>
<p align="center">
 $$
 \begin{aligned}
 A_{pre}[i,j] = \frac{((x_i+p_i)W_Q)((x_jW_K+p_jW_K+r_{j-i})^T}{\sqrt{d_k}}                
 \end{aligned}
 $$
 </p>

<h2 id="illustration">Illustration</h2>

<p>Suppose we have a transformer with a context length $n=20$, and the max-relative position length $k=4$ ($2k+1=9$ if the relation is bidirectional as shown in the figure). Then the absolute position embedding matrix would be of size $20 \times d_{model}$, and the relative position embedding matrix $P_r$ would be of size $9 \times d_k$. What if $(j-i&gt;abs(k))$, for example, $i=2,j=12$? Simply limit it to the upper/lower bound ($k=\pm 4$). Mathematically, $max(-k,min(k,j-i))$.</p>

<p><strong>Note</strong>: Relative positions are <strong>added</strong> separately in the attention sub-layer of <strong>all the layers</strong>. This is not required for APE as it propagates to all the layers along with the input representations.</p>

<h2 id="t5-variation">T5 Variation</h2>
<p>Observe that adding the relative positional info as a learnable vector $(\in \mathbb{R}^{d_k})$ to the key vector ultimately affects the attention score which is a scalar ($\in \mathbb{R}$). Then why don’t we add this scalar directly to the attention score as a learnable parameter? This reduces the number of learnable parameters significantly. This is what exactly followed in the T5 model,</p>
<p align="center">
 $$
 \begin{aligned}
 A_{pre}[i,j] = \frac{((x_i+p_i)W_Q)((x_j+p_j)W_K)^T}{\sqrt{d_k}}+b_{j-1}                
 \end{aligned}
 $$
 </p>

<p>where $b_{j-1} \in \mathbb{R}$ that is shared across heads and layers.</p>

<h2 id="transformer-xl-variation">Transformer-XL variation</h2>
<p><a href="https://arxiv.org/pdf/1901.02860.pdf">Transformer-XL</a> tackles the problem of training longer sequences by introducing recurrence in the hidden states of all the layers. In such a case, using absolute position encoding is not trivial. Therefore, they used RPE with the following modifications</p>

<p align="center">
 $$
 \begin{aligned}
 A_{pre}[i,j] = x_iW_QW_{K1}^Tx_j^T+x_iW_QW_{K2}^Tr_{i-j}^T+u W_{K1}^Tx_j^T+vW_{K2}^Tr_{i-j}^T         
 \end{aligned}
 $$
 </p>
<p>Where the common $W_K$ matrix in the original decomposition is re-parameterized to $W_{K1}$ and $W_{K2}$, new parameter vectors $u,v$ that are <strong>independent of the query position</strong> were introduced. Finally, the relative position $r_{i-j}$ (uses the sinusoidal function for embedding matrix) is used.</p>

<h1 id="rope">RoPE</h1>
<p>It stands for Rotary Position Embeddings. The diagram below shows the vector representation of the position embeddings (in the 2d case) for the first 7 positions. The relative distance as measured (in angles) between $pos=0$ and $pos=1$ are the same for any two positions.</p>
<p align="center">
  <img align="center" src="/images/PositionEncoding/3_rope.PNG" />
</p>
<p>Will that be the case if we consider the last two dimensions (511,512)?. The RoPE proposes to use the rotation matrix in the attention head applied to queries and keys. It rotates two consecutive elements in the <strong>word embedding vector</strong> by using a Block rotation matrix shown below</p>

<p align="center">
  <img align="center" src="/images/PositionEncoding/4_rotationMatrix.PNG" />
</p>

<p>where $m$ is the position index and $\theta_i=10000^{(2-i)/d},i=1,2,\cdots, \frac{d}{2}$. Then the final query-key attention is given by</p>
<p align="center">
 $$
 q_m^Tk_n=(R_{\Theta,m}^d W_qx_m)^T(R_{\Theta,n}^dW_kx_n)      
  $$
 </p>

<p>You may watch this video for intuitive explanations</p>
<div><div class="extensions extensions--video">
  <iframe src="https://www.youtube.com/embed/GQPOtyITy54?rel=0&amp;showinfo=0" frameborder="0" scrolling="no" allowfullscreen=""></iframe>
</div>
</div>

<h1 id="alibi">ALiBi</h1>
<p>Handling sequences larger than the model was trained on using Attention with Linear Bias (ALiBi). The idea is very simple. Just add a bias (hand-crafted) after the query-key product as shown below</p>
<p align="center">
  <img align="center" src="/images/PositionEncoding/2_AliBi.PNG" />
</p>

<p><strong>Interesting question</strong>: how does a model achieve extrapolation
at inference time for sequences that are longer than it saw during training?</p>

<p><strong>Surprising Result</strong>: We show that using ALiBi trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory</p>

<h1 id="nope">NoPE</h1>
<p>How about generalization to downstream tasks? None of the PEs (APE,RPE,RoPE,ALiBI) extrapolate the sequence length when adapted for downstream tasks. The causal attention mask provides some notion of the absolute position of the words. It is argued the postional information is learned by the first hidden layer of the self-attention layers. So, No Positional Encoding (NoPE) is required.</p>

<h1 id="to-do">To do</h1>
<p>Add illustrations, and recent paper xPOS, Is adding positional information for special tokens necessary? How useful it is? <a href="https://openreview.net/pdf?id=09-528y2Fgf">TUPE</a></p>

<h1 id="reference">Reference</h1>
<ol>
  <li><a href="https://arxiv.org/pdf/1803.02155.pdf">Relative Positions</a></li>
  <li><a href="https://arxiv.org/pdf/2003.09229.pdf">FLOATER</a></li>
  <li><a href="https://arxiv.org/pdf/1901.02860.pdf">Transformer-XL</a></li>
  <li><a href="https://arxiv.org/pdf/2104.09864v5.pdf">RoPE</a></li>
  <li><a href="https://arxiv.org/pdf/2108.12409.pdf">ALiBi</a></li>
  <li><a href="https://arxiv.org/pdf/2305.19466.pdf">NoPE</a></li>
</ol>]]></content><author><name>Arun Prakash A</name></author><category term="LLM" /><summary type="html"><![CDATA[Introduction One question that bothers our mind when we read positional encoding is whether it helps the model or not. It is observed that adding position embeddings only helps marginally in a convolutional neural network as CNN uses relative position embedding (implicitly). It is not the case for transformers as the model is permutation invariant without positional information added to the input embeddings. It is also empirically observed that removing positional embeddings (in the transformer architecture) altogether hurts the performance (with certain exceptions like NoPE). Whenever one reads about positional encoding scheme, one may ask the following questions to understand it better Absolute or relative? Fixed (deterministic) or learnable? Injected only at the input block (layer) or all the blocks? Inductive or not-inductive (BERT, RoBERTa)? Notations We will closely follow the standard notations: Embedding matrix: $ E \in \mathbb{R}^{len(vocab)} \times d_{model}$ Token embeddings : $ X \in \mathbb{R}^{n \times d_{model}}$ Absolute Position embeddings: $P \in \mathbb{R}^{n \times d_{model}}$ Input to Transformer: $X+P \in \mathbb{R}^{n \times d_{model}}$, $n$ is the length of the sequence Projection matrices: $W_Q,W_K,W_V \in \mathbb{R}^{d_{model} \times d_k}, \quad d_q=d_k=d_v$ Context window size: $n_{max}$ Absolute Position Encoding In absolute position encoding, we add the position information to the input embeddings. The position is also a sequence [1,2,3,4,…,n]. Mathematically, we say it is simply a progression (arithmetic or geometric). However, to find the $t$-th position in the progression, we need the first position. That is why it is called absolute position encoding (ape). The embeddings of the positions can be fixed or learnable. We have the input matrix $X$ and the position matrix $P$ (fixed or learnable) of the same size. Then the input to the transformer is an elementwise addition of these two matrices. Decomposing the pre-attention $A_{pre}$ matrix provides a better insight into how the positional information is being used by the model $$ \begin{aligned} A_{pre} = QK^T &amp;= (X+P)W_Q W_K^T(X+P)^T=(XW_Q+PW_Q)(W_K^TX^T+W_K^TP^T) \\ &amp; = XW_QW_K^TX^T+PW_QW_K^TX^T+XW_QW_K^TP^T+PW_QW_K^TP^T \end{aligned} $$ The pre-attention score is the sum of 4 terms all involving dot products between the words (the first term), between the positions (the last term) and between the words and positions (2nd and third term). It is reasonable as we expect it to take the position into account while attending to words in a sentence. Therefore, the model is no longer permutation-invariant. Fixed Position Encoding The original transformer architecture followed the fixed sinusoidal embedding. Suppose the embedding dimension $d_{model}=512$, then $j=0,1,2,\cdots,255$. Then each position embedding is a geometric progression of the wavelength of the sinusoid starting from $2 \pi$ (for $j=0$) to $10000 \cdot 2 \pi$ (for $j=255$), $$P(pos,2j)=sin(\frac{pos}{10000^{2j/d_{model}}})$$ $$P(pos,2j+1)=cos(\frac{pos}{10000^{2j/d_{model}}})$$ Either one can concatenate or interleave these positions to get the final position embedding. This particular formulation has many possible interpretations. I have written a detailed article on medium. As mentioned in the paper, for any fixed offset $k$, $P(pos+k)$ can be represented as a linear function of $P(pos)$.For example, Consider a sentence: “A cat is smaller than an elephant” Shift the sentence by $k=2$ units: “ You know, a cat is smaller than an elephant” Suppose the $P_4 \in \mathbb{R}^{1 \times 512}$ be a positional embedding for the second position (for the word “smaller”) in the first sentence, then $P_{4}=P_1W$ or $P_{4}=P_3W$ or $P_{4}=P_iW$ (Proof). That is, one can get the positional embedding for a particular position as a linear function of other position embedding. Therefore, the model learns to attend to the relative positions as well. However, how effectively it learns the relative position is a different question (answer: not effective) and it is addressed by Shaw in their paper on relative positions Learnable The same transformer paper also used learned absolute position embeddings by randomly initializing the position embeddings. However, in terms of performance, both fixed and learned embeddings gave the same level of performance. So going with the fixed embedding saves some computational budget. Improvements: FLOATER transformer generalizes the sinusoidal PE under the continuous dynamic models. They argued that any PE scheme should be Inductive ( handle sequences longer than any sequence seen in the training time), Data-Driven(learnable from data) and Parameter-efficient Drawbacks Despite having the ability to get the embedding for a new position, the performance degrades during inference if it goes beyond the context length (because the model was not trained on the embedding of the new position). Shortformer One important observation from the above decomposition is that one can add positional information directly in the attention layer as well!. The advantages are two-fold Adding it directly to the input embedding is non-trivial due to $n$ possible encodings for each token The dimension of embedding should of $d_{model}$ if added directly, whereas adding it in attention is $d_k$. Shortformer follows this approach by adding positional information (learnable) at the queries and keys but not in values as shown below $$ \begin{aligned} A_{pre} = QK^T &amp;= (X+P)W_Q \quad W_K^T(X+P)^T \\ A &amp;= Softmax(A_{pre})V \end{aligned} $$ Relative Position Encoding The relative position plays a role when the relative positions in a sentence matter and makes sense in the real world. For example, “the cat eats cheese” makes more sense than “the cheese eats cat” (though grammatically correct). We expect the model to learn relative positions just from the absolute positions is good but may not be efficient Often in practice, a relative position encoding scheme is used in addition to the absolute position encoding. However, the relative position of a word in a given sentence of length $n$ has $n$ possible values. It is not necessary to measure the relative position with respect to all other words in a sentence. We may limit it to $k$ words. How do we add a learnable relative position embedding? Let us re-write the $A_pre$ decomposition equation for a single element in the pre-attention matrix and add a learnable vector $r_{j-i} \in \mathbb{R}^{d_k}$ that learns the relative position $j-i$. $$ \begin{aligned} A_{pre}[i,j] = \frac{((x_i+p_i)W_Q)((x_j+p_j)W_K+r_{j-i})^T}{\sqrt{d_k}} \end{aligned} $$ This essentially translates to learning the relative position between the key vector and the query vector (it is also ok to add it to the value vector). One can think of $r_{j-i}$ as adding the inductive bias. In practice, we do not need to compute the relative position of a word to all other words in the sentence. We may limit it to $\pm k$ words. Therefore, the embedding matrix for the relative position is also fixed. $$ \begin{aligned} A_{pre}[i,j] = \frac{((x_i+p_i)W_Q)((x_jW_K+p_jW_K+r_{j-i})^T}{\sqrt{d_k}} \end{aligned} $$ Illustration Suppose we have a transformer with a context length $n=20$, and the max-relative position length $k=4$ ($2k+1=9$ if the relation is bidirectional as shown in the figure). Then the absolute position embedding matrix would be of size $20 \times d_{model}$, and the relative position embedding matrix $P_r$ would be of size $9 \times d_k$. What if $(j-i&gt;abs(k))$, for example, $i=2,j=12$? Simply limit it to the upper/lower bound ($k=\pm 4$). Mathematically, $max(-k,min(k,j-i))$. Note: Relative positions are added separately in the attention sub-layer of all the layers. This is not required for APE as it propagates to all the layers along with the input representations. T5 Variation Observe that adding the relative positional info as a learnable vector $(\in \mathbb{R}^{d_k})$ to the key vector ultimately affects the attention score which is a scalar ($\in \mathbb{R}$). Then why don’t we add this scalar directly to the attention score as a learnable parameter? This reduces the number of learnable parameters significantly. This is what exactly followed in the T5 model, $$ \begin{aligned} A_{pre}[i,j] = \frac{((x_i+p_i)W_Q)((x_j+p_j)W_K)^T}{\sqrt{d_k}}+b_{j-1} \end{aligned} $$ where $b_{j-1} \in \mathbb{R}$ that is shared across heads and layers. Transformer-XL variation Transformer-XL tackles the problem of training longer sequences by introducing recurrence in the hidden states of all the layers. In such a case, using absolute position encoding is not trivial. Therefore, they used RPE with the following modifications $$ \begin{aligned} A_{pre}[i,j] = x_iW_QW_{K1}^Tx_j^T+x_iW_QW_{K2}^Tr_{i-j}^T+u W_{K1}^Tx_j^T+vW_{K2}^Tr_{i-j}^T \end{aligned} $$ Where the common $W_K$ matrix in the original decomposition is re-parameterized to $W_{K1}$ and $W_{K2}$, new parameter vectors $u,v$ that are independent of the query position were introduced. Finally, the relative position $r_{i-j}$ (uses the sinusoidal function for embedding matrix) is used. RoPE It stands for Rotary Position Embeddings. The diagram below shows the vector representation of the position embeddings (in the 2d case) for the first 7 positions. The relative distance as measured (in angles) between $pos=0$ and $pos=1$ are the same for any two positions. Will that be the case if we consider the last two dimensions (511,512)?. The RoPE proposes to use the rotation matrix in the attention head applied to queries and keys. It rotates two consecutive elements in the word embedding vector by using a Block rotation matrix shown below where $m$ is the position index and $\theta_i=10000^{(2-i)/d},i=1,2,\cdots, \frac{d}{2}$. Then the final query-key attention is given by $$ q_m^Tk_n=(R_{\Theta,m}^d W_qx_m)^T(R_{\Theta,n}^dW_kx_n) $$ You may watch this video for intuitive explanations ALiBi Handling sequences larger than the model was trained on using Attention with Linear Bias (ALiBi). The idea is very simple. Just add a bias (hand-crafted) after the query-key product as shown below Interesting question: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? Surprising Result: We show that using ALiBi trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory NoPE How about generalization to downstream tasks? None of the PEs (APE,RPE,RoPE,ALiBI) extrapolate the sequence length when adapted for downstream tasks. The causal attention mask provides some notion of the absolute position of the words. It is argued the postional information is learned by the first hidden layer of the self-attention layers. So, No Positional Encoding (NoPE) is required. To do Add illustrations, and recent paper xPOS, Is adding positional information for special tokens necessary? How useful it is? TUPE Reference Relative Positions FLOATER Transformer-XL RoPE ALiBi NoPE]]></summary></entry><entry><title type="html">Data Pipeline for Large Language models</title><link href="http://localhost:4000/2024/01/26/DataPipelineForLLM.html" rel="alternate" type="text/html" title="Data Pipeline for Large Language models" /><published>2024-01-26T00:00:00+05:30</published><updated>2024-01-26T00:00:00+05:30</updated><id>http://localhost:4000/2024/01/26/DataPipelineForLLM</id><content type="html" xml:base="http://localhost:4000/2024/01/26/DataPipelineForLLM.html"><![CDATA[<p>Data is the fuel for any machine learning model despite the type of learning algorithm (Gradient-based or tree-based) being used. To train and test the model’s generalization capacity, typically, we divide the available samples into three sets: Training, Validation and Test. The typical requirement is that the samples in the test set <strong>should be</strong> different from the training and validation sets. Well, this stringent (of course, valid) requirement could not be met in the case of training and testing LLMs.This is because the datasets are derived from the snapshots of <a href="https://commoncrawl.org/">Common Crawl Web</a>! It is difficult to verify  that the model has not seen the samples used by the benchmarking sets during training.</p>

<p>Moreover, the quality of the large language models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. That requires one to build a data pipeline. Let us first look at the data sources available on the internet.</p>

<h1 id="data-sources">Data Sources</h1>

<p>Fortunately, for the language modelling tasks, all we need is raw texts. One can get it directly from the internet.</p>

<h2 id="wikipedia">Wikipedia</h2>
<ul>
  <li>As of today, it supports articles written in 326 active languages</li>
  <li>English Wikipedia contains about 59 million pages (top) <a href="https://meta.wikimedia.org/wiki/List_of_Wikipedias">Ref</a></li>
  <li>Hindi with 1.2 million pages</li>
  <li>Tamil language with 0.5 million pages</li>
  <li>The following table gives us a better summary of the number of tokens available in each language (as of 2018). The second column (#words) shows the words that occurred at least 5 times in the corpus.</li>
</ul>

<p align="center">
  <img align="center" src="/images/DataPipelineLLMs/1_wiki_data_summary.PNG" />
</p>

<ul>
  <li>
    <p>Overall, 28 languages contain more than 100 million tokens, and 82 languages contain more
than 10 million tokens <a href="https://arxiv.org/pdf/1802.06893.pdf">Ref</a></p>
  </li>
  <li>
    <p><strong>Major Limitation</strong>: Wikipedia is a good source if you are focusing only on English. For languages like Tamil, we have only 0.5 million pages (low resource?). Therefore, one needs to search through the ocean of documents.</p>
  </li>
</ul>

<h2 id="common-crawl">Common Crawl:</h2>
<ul>
  <li>Massive non-curated dataset of webpages in many languages, mixed together in temporal snapshots of the web</li>
  <li>Contains <strong>250 Billion pages</strong> and 3 to 5 Billion pages are being added every month (by sampling the entire web).</li>
  <li>There is little content overlap between monthly snapshots</li>
  <li>Each snapshot is 20 to 30 TB in size (containing uncompressed text)</li>
  <li>Data comes in three formats: raw Web ARChive (WARC), UTF-8 Web Extracted Text (WET), meta-data (WAT)</li>
</ul>

<h1 id="data-pipelines">Data Pipelines</h1>
<p>Common Crawl takes snapshots of everything on the internet. Therefore, to derive a dataset (monolingual or multi-lingual) we have to deploy a pipeline to <strong>remove duplicates, bad words and identify the language in which the document is written</strong>. It is a highly complex and computationally intensive process that costs thousands of dollars. The processes in the pipeline are not standardized and therefore no pipeline is perfect!</p>

<p>In all these pipelines, some sort of language models (n-gram or small language models) are used to decide the quality of the page/article.</p>

<h2 id="fasttext">fastText</h2>
<p>A simple pipeline proposed in the <a href="https://arxiv.org/pdf/1802.06893.pdf">paper</a> shown below contains language identification and deduplication (using hashing algorithms).</p>

<p align="center">
  <img align="center" src="/images/DataPipelineLLMs/2_simple_dp.png" />
</p>

<ul>
  <li>
    <p>Note, however, that LI followed by deduplication or deduplication followed by LI has some impact on the quality of the filtered articles.</p>
  </li>
  <li>
    <p>It uses line-level deduplication.</p>
  </li>
</ul>

<h2 id="ccnet">CCNet</h2>
<ul>
  <li>Describes an automatic data collection pipeline to extract massive high-quality <strong>monolingual</strong> datasets from Common Crawl for a variety of languages</li>
  <li>Follows standard language detection and deduplication process as in fastText</li>
  <li>However, it <strong>preserves document structure</strong> at the paragraph level (instead of line-level language detection as in [1])</li>
  <li>Adds an optional monolingual filtering step that selects documents that are close to high quality sources, like Wikipedia (using a language model to detect it)</li>
  <li>The pipeline is shown below (taken from the paper)</li>
</ul>
<p align="center">
  <img align="center" src="/images/DataPipelineLLMs/3_ccnet_pipeline.PNG" width="800" />
</p>

<ul>
  <li>The 25 TB of data is divided into 1600 shards each of size 5GB.</li>
  <li>Compute <strong>paragraph hashes</strong> from the raw <strong>.WET</strong> files (this will help remove many boilerplate codes in the deduplication step)</li>
  <li>These shards are saved into a JSON file where one entry corresponds to one web page.</li>
  <li>Note that the order of the process has been changed. Deduplication is the first step and then identifying language and applying LM filtering to extract articles from high-quality resources.</li>
</ul>

<h2 id="c4-pipeline-360-billion-tokens">C4 Pipeline (360 Billion tokens)</h2>

<ul>
  <li>Apply <strong>aggressive filtering heuristics</strong> to produce a Colossal Clean Crawled Corpus (C4) of size 750 GB of <strong>natural language text</strong>.</li>
  <li>Derives the following datasets from the C4
    <ol>
      <li>RealNews-like (35 GB)</li>
      <li>WebText-like 17 GB (Used Common Crawl Snapshots from Aug 2018 to Jun 2019)</li>
      <li>Unfiltered C4 (6.1 TB)</li>
    </ol>
  </li>
  <li>The diagram below shows the data flow pipeline. You may take a look at the <a href="https://github.com/tensorflow/datasets/blob/5952d3d60d60e1727786fa7a9a23d24bb463d4d6/tensorflow_datasets/text/c4.py">source code</a> to get more details</li>
</ul>

<p align="center">
  <img align="center" src="/images/DataPipelineLLMs/4_c4_pipeline.PNG" width="800" />
</p>

<ul>
  <li>
    <p>In the subsequent work, the above pipeline is modified to include 100+ other languages by crawling 70+ months of crawl data (amounts to <strong>Petabytes</strong> of data). The new dataset is called <a href="https://arxiv.org/pdf/2010.11934.pdf">mC4</a></p>
  </li>
  <li>
    <p>For example, by using all these snapshots, the number of pages for Tamil language has increased to 3.5 million compared to 0.5 million pages (had we used Wikipedia alone)</p>
  </li>
  <li>
    <p>Another contemporary multilingual dataset is <a href="https://ids-pub.bsz-bw.de/frontdoor/index/index/docId/9021">OSCAR</a>. However, it followed fastText pipeline and did not apply any filtering heuristics.</p>
  </li>
  <li>
    <p><strong>Limitation</strong>: C4 and mC4 applies aggressive filtering. Moreover, would it be helpful to consider WARC files from the common crawl to extract texts? How to make the data as diverse as possible?</p>
  </li>
</ul>

<h2 id="the-pile-340-billion-tokens">The Pile (340 Billion tokens)</h2>
<ul>
  <li>Follows the same pipeline above but uses <strong>WARC</strong> format directly instead of WET format. Uses justText to extract text directly from WARC and pycld2 instead of fastText for language detection. Imposes <strong>stringent quality</strong> requirements. This results in 250 GB of English text.</li>
  <li>Uses Fuzzy deduplication</li>
  <li>Emphasizing on the <strong>quality and diversity</strong>, the dataset contains text from various sources as shown in the table below (taken from the paper)</li>
</ul>

<p align="center">
  <img align="center" src="/images/DataPipelineLLMs/5_the_pile.PNG" /> 
  </p>
<ul>
  <li>For this reason it is called <strong>curated dataset</strong>
    <h2 id="refined-web-5-trillion-tokens">Refined-web (5 Trillion tokens)</h2>
  </li>
  <li>Once again, the quality of data matters! Extract directly from WARC</li>
  <li>Uses both exact and fuzzy deduplication</li>
  <li><strong>Important</strong>: It uses data from the Web only (that is, extracted from CC ). Not from diverse sources as suggested in the pile.</li>
</ul>

<h1 id="end-note">End note</h1>

<ul>
  <li>The list is not exhaustive. I can quote a few more datasets, say RedPajama, MassiveWeb and so on. The underlying data pipeline of all large-scale datasets follows more or less the pipeline described in CCNet or fastText. The released datasets are either curated or derived from CC.</li>
</ul>

<h1 id="questions-to-ponder">Questions to Ponder</h1>
<ul>
  <li>Will we run out of unique high-quality text data soon?</li>
  <li>How scalable is the curation?</li>
  <li>How good are these datasets? How do we measure? (one idea is to see the impact on zero-shot generalization of LLMs as mentioned in [6])</li>
  <li>How do we build a dataset for Indic languages? There are great open-source initiatives by <a href="https://ai4bharat.iitm.ac.in/resources/datasets/">AI4Bharat</a> and other organizations.</li>
</ul>

<h1 id="references">References</h1>
<ol>
  <li><a href="https://arxiv.org/pdf/1802.06893.pdf">Learning word vectors for 157 Languages</a></li>
  <li><a href="https://arxiv.org/pdf/1911.00359.pdf">CCNet</a></li>
  <li><a href="https://arxiv.org/pdf/1910.10683.pdf">T5-C4</a></li>
  <li><a href="https://arxiv.org/pdf/2010.11934.pdf">mT5-mC4</a></li>
  <li><a href="https://arxiv.org/pdf/2101.00027.pdf">Pile</a></li>
  <li><a href="https://arxiv.org/pdf/2306.01116.pdf">Refined Web</a></li>
  <li><a href="https://arxiv.org/pdf/2210.15424.pdf">What Language Model to Train if You Have One Million GPU Hours?</a></li>
</ol>]]></content><author><name>Arun Prakash A</name></author><category term="LLM" /><summary type="html"><![CDATA[Data is the fuel for any machine learning model despite the type of learning algorithm (Gradient-based or tree-based) being used. To train and test the model’s generalization capacity, typically, we divide the available samples into three sets: Training, Validation and Test. The typical requirement is that the samples in the test set should be different from the training and validation sets. Well, this stringent (of course, valid) requirement could not be met in the case of training and testing LLMs.This is because the datasets are derived from the snapshots of Common Crawl Web! It is difficult to verify that the model has not seen the samples used by the benchmarking sets during training. Moreover, the quality of the large language models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. That requires one to build a data pipeline. Let us first look at the data sources available on the internet. Data Sources Fortunately, for the language modelling tasks, all we need is raw texts. One can get it directly from the internet. Wikipedia As of today, it supports articles written in 326 active languages English Wikipedia contains about 59 million pages (top) Ref Hindi with 1.2 million pages Tamil language with 0.5 million pages The following table gives us a better summary of the number of tokens available in each language (as of 2018). The second column (#words) shows the words that occurred at least 5 times in the corpus. Overall, 28 languages contain more than 100 million tokens, and 82 languages contain more than 10 million tokens Ref Major Limitation: Wikipedia is a good source if you are focusing only on English. For languages like Tamil, we have only 0.5 million pages (low resource?). Therefore, one needs to search through the ocean of documents. Common Crawl: Massive non-curated dataset of webpages in many languages, mixed together in temporal snapshots of the web Contains 250 Billion pages and 3 to 5 Billion pages are being added every month (by sampling the entire web). There is little content overlap between monthly snapshots Each snapshot is 20 to 30 TB in size (containing uncompressed text) Data comes in three formats: raw Web ARChive (WARC), UTF-8 Web Extracted Text (WET), meta-data (WAT) Data Pipelines Common Crawl takes snapshots of everything on the internet. Therefore, to derive a dataset (monolingual or multi-lingual) we have to deploy a pipeline to remove duplicates, bad words and identify the language in which the document is written. It is a highly complex and computationally intensive process that costs thousands of dollars. The processes in the pipeline are not standardized and therefore no pipeline is perfect! In all these pipelines, some sort of language models (n-gram or small language models) are used to decide the quality of the page/article. fastText A simple pipeline proposed in the paper shown below contains language identification and deduplication (using hashing algorithms). Note, however, that LI followed by deduplication or deduplication followed by LI has some impact on the quality of the filtered articles. It uses line-level deduplication. CCNet Describes an automatic data collection pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages Follows standard language detection and deduplication process as in fastText However, it preserves document structure at the paragraph level (instead of line-level language detection as in [1]) Adds an optional monolingual filtering step that selects documents that are close to high quality sources, like Wikipedia (using a language model to detect it) The pipeline is shown below (taken from the paper) The 25 TB of data is divided into 1600 shards each of size 5GB. Compute paragraph hashes from the raw .WET files (this will help remove many boilerplate codes in the deduplication step) These shards are saved into a JSON file where one entry corresponds to one web page. Note that the order of the process has been changed. Deduplication is the first step and then identifying language and applying LM filtering to extract articles from high-quality resources. C4 Pipeline (360 Billion tokens) Apply aggressive filtering heuristics to produce a Colossal Clean Crawled Corpus (C4) of size 750 GB of natural language text. Derives the following datasets from the C4 RealNews-like (35 GB) WebText-like 17 GB (Used Common Crawl Snapshots from Aug 2018 to Jun 2019) Unfiltered C4 (6.1 TB) The diagram below shows the data flow pipeline. You may take a look at the source code to get more details In the subsequent work, the above pipeline is modified to include 100+ other languages by crawling 70+ months of crawl data (amounts to Petabytes of data). The new dataset is called mC4 For example, by using all these snapshots, the number of pages for Tamil language has increased to 3.5 million compared to 0.5 million pages (had we used Wikipedia alone) Another contemporary multilingual dataset is OSCAR. However, it followed fastText pipeline and did not apply any filtering heuristics. Limitation: C4 and mC4 applies aggressive filtering. Moreover, would it be helpful to consider WARC files from the common crawl to extract texts? How to make the data as diverse as possible? The Pile (340 Billion tokens) Follows the same pipeline above but uses WARC format directly instead of WET format. Uses justText to extract text directly from WARC and pycld2 instead of fastText for language detection. Imposes stringent quality requirements. This results in 250 GB of English text. Uses Fuzzy deduplication Emphasizing on the quality and diversity, the dataset contains text from various sources as shown in the table below (taken from the paper) For this reason it is called curated dataset Refined-web (5 Trillion tokens) Once again, the quality of data matters! Extract directly from WARC Uses both exact and fuzzy deduplication Important: It uses data from the Web only (that is, extracted from CC ). Not from diverse sources as suggested in the pile. End note The list is not exhaustive. I can quote a few more datasets, say RedPajama, MassiveWeb and so on. The underlying data pipeline of all large-scale datasets follows more or less the pipeline described in CCNet or fastText. The released datasets are either curated or derived from CC. Questions to Ponder Will we run out of unique high-quality text data soon? How scalable is the curation? How good are these datasets? How do we measure? (one idea is to see the impact on zero-shot generalization of LLMs as mentioned in [6]) How do we build a dataset for Indic languages? There are great open-source initiatives by AI4Bharat and other organizations. References Learning word vectors for 157 Languages CCNet T5-C4 mT5-mC4 Pile Refined Web What Language Model to Train if You Have One Million GPU Hours?]]></summary></entry><entry><title type="html">Experimental Settings of Famous Language Models</title><link href="http://localhost:4000/2023/12/26/Comparing-LLM-Models.html" rel="alternate" type="text/html" title="Experimental Settings of Famous Language Models" /><published>2023-12-26T00:00:00+05:30</published><updated>2023-12-26T00:00:00+05:30</updated><id>http://localhost:4000/2023/12/26/Comparing-LLM-Models</id><content type="html" xml:base="http://localhost:4000/2023/12/26/Comparing-LLM-Models.html"><![CDATA[<h2 id="gpt-generative-pre-trained-transformer">GPT (Generative Pre-trained Transformer)</h2>

<ul>
  <li><span style="color:blue"> Pre-training Dataset</span>: Book corpus (0.8 Billion words)</li>
  <li><span style="color:blue"> Unsupervised objective</span>: CLM (Autoregressive)</li>
  <li><span style="color:blue"> Tokenizer</span>: Byte Pair Encoding (BPE)</li>
  <li><span style="color:blue"> Vocab size</span>: 40K</li>
  <li><span style="color:blue"> Architecture</span>: Decoder only (12 Layers)</li>
  <li><span style="color:blue"> Activation</span>: GELU</li>
  <li><span style="color:blue"> Attention</span>: Dense</li>
  <li><span style="color:blue"> FFN</span>: Dense</li>
  <li><span style="color:blue"> Attention mask</span>: Causal Mask</li>
  <li><span style="color:blue"> Positional Encoding</span>: Absolute (learnable)</li>
  <li><span style="color:blue"> Optimizer</span>: Adam</li>
  <li><span style="color:blue"> Training steps</span>: 100 epochs ($2.4 \times 10^6$ steps), ($BS:64 \times T:512$) tokens/step</li>
  <li><span style="color:blue"> Number of parameters</span>: 0.12 Billion</li>
  <li><span style="color:blue"> Evaluated on </span>: 12 Tasks (includes NLI, QA, Comprehension, Classification)</li>
</ul>

<h2 id="bert-bidirectional-encoder-representation-from-transformers">BERT (Bidirectional Encoder Representation from Transformers)</h2>
<ul>
  <li><span style="color:blue"> Pre-training Dataset</span>: Book corpus (0.8B words), English Wikipedia (2.5B words)</li>
  <li><span style="color:blue"> Unsupervised objective</span>: MLM (Autoencoding)</li>
  <li><span style="color:blue"> Tokenizer</span>: WordPiece (similar to BPE)</li>
  <li><span style="color:blue"> Vocab_size</span>: 30K</li>
  <li><span style="color:blue"> Architecture</span>: Encoder only (12 Layers (BERT-base), 24 layers (BERT-large))</li>
  <li><span style="color:blue"> Activation</span>: GELU</li>
  <li><span style="color:blue"> Attention</span>: Dense</li>
  <li><span style="color:blue"> FFN</span>: Dense</li>
  <li><span style="color:blue"> Attention mask</span>: No Mask</li>
  <li><span style="color:blue"> Positional Encoding</span>: Absolute (learnable)</li>
  <li><span style="color:blue"> Optimizer</span>: Adam</li>
  <li><span style="color:blue"> Training steps</span>: 40 epochs ($1 \times 10^6$ steps), ($BS:256 \times T:512$) tokens/step</li>
  <li><span style="color:blue"> Number of parameters</span>: 0.12 Billion (Base) to 0.34 (Large)</li>
  <li><span style="color:blue"> Evaluated on </span>: 11 Tasks (includes NLI, QA, Comprehension, Classification)</li>
</ul>

<h2 id="bart-bidirectional-autoregressive-training">BART (Bidirectional Autoregressive Training)</h2>
<ul>
  <li><span style="color:blue"> Pre-training Dataset</span>: Book corpus,English Wikipedia,CC News, Stories (total:160 GB of text data)</li>
  <li><span style="color:blue"> Unsupervised objective</span>: MLM (token masking, deletion, text infilling, sentence shuffling)</li>
  <li><span style="color:blue"> Tokenizer</span>: BPE</li>
  <li><span style="color:blue"> Vocab_size</span>: 50K</li>
  <li><span style="color:blue"> Architecture</span>: Encoder-Decoder (6-6 Layers (BART-base), 12-12 layers (BART-large))</li>
  <li><span style="color:blue"> Activation</span>: GELU</li>
  <li><span style="color:blue"> Attention</span>: Dense</li>
  <li><span style="color:blue"> FFN</span>: Dense</li>
  <li><span style="color:blue"> Attention mask</span>: Causal mask in Decoder</li>
  <li><span style="color:blue"> Positional Encoding</span>: Absolute (Learneable)</li>
  <li><span style="color:blue"> Optimizer</span>: Adam</li>
  <li><span style="color:blue"> Training steps</span>: BERT-Like : 40 epochs ($1 \times 10^6$ steps), ($BS:256 \times T:512$) tokens/step</li>
  <li><span style="color:blue"> Training steps</span>: RoBERTa Like: $0.5 \times 10^6$ steps, ($BS:8000 \times T:512$) tokens/step</li>
  <li><span style="color:blue"> Number of parameters</span>: 0.13 Billion (Base) to 0.37 (Large)</li>
  <li><span style="color:blue"> Evaluated on </span>: 15 Tasks</li>
</ul>

<h2 id="t5-pushing-the-limits">T5 (Pushing the limits)</h2>
<ul>
  <li><span style="color:blue"> Objective</span>: Extensive study on existing approaches to building (Large) LMs under a unified <strong>(Text-To-Text-Transfer-Learning)</strong> framework.</li>
  <li><span style="color:blue"> Pre-training Dataset</span>: Colossal Clean Crawled Corpus <b>C4</b> (156 Billion tokens)</li>
  <li><span style="color:blue"> Unsupervised objective</span>: MLM-Denoising (predicting a span of missing tokens)</li>
  <li><span style="color:blue"> Tokenizer</span>: Sentencepiece</li>
  <li><span style="color:blue"> Vocab_size</span>: 32K</li>
  <li><span style="color:blue"> Architecture</span>: Encoder-Decoder (tiny, small, medium, large)</li>
  <li><span style="color:blue"> Activation: ReLU</span></li>
  <li><span style="color:blue"> Attention</span>: Dense</li>
  <li><span style="color:blue"> FFN</span>: Dense</li>
  <li><span style="color:blue"> Attention mask</span>: Causal mask in Decoder</li>
  <li><span style="color:blue"> Positional Encoding</span>: Modified position encoding</li>
  <li><span style="color:blue"> Optimizer</span>: Adafactor</li>
  <li><span style="color:blue"> Training steps</span>: <strong>$&lt;\frac{1}{4}$ of epoch</strong> ($2^{19}=524,288$ steps), ($BS:128 \times T:512=65536$) tokens/step</li>
  <li><span style="color:blue"> Number of parameters</span>: 0.13 Billion (small) to 11 Billion (Large)</li>
  <li><span style="color:blue"> Evaluated on</span>: 23 Tasks (GLUE, superGLUE, SQuAD, CNN/DM, WMT)</li>
</ul>

<h2 id="gpt-2">GPT-2</h2>
<ul>
  <li><span style="color:blue"> Pre-training Dataset</span>: Study zero-shot task transfer</li>
  <li><span style="color:blue"> Pre-training Dataset</span>: WebText (40 GB) (less than 19 Billion tokens)</li>
  <li><span style="color:blue"> Unsupervised objective</span>: CLM</li>
  <li><span style="color:blue"> Tokenizer</span>: Bytelevel BPE</li>
  <li><span style="color:blue"> Vocab_size</span>: 50K</li>
  <li><span style="color:blue"> Architecture</span>: Decoder (4 variants)</li>
  <li><span style="color:blue"> Activation</span>: GELU</li>
  <li><span style="color:blue"> Attention</span>: Dense</li>
  <li><span style="color:blue"> FFN</span>: Dense</li>
  <li><span style="color:blue"> Attention mask</span>: Causal mask</li>
  <li><span style="color:blue"> Positional Encoding</span>: Absolute (learnable)</li>
  <li><span style="color:blue"> Optimizer</span>: Adam</li>
  <li><span style="color:blue"> Training steps</span>: not disclosed. Model underfits the dataset (similar to T5) ($BS:64 \times T:1024=65536$) tokens/step</li>
  <li><span style="color:blue"> Number of parameters</span>: 0.12 Billion (small) to 1.5 Billion (Large)</li>
  <li><span style="color:blue"> Evaluated on</span>: 8 Tasks</li>
</ul>

<h2 id="gpt-3">GPT-3</h2>
<ul>
  <li><span style="color:blue"> Objective</span>: Scaling parameters and improve zero-shot, few-shot performance</li>
  <li><span style="color:blue"> Pre-training Dataset</span>: (700 GB) 300 Billion tokens from weighted sampling (60% of Common Crawl filtered (410 B), 22% of WebText-2 (19 B),8% of Books 1(12 B),8% of Books 2 (55 B),2-3% of Wikipedia (3 B))</li>
  <li><span style="color:blue"> Unsupervised objective</span>: CLM</li>
  <li><span style="color:blue"> Tokenizer</span>: Bytelevel BPE</li>
  <li><span style="color:blue"> Vocab_size</span>: 50K</li>
  <li><span style="color:blue"> Architecture</span>: Decoder (12 layers to 96 layers)</li>
  <li><span style="color:blue"> Activation</span>: GELU</li>
  <li><span style="color:blue"> Attention</span>: <a href="https://arxiv.org/pdf/1904.10509.pdf">Sparse Factorization</a></li>
  <li><span style="color:blue"> FFN</span>: Dense</li>
  <li><span style="color:blue"> Attention mask</span>: Causal</li>
  <li><span style="color:blue"> Positional Encoding</span>: Absolute (learnable)</li>
  <li><span style="color:blue"> Optimizer</span>: Adam</li>
  <li><span style="color:blue"> Training steps</span>: (Inferred) 1 epoch = 93570 steps with  3.2M batch size (tokens) ($\frac{300 \times 10^9}{3.2 \times 10^6}$), ($BS:0.5M \rightarrow 3.2M \tokens$). Total training steps could also be inferred from the total compute (PF-days) used to train the model.</li>
  <li><span style="color:blue"> Number of parameters</span>: 0.12 Billion (small) to <strong>175 Billion (Large)</strong></li>
  <li><span style="color:blue"> Evaluated on</span>: 28+ Tasks</li>
  <li><span style="color:blue"> New finding</span>: In-context learning</li>
</ul>

<h2 id="switch-scaling-up-t5-with-moe">Switch (Scaling up T5 with MoE)</h2>
<ul>
  <li><span style="color:blue"> Objective</span>: Scale to Trillion parameters with a reduced <b>computational budget</b> (FLOPS/token)</li>
  <li><span style="color:blue"> Baseline</span>: T5 small (223M), T5 Large (739)</li>
  <li><span style="color:blue"> Pre-training Dataset</span>: Improved <b>C4</b> (156 Billion tokens)</li>
  <li><span style="color:blue"> Unsupervised objective</span>: MLM-Denoising (predicting span of missing tokens)</li>
  <li><span style="color:blue"> Tokenizer</span>: Sentencepiece</li>
  <li><span style="color:blue"> Vocab_size</span>: 32K</li>
  <li><span style="color:blue"> Architecture</span>: Encoder-Decoder (tiny, small, medium, large)</li>
  <li><span style="color:blue"> Activation</span>: ReLU</li>
  <li><span style="color:blue"> Attention</span>: Dense</li>
  <li><span style="color:blue"> FFN</span>: Sparsely activated with MoE (Mixture of Experts) paradigm.</li>
  <li><span style="color:blue"> Number of experts</span>: 64 (base)</li>
  <li><span style="color:blue"> Attention mask</span>: Causal mask in Decoder</li>
  <li><span style="color:blue"> Positional Encoding</span>: Modified position encoding</li>
  <li><span style="color:blue"> Optimizer</span>: Adafactor</li>
  <li><span style="color:blue"> Training steps</span>: <strong>$&lt;\frac{1}{4}$ of epoch</strong> ($2^{19}=524,288$ steps), ($BS:128 \times T:512=65536$) tokens/step</li>
  <li><span style="color:blue"> Number of parameters</span>: 7 Billion (base) to <strong>1571 Billion (Switch-C)</strong></li>
  <li><span style="color:blue"> Speed-up</span>: 7.5x (switch base over dense T5 base), 2.5x (switch large over dense T5 large) Metric for comparison against T5 and MoE Transformers&lt;/span&gt;: Constant FLOPS (T5 base (0.2B Parameters) and Switch base (7B parameters) have same FLOPS/Sequence)</li>
  <li>In the diagram below $e$ denotes the experts. For example, $64e$ means 64 experts. Note carefully that increasing experts beyond a point increases the communication cost and hence decreases speed-up.</li>
</ul>

<p align="center">
 <img align="center" src="/images/Comparing-Famous-LLMs/speedup.JPG" />
</p>

<h2 id="glm-generalized-language-models---unifying-framework">GLM (Generalized Language Models - Unifying Framework)</h2>
<ul>
  <li><span style="color:blue"> Objective</span>: Pose all problems as text generation problem</li>
  <li><span style="color:blue"> Pre-training Dataset</span>: Book corpus, English Wikipedia, CC News-en, OpenWebText-2 (158GB total)</li>
  <li><span style="color:blue"> Unsupervised objective</span>: MLM-text-infilling</li>
  <li><span style="color:blue"> Tokenizer</span>: Sentencepiece</li>
  <li><span style="color:blue"> Vocab_size</span>: 30K</li>
  <li><span style="color:blue"> Architecture</span>: Encoder-Decoder (3 variants)</li>
  <li><span style="color:blue"> Activation</span>: GeLU</li>
  <li><span style="color:blue"> Attention</span>: Dense</li>
  <li><span style="color:blue"> FFN</span>: Dense</li>
  <li><span style="color:blue"> Attention mask</span>: Causal mask in Decoder</li>
  <li><span style="color:blue"> Positional Encoding</span>: <b> 2D positional encoding </b> (i.e., instead of using different positional encoding each for encoder and decoder)</li>
  <li><span style="color:blue"> Training steps</span>: Same as BERT for comparison, half of RoBERTa and BART due to resource constraints.</li>
</ul>

<h2 id="gopher-deepmind">Gopher (Deepmind)</h2>
<ul>
  <li><span style="color:blue"> Objective</span>: Scaling parameters and improve zero-shot, few-shot performance</li>
  <li><span style="color:blue"> Pre-training Dataset</span>: <b>MassiveText (10TB, 2 Trillion tokens) (compiled from MassiveWeb, Books, C4, Github, Wikipedia)</b>, use 300 Billion tokens (12% of the total), evaluated on <b>The pile (800GB)</b></li>
  <li><span style="color:blue"> Unsupervised objective</span>: CLM</li>
  <li><span style="color:blue"> Tokenizer</span>: Bytelevel BPE</li>
  <li><span style="color:blue"> Vocab_size</span>: 50K</li>
  <li><span style="color:blue"> Architecture</span>: Decoder (6 variants, 8 layers to 80 layers)</li>
  <li><span style="color:blue"> Activation</span>: GELU</li>
  <li><span style="color:blue"> Attention</span>: Dense</li>
  <li><span style="color:blue"> FFN</span>: Dense</li>
  <li><span style="color:blue"> Attention mask</span>: Causal</li>
  <li><span style="color:blue"> Floating point Precision</span>: <b>fp32,bfloat16</b></li>
  <li><span style="color:blue"> Positional Encoding</span>: <b>Relative</b></li>
  <li><span style="color:blue"> Optimizer</span>: <strong>Adam (pre-training)/AdaFactor(fine-tuning)</strong></li>
  <li><span style="color:blue"> Number of parameters:</span> 44 Million to <strong>280 Billion</strong></li>
  <li><span style="color:blue"> Evaluated on</span>: 120 tasks</li>
</ul>
<p align="center">
 <img align="center" src="/images/Comparing-Famous-LLMs/evalautionDatasets.JPG" />
</p>

<h2 id="instructgpt-precursor-to-chatgpt">InstructGPT (precursor to chatGPT)</h2>
<ul>
  <li>
    <p><span style="color:blue"> Traditional approach</span>: Train /finetune LMs (<strong>Meena</strong> (2B parameters, 40B tokens) and <strong>LaMDA</strong> (137B parameters, 1.4T tokens)) for chat applications on dialog datasets (like social media conversations) using LM objectives. The responses generated by them were unintended and toxic.</p>
  </li>
  <li>
    <ul>
      <li><span style="color:blue"> Problem</span>: Predicting the next token objective is different from <strong>following user instructions helpfully and safely</strong>.</li>
    </ul>
  </li>
  <li>
    <p><span style="color:blue"> Solution</span>: <strong>Align the model objective to user intent</strong> with human feedback</p>
  </li>
  <li>
    <p><span style="color:blue"> Guiding principles</span>: <strong>Helpful (solve the task)-Honest(do not fabricate or mislead)- Harmless(physical or psychological)</strong></p>
  </li>
  <li>
    <p><span style="color:blue">Pre-trained model</span>: GPT-3</p>
  </li>
  <li><span style="color:blue"> Fine-tuning strategy</span>: RLHF (Reinforcement Learning with Human Feedback) where human feedback acts as a reward signal (Supervised Fine Tuning - Reward Modelling - RLHF with PPO <a href="https://arxiv.org/pdf/1707.06347.pdf">(Proximal Policy Optimization))</a></li>
</ul>

<h2 id="chinchilla-compute-optimal-language-model">Chinchilla (Compute-Optimal Language Model)</h2>

<ul>
  <li><span style="color:blue"> Objective </span>: Find the optimal model size and number of tokens for training a transformer language model under a given compute budget <strong>(similar to power law)</strong></li>
  <li><span style="color:blue"> Core problem </span>: Is the model size of Gopher optimal for the given compute budget ($10^{25}$ FLOPS)?</li>
  <li><span style="color:blue"> Findings</span>: Model size and data size scale equally (doubling the model size (parameters) requires us to double the data size (measured in tokens) to get improved performance) as shown in the figure below (last row)</li>
</ul>
<p align="center">
 <img align="center" src="/images/Comparing-Famous-LLMs/chinchilla.JPG" />
</p>

<ul>
  <li>
    <p><span style="color:blue"> The optimal size of Gopher</span>: According to the new law, the optimal model size (parameters) of Gopher (trained with 1.4 Trillion tokens) is <b>not 280 Billion</b> but 70 Billion.</p>
  </li>
  <li>
    <p><span style="color:blue"> The optimal size of GPT-3</span>: According to the new law, the GPT-3 with 175 Billion parameters should have been trained on 4.2 Trillion tokens (for it to be optimal)</p>
  </li>
  <li><span style="color:blue"> Pre-training Dataset</span>: <b> MassiveText with a slight modification of weightage to account for 1.4 Trillion tokens</b></li>
  <li><span style="color:blue"> Tokenizer</span>: SentencePiece (to represent math and chemistry symbols)</li>
  <li><span style="color:blue"> Vocab_size</span>: 45K</li>
  <li><span style="color:blue"> Architecture</span>: Decoder (80 layers)</li>
  <li><span style="color:blue"> Activation</span>: GELU</li>
  <li><span style="color:blue"> Attention</span>: Dense</li>
  <li><span style="color:blue"> FFN</span>: Dense</li>
  <li><span style="color:blue"> Attention mask</span>: Causal</li>
  <li><span style="color:blue"> Floating point Precision</span>: <b>fp32 (storage),bfloat16 (training)</b></li>
  <li><span style="color:blue"> Positional Encoding</span>: <b>Relative</b></li>
  <li><span style="color:blue"> Optimizer</span>: <strong>AdamW</strong></li>
  <li><span style="color:blue"> Number of parameters:</span> 70 Billion (optimized Gopher :-) )</li>
</ul>

<h2 id="palm-pathways-language-model">PaLM (Pathways Language Model)</h2>
<ul>
  <li><span style="color:blue"> Objective</span>: <strong>Efficiently</strong> Scale the model to 540 Billion parameters using <strong>Pathways</strong> system.</li>
  <li><span style="color:blue"> Pre-training Dataset</span>: 780 Billion tokens (natural language, codes from 24 programming languages, social media conversations)</li>
  <li><span style="color:blue"> Unsupervised objective</span>: CLM</li>
  <li><span style="color:blue"> Tokenizer</span>: SentencePiece</li>
  <li><span style="color:blue"> Vocab_size</span>: <strong>256K</strong></li>
  <li><span style="color:blue"> Architecture</span>: Decoder (32, 64, 118 Layers)</li>
  <li><span style="color:blue"> Activation</span>: SwiGELU (SwishGELU)</li>
  <li><span style="color:blue"> Attention</span>: <strong>Multi-Query Attention</strong> (to improve decoding speed)</li>
  <li><span style="color:blue"> FFN</span>: Dense</li>
  <li><span style="color:blue"> Attention mask</span>: Causal</li>
  <li><span style="color:blue"> Positional Encoding</span>: <strong>RoPE</strong> (works better for long sequences)</li>
  <li><span style="color:blue"> Normalization</span>: Pre-Norm (RMSNorm)</li>
  <li><span style="color:blue"> Optimizer</span>: AdaFactor (without factorization).</li>
  <li><span style="color:blue"> Training steps</span>: 255k (BS: vary from 1M,2M and 4M tokens)</li>
  <li><span style="color:blue"> Evaluated on</span>: 120+ tasks (NLU, MMLU, BIG-Bench,..)</li>
  <li><span style="color:blue"> Hardware</span>: 6144 TPU v4 chips</li>
  <li><span style="color:blue"> Parallelism</span>: Data and Model Parallelism</li>
</ul>

<h2 id="llama">LLaMA</h2>
<ul>
  <li><span style="color:blue"> Motivation</span>: In line with Chinchilla and scaling law, train a smaller model with Trillion tokens (longer time)</li>
  <li><span style="color:blue"> Objective</span>: Optimize the inference budget with smaller models (7B to 65B)</li>
  <li><span style="color:blue"> Pre-training Dataset</span>: Common Crawl (2017 to 2020), C4, Github, Arxiv, StackExchange</li>
  <li><span style="color:blue"> Unsupervised objective</span>: CLM</li>
  <li><span style="color:blue"> Tokenizer</span>: BPE</li>
  <li><span style="color:blue"> Architecture</span>: Decoder (32, 40, 60, 80 Layers)</li>
  <li><span style="color:blue"> Activation</span>: SwiGELU (SwishGELU)</li>
  <li><span style="color:blue"> Attention</span>: <strong>Flash Attention</strong></li>
  <li><span style="color:blue"> FFN</span>: Dense</li>
  <li><span style="color:blue"> Attention mask</span>: Causal</li>
  <li><span style="color:blue"> Positional Encoding</span>: <strong>RoPE</strong> (works better for long sequences)</li>
  <li><span style="color:blue"> Normalization</span>: Pre-Norm (RMSNorm)</li>
  <li><span style="color:blue"> Optimizer</span>: AdamW.</li>
  <li><span style="color:blue"> Training steps</span>: 255k (BS: vary from 1M,2M and 4M tokens)</li>
  <li><span style="color:blue"> Evaluated on</span>: 120+ tasks (NLU, MMLU, BIG-Bench,..)</li>
  <li><span style="color:blue"> Hardware</span>: 2048 A100 GPUs (80GB RAM) (380 tokens/s/GPU)</li>
  <li><span style="color:blue"> Parallelism</span>: Data and Model Parallelism</li>
</ul>

<h2 id="references">References</h2>
<ol>
  <li><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">GPT</a></li>
  <li><a href="https://arxiv.org/pdf/1810.04805.pdf">BERT</a></li>
  <li><a href="https://arxiv.org/pdf/1910.13461.pdf">BART</a></li>
  <li><a href="https://arxiv.org/pdf/1910.10683.pdf">T5</a></li>
  <li><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a></li>
  <li><a href="https://arxiv.org/pdf/2005.14165.pdf">GPT-3</a></li>
  <li><a href="https://arxiv.org/pdf/2101.03961.pdf">Switch</a></li>
  <li><a href="https://arxiv.org/pdf/2103.10360.pdf">GLM</a></li>
  <li><a href="https://arxiv.org/pdf/2112.11446.pdf">Gopher</a></li>
  <li><a href="https://arxiv.org/pdf/2203.02155.pdf">InstructGPT</a></li>
  <li><a href="https://arxiv.org/pdf/2203.15556v1.pdf">Chinchilla</a></li>
  <li><a href="https://arxiv.org/pdf/2204.02311.pdf">PaLM</a></li>
  <li><a href="https://arxiv.org/pdf/2302.13971.pdf">LLaMA</a></li>
</ol>]]></content><author><name>Arun Prakash A</name></author><category term="ML" /><category term="DL" /><category term="LLM" /><summary type="html"><![CDATA[GPT (Generative Pre-trained Transformer) Pre-training Dataset: Book corpus (0.8 Billion words) Unsupervised objective: CLM (Autoregressive) Tokenizer: Byte Pair Encoding (BPE) Vocab size: 40K Architecture: Decoder only (12 Layers) Activation: GELU Attention: Dense FFN: Dense Attention mask: Causal Mask Positional Encoding: Absolute (learnable) Optimizer: Adam Training steps: 100 epochs ($2.4 \times 10^6$ steps), ($BS:64 \times T:512$) tokens/step Number of parameters: 0.12 Billion Evaluated on : 12 Tasks (includes NLI, QA, Comprehension, Classification) BERT (Bidirectional Encoder Representation from Transformers) Pre-training Dataset: Book corpus (0.8B words), English Wikipedia (2.5B words) Unsupervised objective: MLM (Autoencoding) Tokenizer: WordPiece (similar to BPE) Vocab_size: 30K Architecture: Encoder only (12 Layers (BERT-base), 24 layers (BERT-large)) Activation: GELU Attention: Dense FFN: Dense Attention mask: No Mask Positional Encoding: Absolute (learnable) Optimizer: Adam Training steps: 40 epochs ($1 \times 10^6$ steps), ($BS:256 \times T:512$) tokens/step Number of parameters: 0.12 Billion (Base) to 0.34 (Large) Evaluated on : 11 Tasks (includes NLI, QA, Comprehension, Classification) BART (Bidirectional Autoregressive Training) Pre-training Dataset: Book corpus,English Wikipedia,CC News, Stories (total:160 GB of text data) Unsupervised objective: MLM (token masking, deletion, text infilling, sentence shuffling) Tokenizer: BPE Vocab_size: 50K Architecture: Encoder-Decoder (6-6 Layers (BART-base), 12-12 layers (BART-large)) Activation: GELU Attention: Dense FFN: Dense Attention mask: Causal mask in Decoder Positional Encoding: Absolute (Learneable) Optimizer: Adam Training steps: BERT-Like : 40 epochs ($1 \times 10^6$ steps), ($BS:256 \times T:512$) tokens/step Training steps: RoBERTa Like: $0.5 \times 10^6$ steps, ($BS:8000 \times T:512$) tokens/step Number of parameters: 0.13 Billion (Base) to 0.37 (Large) Evaluated on : 15 Tasks T5 (Pushing the limits) Objective: Extensive study on existing approaches to building (Large) LMs under a unified (Text-To-Text-Transfer-Learning) framework. Pre-training Dataset: Colossal Clean Crawled Corpus C4 (156 Billion tokens) Unsupervised objective: MLM-Denoising (predicting a span of missing tokens) Tokenizer: Sentencepiece Vocab_size: 32K Architecture: Encoder-Decoder (tiny, small, medium, large) Activation: ReLU Attention: Dense FFN: Dense Attention mask: Causal mask in Decoder Positional Encoding: Modified position encoding Optimizer: Adafactor Training steps: $&lt;\frac{1}{4}$ of epoch ($2^{19}=524,288$ steps), ($BS:128 \times T:512=65536$) tokens/step Number of parameters: 0.13 Billion (small) to 11 Billion (Large) Evaluated on: 23 Tasks (GLUE, superGLUE, SQuAD, CNN/DM, WMT) GPT-2 Pre-training Dataset: Study zero-shot task transfer Pre-training Dataset: WebText (40 GB) (less than 19 Billion tokens) Unsupervised objective: CLM Tokenizer: Bytelevel BPE Vocab_size: 50K Architecture: Decoder (4 variants) Activation: GELU Attention: Dense FFN: Dense Attention mask: Causal mask Positional Encoding: Absolute (learnable) Optimizer: Adam Training steps: not disclosed. Model underfits the dataset (similar to T5) ($BS:64 \times T:1024=65536$) tokens/step Number of parameters: 0.12 Billion (small) to 1.5 Billion (Large) Evaluated on: 8 Tasks GPT-3 Objective: Scaling parameters and improve zero-shot, few-shot performance Pre-training Dataset: (700 GB) 300 Billion tokens from weighted sampling (60% of Common Crawl filtered (410 B), 22% of WebText-2 (19 B),8% of Books 1(12 B),8% of Books 2 (55 B),2-3% of Wikipedia (3 B)) Unsupervised objective: CLM Tokenizer: Bytelevel BPE Vocab_size: 50K Architecture: Decoder (12 layers to 96 layers) Activation: GELU Attention: Sparse Factorization FFN: Dense Attention mask: Causal Positional Encoding: Absolute (learnable) Optimizer: Adam Training steps: (Inferred) 1 epoch = 93570 steps with 3.2M batch size (tokens) ($\frac{300 \times 10^9}{3.2 \times 10^6}$), ($BS:0.5M \rightarrow 3.2M \tokens$). Total training steps could also be inferred from the total compute (PF-days) used to train the model. Number of parameters: 0.12 Billion (small) to 175 Billion (Large) Evaluated on: 28+ Tasks New finding: In-context learning Switch (Scaling up T5 with MoE) Objective: Scale to Trillion parameters with a reduced computational budget (FLOPS/token) Baseline: T5 small (223M), T5 Large (739) Pre-training Dataset: Improved C4 (156 Billion tokens) Unsupervised objective: MLM-Denoising (predicting span of missing tokens) Tokenizer: Sentencepiece Vocab_size: 32K Architecture: Encoder-Decoder (tiny, small, medium, large) Activation: ReLU Attention: Dense FFN: Sparsely activated with MoE (Mixture of Experts) paradigm. Number of experts: 64 (base) Attention mask: Causal mask in Decoder Positional Encoding: Modified position encoding Optimizer: Adafactor Training steps: $&lt;\frac{1}{4}$ of epoch ($2^{19}=524,288$ steps), ($BS:128 \times T:512=65536$) tokens/step Number of parameters: 7 Billion (base) to 1571 Billion (Switch-C) Speed-up: 7.5x (switch base over dense T5 base), 2.5x (switch large over dense T5 large) Metric for comparison against T5 and MoE Transformers&lt;/span&gt;: Constant FLOPS (T5 base (0.2B Parameters) and Switch base (7B parameters) have same FLOPS/Sequence) In the diagram below $e$ denotes the experts. For example, $64e$ means 64 experts. Note carefully that increasing experts beyond a point increases the communication cost and hence decreases speed-up. GLM (Generalized Language Models - Unifying Framework) Objective: Pose all problems as text generation problem Pre-training Dataset: Book corpus, English Wikipedia, CC News-en, OpenWebText-2 (158GB total) Unsupervised objective: MLM-text-infilling Tokenizer: Sentencepiece Vocab_size: 30K Architecture: Encoder-Decoder (3 variants) Activation: GeLU Attention: Dense FFN: Dense Attention mask: Causal mask in Decoder Positional Encoding: 2D positional encoding (i.e., instead of using different positional encoding each for encoder and decoder) Training steps: Same as BERT for comparison, half of RoBERTa and BART due to resource constraints. Gopher (Deepmind) Objective: Scaling parameters and improve zero-shot, few-shot performance Pre-training Dataset: MassiveText (10TB, 2 Trillion tokens) (compiled from MassiveWeb, Books, C4, Github, Wikipedia), use 300 Billion tokens (12% of the total), evaluated on The pile (800GB) Unsupervised objective: CLM Tokenizer: Bytelevel BPE Vocab_size: 50K Architecture: Decoder (6 variants, 8 layers to 80 layers) Activation: GELU Attention: Dense FFN: Dense Attention mask: Causal Floating point Precision: fp32,bfloat16 Positional Encoding: Relative Optimizer: Adam (pre-training)/AdaFactor(fine-tuning) Number of parameters: 44 Million to 280 Billion Evaluated on: 120 tasks InstructGPT (precursor to chatGPT) Traditional approach: Train /finetune LMs (Meena (2B parameters, 40B tokens) and LaMDA (137B parameters, 1.4T tokens)) for chat applications on dialog datasets (like social media conversations) using LM objectives. The responses generated by them were unintended and toxic. Problem: Predicting the next token objective is different from following user instructions helpfully and safely. Solution: Align the model objective to user intent with human feedback Guiding principles: Helpful (solve the task)-Honest(do not fabricate or mislead)- Harmless(physical or psychological) Pre-trained model: GPT-3 Fine-tuning strategy: RLHF (Reinforcement Learning with Human Feedback) where human feedback acts as a reward signal (Supervised Fine Tuning - Reward Modelling - RLHF with PPO (Proximal Policy Optimization)) Chinchilla (Compute-Optimal Language Model) Objective : Find the optimal model size and number of tokens for training a transformer language model under a given compute budget (similar to power law) Core problem : Is the model size of Gopher optimal for the given compute budget ($10^{25}$ FLOPS)? Findings: Model size and data size scale equally (doubling the model size (parameters) requires us to double the data size (measured in tokens) to get improved performance) as shown in the figure below (last row) The optimal size of Gopher: According to the new law, the optimal model size (parameters) of Gopher (trained with 1.4 Trillion tokens) is not 280 Billion but 70 Billion. The optimal size of GPT-3: According to the new law, the GPT-3 with 175 Billion parameters should have been trained on 4.2 Trillion tokens (for it to be optimal) Pre-training Dataset: MassiveText with a slight modification of weightage to account for 1.4 Trillion tokens Tokenizer: SentencePiece (to represent math and chemistry symbols) Vocab_size: 45K Architecture: Decoder (80 layers) Activation: GELU Attention: Dense FFN: Dense Attention mask: Causal Floating point Precision: fp32 (storage),bfloat16 (training) Positional Encoding: Relative Optimizer: AdamW Number of parameters: 70 Billion (optimized Gopher :-) ) PaLM (Pathways Language Model) Objective: Efficiently Scale the model to 540 Billion parameters using Pathways system. Pre-training Dataset: 780 Billion tokens (natural language, codes from 24 programming languages, social media conversations) Unsupervised objective: CLM Tokenizer: SentencePiece Vocab_size: 256K Architecture: Decoder (32, 64, 118 Layers) Activation: SwiGELU (SwishGELU) Attention: Multi-Query Attention (to improve decoding speed) FFN: Dense Attention mask: Causal Positional Encoding: RoPE (works better for long sequences) Normalization: Pre-Norm (RMSNorm) Optimizer: AdaFactor (without factorization). Training steps: 255k (BS: vary from 1M,2M and 4M tokens) Evaluated on: 120+ tasks (NLU, MMLU, BIG-Bench,..) Hardware: 6144 TPU v4 chips Parallelism: Data and Model Parallelism LLaMA Motivation: In line with Chinchilla and scaling law, train a smaller model with Trillion tokens (longer time) Objective: Optimize the inference budget with smaller models (7B to 65B) Pre-training Dataset: Common Crawl (2017 to 2020), C4, Github, Arxiv, StackExchange Unsupervised objective: CLM Tokenizer: BPE Architecture: Decoder (32, 40, 60, 80 Layers) Activation: SwiGELU (SwishGELU) Attention: Flash Attention FFN: Dense Attention mask: Causal Positional Encoding: RoPE (works better for long sequences) Normalization: Pre-Norm (RMSNorm) Optimizer: AdamW. Training steps: 255k (BS: vary from 1M,2M and 4M tokens) Evaluated on: 120+ tasks (NLU, MMLU, BIG-Bench,..) Hardware: 2048 A100 GPUs (80GB RAM) (380 tokens/s/GPU) Parallelism: Data and Model Parallelism References GPT BERT BART T5 GPT-2 GPT-3 Switch GLM Gopher InstructGPT Chinchilla PaLM LLaMA]]></summary></entry><entry><title type="html">Emergence of Large Language Models (LLMs)</title><link href="http://localhost:4000/2023/12/22/EmergenceOfLLMs.html" rel="alternate" type="text/html" title="Emergence of Large Language Models (LLMs)" /><published>2023-12-22T00:00:00+05:30</published><updated>2023-12-22T00:00:00+05:30</updated><id>http://localhost:4000/2023/12/22/EmergenceOfLLMs</id><content type="html" xml:base="http://localhost:4000/2023/12/22/EmergenceOfLLMs.html"><![CDATA[<h2 id="motivation">Motivation</h2>
<p>Usually, in traditional machine learning, we use numerous approaches (<a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection">model selection</a>) like $K-$fold cross-validation and grid search to find the best model that generalizes well in the real world. However, when it comes to deep learning, it is quite challenging due to compute-cost constraints. It holds for neural language models too.</p>

<p>Researchers used transformer architectures to build language models like GPT, BERT and its variants from 2017 to 2019. These transformer-based models outperformed other neural language models like RNN and its variants in most of the language understanding tasks. It is well known that the performance of deep learning models scales well with the data and compute. The word <em>scaling</em> here not only implies increasing the number of layers or the number of parameters (as commonly perceived) but also increasing the size of the dataset and increasing the duration of training time (steps).</p>

<p>Answering questions on the design choices (hyperparameters) would save us huge training costs! Often, we have constraints on the compute resources. We need to optimally use the resources to build a better model as we do not have the luxury to experiment with many variations. For example,</p>

<ul>
  <li>Is increasing the model’s parameters/dataset size/training duration helpful or harmful to the model’s performance on downstream tasks?</li>
  <li>Whether to Scale the GPT (decoder-only) or BERT (encoder-only) or BART (encoder-decoder)?</li>
  <li>Can we <b>predict</b> all of these even before starting the training process?</li>
  <li>How does loss value change as we scale the model? Can we <b>predict</b> it as a function of other design choices like batch size, training steps and compute budget?</li>
  <li>If we used data parallelism to train the model, then what is the <b>critical batch size</b>?</li>
  <li>For a given computing budget, would you use it to train a larger model for a short time or a smaller model for a long time?</li>
</ul>

<p>To answer questions like these, one may need to carry out an extensive study with the combinatorial combination of design choices and make recommendations out of it as in [1] and the other way is to come up with simple rules [2] that help us make decisions based on the hyperparameters such as batch size, number of parameters and data size. These two seminal works advocated scaling the model, computing resources and data in tandem. That’s how the era of large language models (LLMs) begun.</p>

<p>Note: This page is a <b>running</b> summary of papers (specifically on scaling) that I have gone through in detail.</p>

<h2 id="scaling-an-encoder-decoder-model">Scaling an Encoder-Decoder model</h2>

<p>When it comes to pre-train a language model using transformers, there are a lot of <b>design choices</b> such as choosing the architecture type (encoder, decoder, or encoder-decoder), training objectives (CLM, MLM, PLM, deshuffle, denoise), the <b>scale</b> of the model (number of layers, number of parameters, training duration), training strategies (pre-train+finetune, multitask learning) and fine-tuning strategies (adapters, gradual unfreezing).</p>

<p>All of these could influence the performance of the model in various downstream tasks. So it requires extensive study. Fortunately, there is one such study[1]:”<b> Text-To-Text-Transfer-Transformer (T5)[1] </b>” using a really large dataset called (C4) (of size 750 GB, 156 Billion tokens). Each section of the paper gives you a lot of insights.</p>

<p align="center">
 <img align="center" src="/images/ScalingLLMs/t5.PNG" />
 </p>

<p>Well, the recommendations are the following</p>

<ul>
  <li>It is not the size but the <b> quality of samples</b> in the dataset that matters</li>
  <li>Prefer <b>*encoder-decoder </b> architecture over encoder-only (like BERT) or decoder-only (GPT) architectures</li>
  <li>Pre-training the model with <b> denoising objective </b> performs better across downstream tasks</li>
  <li>If you want to improve the performance of smaller models (for faster inference), then pre-train the model for longer. (A similar line of study <a href="https://arxiv.org/pdf/2203.15556v1.pdf">Chinchilla</a> finds the same)</li>
  <li><b>Underfitting is fine </b> (given the larger dataset, you can barely train for 1 epoch but that is fine)</li>
  <li>(careful) Multi-task training is fine</li>
  <li>Training parameters from scratch is better if you have a large dataset (as in the case of translation tasks)</li>
</ul>

<p>By following these recommendations, they built a large model with <b>11 Billion</b> parameters (called <em>T5-Large</em>) and measured its performance. Unsurprisingly, it outperformed SOTA models in almost all downstream tasks! (at that time).</p>

<p><b>*</b> the current (at the end of the year 2023) trend is to scale the decoder-only models (that is a model that uses autoregressive training)</p>

<p>Well, here comes the decoder-only model (as you might already be aware of GPT-3/4 which is a language model that was released after this study)</p>

<h2 id="scaling-a-decoder-model-language-model">Scaling a Decoder Model (Language Model)</h2>

<p><b> Idea: </b> Tune on a few smaller models and extrapolate to bigger ones</p>

<p><b> Dataset: </b> WebText-2 (40GB,22 Billion tokens) is much smaller than C4 dataset (750GB,156 Billion tokens). Reserve 0.6 Billion toke to compute test loss.<br />
 <b>Siz of vocabulary $|V|$:</b>= 50,257 <br />
 <b>Context window length $n_{ctx}$:</b> 1024 <br /></p>

<p>We are primarily interested in finding the relationship among four important factors to the cross-entropy loss $L$ (usually the loss is averaged over the tokens in the context, sometimes it is intra-token loss).</p>

<ol>
  <li>The number of parameters $N$ (excluding embedding parameters (fixed despite the number of layers in the model) helps establish a cleaner relationship)</li>
  <li>The dataset size $D$ in tokens (roughly equals to number of words)</li>
  <li>Training steps $S$ (1 step of training equals updating the parameters once)</li>
  <li>Compute Budget $C$ in peta-FLOPs-Day (PF-Days=$10^{15} \times 24 \times 3600 = 8.64 \times 10^{19} $ floating point ooperations)</li>
</ol>

<p>Given $d_{model},d_{atten},d_{ff}$, what is the <b> total number of (non-embedding) parameters</b>?</p>

<p align="center">
<span style="font-size:1.5em; line-height:0%">
 $N \approx 2 \times d_{model} \times d_{atten} \times d_{ff}$. 
 </span>
</p>

<p>Typically $d_{ff}$ is much greater than the other two. Therefore, $d_{ff}$ contributes more to the number of parameters. So, can we just increase the number of parameters by increasing $d_{ff}$ (keeping the number of layers constant) and expect a decrease in test loss?</p>

<p>Yes, but it observed that increasing the number of parameters works well with the corresponding <b>increase in the number of layers</b> by keeping $d_{ff}$ fixed in all layers. That is, having one layer with 1 Billion parameters performs poorly compared to having the same 1 Billion parameters with 6 layers (The deeper the better! [2]).</p>

<p align="center">
 <img align="center" src="/images/ScalingLLMs/parameterVsLoss.PNG" />
</p>
<p>Note carefully the use of log-scale for the x-axis (therefore, the relationship is linear on the log-scale).</p>

<p>Given the number of parameters $N$, batch size $B$ and the number of training steps $S$, <b> how much compute $C$ </b> (in FLOPs) do we need?</p>
<p align="center">
 <span style="font-size:1.5em; line-height:0%">
 $C \approx 6BNS$
 </span>
</p>

<p><b>Important:</b> <br />
  If we scale the dataset size, we have to scale the model size and compute size <b>appropriately</b>. For example, if we train a smaller model on a bigger dataset for a longer time, it won’t scale the performance. If we increase the model by $8\times$, then we have to increase the dataset size $5\times$ (Chinchilla differs in this). The figure below shows the smooth performance as we scale these factors</p>

<p align="center">
 <img align="center" src="/images/ScalingLLMs/3.PNG" />
</p>

<p>Suppose we train $L$ different models <b>varying</b> in parameters $N$ ranging from $10^3$ to $10^9$ (figure: right) on a sufficiently large dataset $D$ <b>(fixed)</b>  and compute budget $C$<b>(fixed)</b>. Then the loss could be <b> predicted </b> as a function of $N$</p>
<p align="center">
<span style="font-size:1.5em; line-height:0%">
$L(N)=\bigg(\frac{8.8 \times 10^{13}}{N}\bigg)^{0.076}$
</span>
</p>

<p>Therefore, for $N=10^3$, the loss starts at 6.7886 and for $N=10^9$ (Billion non-embedding parameters), the loss reaches the value of 2.37. Following this trend, it requires 10 Trillion parameters to get the loss value of 1.</p>

<p>Similarly, we can fix $D$ and $N$ (say, $N=10^9$) and vary $C$ (figure: Left). Refer to the paper [2] to know the relationships.</p>

<p><b>Caution</b>: The precise numerical values of $Nc=8.8 \times 10^{13}$, $D_c$ and $C_{min}$ depend on the vocabulary size and tokenization and hence do not have a fundamental meaning.</p>

<h3 id="chinchilla-scaling-law">Chinchilla Scaling Law:</h3>

<ul>
  <li><a href="https://arxiv.org/pdf/2203.15556v1.pdf">Chinchilla</a> recommends scaling the model size (number of parameters) and data size (number of tokens) equally to get optimal performance for the given compute budget. The loss is predictable with the following relationship</li>
</ul>

<h2 id="consequences">Consequences</h2>
<p>Well, you might wonder, is there a study on scaling the encoder-only model with MLM (Masked Language Modeling) objective? Yes, it was studied in the T5 paper. However, authors of T5 advocated for encoder-decoder models with the denoising objective and authors of GPT-2 and scaling law advocated a decoder-only model with autoregressive training (CLM objective). Therefore, people started scaling these two models after 2021 as shown in the figure below [4],</p>
<p align="center">
 <img align="center" src="/images/ScalingLLMs/4.PNG" />
</p>

<p>We can see five major branches from the root of the three. The third one which corresponds to encoder-only models stopped growing after 2021. The fourth one (encoder-decoder) is competing with the fifth one (decoder-only).</p>

<h2 id="conclusion">Conclusion</h2>
<p>After these two extensive studies, one could anticipate that scaling the models will improve the performance and will eventually require less number of samples for fine-tuning (sample efficiency). These studies motivated to building “Large Language Models” having parameters ranging from 11 Billion (T5) to 1 Trillion (GPT-4).</p>

<h2 id="references">References</h2>
<ol>
  <li><a href="https://arxiv.org/abs/1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a></li>
  <li><a href="https://arxiv.org/pdf/2001.08361.pdf">Scaling Laws for Neural Language Models</a></li>
  <li><a href="https://stanford-cs324.github.io/winter2022/assets/pdfs/Scaling%20laws%20pdf.pdf">cs324-Lecture presentation-Stanford</a></li>
  <li><a href="https://arxiv.org/pdf/2304.13712v2.pdf">Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</a></li>
</ol>]]></content><author><name>Arun Prakash A</name></author><category term="ML" /><category term="DL" /><category term="LLM" /><summary type="html"><![CDATA[Motivation Usually, in traditional machine learning, we use numerous approaches (model selection) like $K-$fold cross-validation and grid search to find the best model that generalizes well in the real world. However, when it comes to deep learning, it is quite challenging due to compute-cost constraints. It holds for neural language models too. Researchers used transformer architectures to build language models like GPT, BERT and its variants from 2017 to 2019. These transformer-based models outperformed other neural language models like RNN and its variants in most of the language understanding tasks. It is well known that the performance of deep learning models scales well with the data and compute. The word scaling here not only implies increasing the number of layers or the number of parameters (as commonly perceived) but also increasing the size of the dataset and increasing the duration of training time (steps). Answering questions on the design choices (hyperparameters) would save us huge training costs! Often, we have constraints on the compute resources. We need to optimally use the resources to build a better model as we do not have the luxury to experiment with many variations. For example, Is increasing the model’s parameters/dataset size/training duration helpful or harmful to the model’s performance on downstream tasks? Whether to Scale the GPT (decoder-only) or BERT (encoder-only) or BART (encoder-decoder)? Can we predict all of these even before starting the training process? How does loss value change as we scale the model? Can we predict it as a function of other design choices like batch size, training steps and compute budget? If we used data parallelism to train the model, then what is the critical batch size? For a given computing budget, would you use it to train a larger model for a short time or a smaller model for a long time? To answer questions like these, one may need to carry out an extensive study with the combinatorial combination of design choices and make recommendations out of it as in [1] and the other way is to come up with simple rules [2] that help us make decisions based on the hyperparameters such as batch size, number of parameters and data size. These two seminal works advocated scaling the model, computing resources and data in tandem. That’s how the era of large language models (LLMs) begun. Note: This page is a running summary of papers (specifically on scaling) that I have gone through in detail. Scaling an Encoder-Decoder model When it comes to pre-train a language model using transformers, there are a lot of design choices such as choosing the architecture type (encoder, decoder, or encoder-decoder), training objectives (CLM, MLM, PLM, deshuffle, denoise), the scale of the model (number of layers, number of parameters, training duration), training strategies (pre-train+finetune, multitask learning) and fine-tuning strategies (adapters, gradual unfreezing). All of these could influence the performance of the model in various downstream tasks. So it requires extensive study. Fortunately, there is one such study[1]:” Text-To-Text-Transfer-Transformer (T5)[1] ” using a really large dataset called (C4) (of size 750 GB, 156 Billion tokens). Each section of the paper gives you a lot of insights. Well, the recommendations are the following It is not the size but the quality of samples in the dataset that matters Prefer *encoder-decoder architecture over encoder-only (like BERT) or decoder-only (GPT) architectures Pre-training the model with denoising objective performs better across downstream tasks If you want to improve the performance of smaller models (for faster inference), then pre-train the model for longer. (A similar line of study Chinchilla finds the same) Underfitting is fine (given the larger dataset, you can barely train for 1 epoch but that is fine) (careful) Multi-task training is fine Training parameters from scratch is better if you have a large dataset (as in the case of translation tasks) By following these recommendations, they built a large model with 11 Billion parameters (called T5-Large) and measured its performance. Unsurprisingly, it outperformed SOTA models in almost all downstream tasks! (at that time). * the current (at the end of the year 2023) trend is to scale the decoder-only models (that is a model that uses autoregressive training) Well, here comes the decoder-only model (as you might already be aware of GPT-3/4 which is a language model that was released after this study) Scaling a Decoder Model (Language Model) Idea: Tune on a few smaller models and extrapolate to bigger ones Dataset: WebText-2 (40GB,22 Billion tokens) is much smaller than C4 dataset (750GB,156 Billion tokens). Reserve 0.6 Billion toke to compute test loss. Siz of vocabulary $|V|$:= 50,257 Context window length $n_{ctx}$: 1024 We are primarily interested in finding the relationship among four important factors to the cross-entropy loss $L$ (usually the loss is averaged over the tokens in the context, sometimes it is intra-token loss). The number of parameters $N$ (excluding embedding parameters (fixed despite the number of layers in the model) helps establish a cleaner relationship) The dataset size $D$ in tokens (roughly equals to number of words) Training steps $S$ (1 step of training equals updating the parameters once) Compute Budget $C$ in peta-FLOPs-Day (PF-Days=$10^{15} \times 24 \times 3600 = 8.64 \times 10^{19} $ floating point ooperations) Given $d_{model},d_{atten},d_{ff}$, what is the total number of (non-embedding) parameters? $N \approx 2 \times d_{model} \times d_{atten} \times d_{ff}$. Typically $d_{ff}$ is much greater than the other two. Therefore, $d_{ff}$ contributes more to the number of parameters. So, can we just increase the number of parameters by increasing $d_{ff}$ (keeping the number of layers constant) and expect a decrease in test loss? Yes, but it observed that increasing the number of parameters works well with the corresponding increase in the number of layers by keeping $d_{ff}$ fixed in all layers. That is, having one layer with 1 Billion parameters performs poorly compared to having the same 1 Billion parameters with 6 layers (The deeper the better! [2]). Note carefully the use of log-scale for the x-axis (therefore, the relationship is linear on the log-scale). Given the number of parameters $N$, batch size $B$ and the number of training steps $S$, how much compute $C$ (in FLOPs) do we need? $C \approx 6BNS$ Important: If we scale the dataset size, we have to scale the model size and compute size appropriately. For example, if we train a smaller model on a bigger dataset for a longer time, it won’t scale the performance. If we increase the model by $8\times$, then we have to increase the dataset size $5\times$ (Chinchilla differs in this). The figure below shows the smooth performance as we scale these factors Suppose we train $L$ different models varying in parameters $N$ ranging from $10^3$ to $10^9$ (figure: right) on a sufficiently large dataset $D$ (fixed) and compute budget $C$(fixed). Then the loss could be predicted as a function of $N$ $L(N)=\bigg(\frac{8.8 \times 10^{13}}{N}\bigg)^{0.076}$ Therefore, for $N=10^3$, the loss starts at 6.7886 and for $N=10^9$ (Billion non-embedding parameters), the loss reaches the value of 2.37. Following this trend, it requires 10 Trillion parameters to get the loss value of 1. Similarly, we can fix $D$ and $N$ (say, $N=10^9$) and vary $C$ (figure: Left). Refer to the paper [2] to know the relationships. Caution: The precise numerical values of $Nc=8.8 \times 10^{13}$, $D_c$ and $C_{min}$ depend on the vocabulary size and tokenization and hence do not have a fundamental meaning. Chinchilla Scaling Law: Chinchilla recommends scaling the model size (number of parameters) and data size (number of tokens) equally to get optimal performance for the given compute budget. The loss is predictable with the following relationship Consequences Well, you might wonder, is there a study on scaling the encoder-only model with MLM (Masked Language Modeling) objective? Yes, it was studied in the T5 paper. However, authors of T5 advocated for encoder-decoder models with the denoising objective and authors of GPT-2 and scaling law advocated a decoder-only model with autoregressive training (CLM objective). Therefore, people started scaling these two models after 2021 as shown in the figure below [4], We can see five major branches from the root of the three. The third one which corresponds to encoder-only models stopped growing after 2021. The fourth one (encoder-decoder) is competing with the fifth one (decoder-only). Conclusion After these two extensive studies, one could anticipate that scaling the models will improve the performance and will eventually require less number of samples for fine-tuning (sample efficiency). These studies motivated to building “Large Language Models” having parameters ranging from 11 Billion (T5) to 1 Trillion (GPT-4). References Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Scaling Laws for Neural Language Models cs324-Lecture presentation-Stanford Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond]]></summary></entry><entry><title type="html">Lagrange Multiplier : Intuition via Interaction</title><link href="http://localhost:4000/2023/12/18/Constrained-Optimization.html" rel="alternate" type="text/html" title="Lagrange Multiplier : Intuition via Interaction" /><published>2023-12-18T00:00:00+05:30</published><updated>2023-12-18T00:00:00+05:30</updated><id>http://localhost:4000/2023/12/18/Constrained-Optimization</id><content type="html" xml:base="http://localhost:4000/2023/12/18/Constrained-Optimization.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>I guess you end up being here after coming across the term “constrained optimization” or “Lagrangian” and wanted to understand what “Lagrange multiplier is?”. Well, in this post, I help you understand the foundation of it with interactive plots (you can find plenty of mathematical reasoning on the net). Let’s get straight to the point. Consider the (objective) function, $z=2x+y$. What is the maximum or the minimum value of the function $z$. Well, we can see that the function neither has a maximum nor a minimum. To verify, we take a derivate and set it to zero. Then you will see that the derivative is a constant (the function is changing at a constant pace everywhere in the domain $x$ and $y$).The function is plotted below (rotate,zoom in/out..)</p>
<p align="center">
<iframe scrolling="no" title="plane-cosntrOpt" src="https://www.geogebra.org/material/iframe/id/ypgqkjzb/width/700/height/500/border/888888/sfsb/true/smb/false/stb/false/stbh/false/ai/false/asb/false/sri/false/rc/false/ld/false/sdz/true/ctl/false" width="700px" height="500px" style="border:0px;"> </iframe>
</p>

<h2 id="putting-constraints">Putting constraints</h2>

<p>Now, we can put constraints on the range of values that input variables can take. Say for example, $x^2+y^2=1$. This is an equality constraint (circle). We can have inequality constraints as well (disk). Note that, the constraint is also a function of input variables $(x,y)$. The function $z$ with the constraints on the input variables has a maximum and a minimum. 
The problem is formulated as follows</p>

<center>
$\max \limits_{x,y} z$, subject to (s.t), $x^2+y^2=1$
</center>

<p>How do you find the maximum (or minimum)?.</p>

<h2 id="explorative-approach">Explorative Approach</h2>

<p>Take all the points $(x,y)$ on the unit circle and evaluate the function at each point. The function attains its maximum (minimum) at some point $(x=x_0,y=y_0)$. Move the point $A$ on the circle and observe the change in the function values (better note down the values)</p>
<p align="center">
<iframe scrolling="no" title="ConstrainedOptimization-1" src="https://www.geogebra.org/material/iframe/id/xyrpjdcd/width/700/height/500/border/888888/sfsb/true/smb/false/stb/false/stbh/false/ai/false/asb/false/sri/false/rc/false/ld/false/sdz/true/ctl/false" width="700px" height="500px" style="border:0px;"> </iframe>
</p>
<p>The maximum occurs at $x=0.88,y=0.49$ with the maximum value of the function being 2.24.If you move the point $A$ a bit clockwise or anti-clockwise, the function value decreases from 2.24. Similarly, if you move the point $A$ around the circle (that is, in the feasible region), then you will find the values of $(x,y)$ for which the function attains its minimum.</p>

<p>The explorative approach of finding the maximum by evaluating the function $z$ by considering all possible input points that satisfy the constraints is helpful to get started but inefficient (infeasible). However, we can explore it a bit further to see what we are actually looking for. Could we observe something unique?</p>

<ul>
  <li>The constraint function $x^2+y^2=0$ is living in $\mathbb{R}^2$</li>
  <li>The objective function $z=2x+y$ is in $\mathbb{R}^3$</li>
</ul>

<p>Therefore, let’s plot the <b>contours of the objective </b> function $2x+y=k$ for various values of $k$ (caution: $k \subset z$, that satisfies the constraint).That is for each value of $k$, a plane cuts through the objective function and we project it down on to the coutours of the constraint function (now, both live in $\mathbb{R}^2$). Now, let us move the point $A$ and also display the <b>contours</b> of the objective function $k=2x(A)+y(A)$ where $x(A)$ represents the $x$ coordinate of the point $A$.</p>
<p align="center">
<iframe scrolling="no" title="ConstrainedOptimization-2" src="https://www.geogebra.org/material/iframe/id/nzcvzjbj/width/700/height/500/border/888888/sfsb/true/smb/false/stb/false/stbh/false/ai/false/asb/false/sri/false/rc/false/ld/false/sdz/true/ctl/false" width="700px" height="500px" style="border:0px;"> </iframe>
</p>

<p>What is your observation about the line (contours of the objective function) to the circle (contours of the constraint)? Especially, at the point it maximizes (minimizes) the objective function.</p>

<p><b> The line is tangent to the circle (In general, the contour of the objective function is tangent to the contour of the constraint function at maximum or minimum)</b></p>

<h2 id="analytical-approach">Analytical approach</h2>

<p>With this observation, we can introduce the gradient vector (a simple quantity that we can calculate) of both objective and constraint functions and see how they are related. Once again, move the point $A$ and observe the direction of gradient vectors. Where they are perpendicular to each other? Where they are parallel to each other?</p>
<p align="center">
<iframe scrolling="no" title="GradientConstrainedOpt" src="https://www.geogebra.org/material/iframe/id/zyrkackr/width/700/height/500/border/888888/sfsb/true/smb/false/stb/false/stbh/false/ai/false/asb/false/sri/false/rc/false/ld/false/sdz/true/ctl/false" width="700px" height="500px" style="border:0px;"> </iframe>
</p>
<p>We can see that the gradient of the objective function and the gradient of the constraints are <b>parallel</b> to each other at the point $(x_0,y_0)$ where the function attains its maximum or minimum. Why is that? Think about it for a minute.</p>

<p>So we can write the following important equality</p>

<p align="center"> 
<span style="font-size:1.5em; line-height:0%">
    $ \bbox[5px, border: 2px solid blue]{\nabla f(x_0,y_0)=\lambda \nabla g(x_0,y_0)}$
</span>
</p>
<p><b> Important: </b> The above relation only holds at the point $(x_0,y_0)$</p>

<p>The $\lambda$ is called <b> Lagrange multiplier </b>. This helps us to scale the magnitude of the gradient of the constraint function to match the magnitude of the gradient of the objective function. How do we make use of the equation to find $(x_0,y_0)$ ?</p>

<p align="center">

    $ \nabla_f \begin{bmatrix} 2 \\ 1 \end{bmatrix}=\lambda 
    \nabla_g \begin{bmatrix} 2x_0 \\ 2y_0 \end{bmatrix}$

</p>

<p>Now we can solve for the $(x_0,y_0)$ by equating the gradients. You can take a look at the detailed calculations at <a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/lagrange-multipliers-single-constraint">Khan Academy</a></p>

<p>Now, we can conceptually extend the same for multiple constraints.</p>]]></content><author><name>Arun Prakash A</name></author><category term="Math" /><category term="Geogebra" /><category term="ML" /><summary type="html"><![CDATA[Introduction I guess you end up being here after coming across the term “constrained optimization” or “Lagrangian” and wanted to understand what “Lagrange multiplier is?”. Well, in this post, I help you understand the foundation of it with interactive plots (you can find plenty of mathematical reasoning on the net). Let’s get straight to the point. Consider the (objective) function, $z=2x+y$. What is the maximum or the minimum value of the function $z$. Well, we can see that the function neither has a maximum nor a minimum. To verify, we take a derivate and set it to zero. Then you will see that the derivative is a constant (the function is changing at a constant pace everywhere in the domain $x$ and $y$).The function is plotted below (rotate,zoom in/out..) Putting constraints Now, we can put constraints on the range of values that input variables can take. Say for example, $x^2+y^2=1$. This is an equality constraint (circle). We can have inequality constraints as well (disk). Note that, the constraint is also a function of input variables $(x,y)$. The function $z$ with the constraints on the input variables has a maximum and a minimum. The problem is formulated as follows $\max \limits_{x,y} z$, subject to (s.t), $x^2+y^2=1$ How do you find the maximum (or minimum)?. Explorative Approach Take all the points $(x,y)$ on the unit circle and evaluate the function at each point. The function attains its maximum (minimum) at some point $(x=x_0,y=y_0)$. Move the point $A$ on the circle and observe the change in the function values (better note down the values) The maximum occurs at $x=0.88,y=0.49$ with the maximum value of the function being 2.24.If you move the point $A$ a bit clockwise or anti-clockwise, the function value decreases from 2.24. Similarly, if you move the point $A$ around the circle (that is, in the feasible region), then you will find the values of $(x,y)$ for which the function attains its minimum. The explorative approach of finding the maximum by evaluating the function $z$ by considering all possible input points that satisfy the constraints is helpful to get started but inefficient (infeasible). However, we can explore it a bit further to see what we are actually looking for. Could we observe something unique? The constraint function $x^2+y^2=0$ is living in $\mathbb{R}^2$ The objective function $z=2x+y$ is in $\mathbb{R}^3$ Therefore, let’s plot the contours of the objective function $2x+y=k$ for various values of $k$ (caution: $k \subset z$, that satisfies the constraint).That is for each value of $k$, a plane cuts through the objective function and we project it down on to the coutours of the constraint function (now, both live in $\mathbb{R}^2$). Now, let us move the point $A$ and also display the contours of the objective function $k=2x(A)+y(A)$ where $x(A)$ represents the $x$ coordinate of the point $A$. What is your observation about the line (contours of the objective function) to the circle (contours of the constraint)? Especially, at the point it maximizes (minimizes) the objective function. The line is tangent to the circle (In general, the contour of the objective function is tangent to the contour of the constraint function at maximum or minimum) Analytical approach With this observation, we can introduce the gradient vector (a simple quantity that we can calculate) of both objective and constraint functions and see how they are related. Once again, move the point $A$ and observe the direction of gradient vectors. Where they are perpendicular to each other? Where they are parallel to each other? We can see that the gradient of the objective function and the gradient of the constraints are parallel to each other at the point $(x_0,y_0)$ where the function attains its maximum or minimum. Why is that? Think about it for a minute. So we can write the following important equality $ \bbox[5px, border: 2px solid blue]{\nabla f(x_0,y_0)=\lambda \nabla g(x_0,y_0)}$ Important: The above relation only holds at the point $(x_0,y_0)$ The $\lambda$ is called Lagrange multiplier . This helps us to scale the magnitude of the gradient of the constraint function to match the magnitude of the gradient of the objective function. How do we make use of the equation to find $(x_0,y_0)$ ? $ \nabla_f \begin{bmatrix} 2 \\ 1 \end{bmatrix}=\lambda \nabla_g \begin{bmatrix} 2x_0 \\ 2y_0 \end{bmatrix}$ Now we can solve for the $(x_0,y_0)$ by equating the gradients. You can take a look at the detailed calculations at Khan Academy Now, we can conceptually extend the same for multiple constraints.]]></summary></entry><entry><title type="html">Maximum Likelihood Estimation</title><link href="http://localhost:4000/2023/11/05/MaximumLikelihood-estimation.html" rel="alternate" type="text/html" title="Maximum Likelihood Estimation" /><published>2023-11-05T00:00:00+05:30</published><updated>2023-11-05T00:00:00+05:30</updated><id>http://localhost:4000/2023/11/05/MaximumLikelihood-estimation</id><content type="html" xml:base="http://localhost:4000/2023/11/05/MaximumLikelihood-estimation.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>The concept of estimation of an unknown quantity from the given observations has been a fascinating area of study for many centuries. However, it remains elusive for many beginners. Let’s start with a concept that we are already familiar with. Here is a sequence $x_1=[1,3,5,7,9,11, \times,\cdots,]$. What could be the value of the sequence at index 6 (denoted by $\times$)? You may extend the sequence by adding +2 to the previous value. The answer could be thirteen. How about the value at the index 1000? Well, you can keep extending the sequence or you can come up with the mathematical relation that describes the sequence as a function of index $n$. We could write $x_1(n)=2n+1$. Once we find the functional representation of a sequence from the observation, we may throw away the observations.</p>

<p>Similarly, consider one more sequence of values $x_2 = [1,0.90,0.81,0.74,0.67, \times, \cdots]$. What could be the value at $\times$?. We can see that the element values in the sequence are decreasing slowly. Therefore, I <strong>assume</strong> (by chance) that the underlying function that <strong>(likely)</strong> generated the sequence is of the form $e^{-ct}, \quad t=0,1,2,3,\cdots$. However, I do not know the value of $c$. The value of $c$ can be <strong>estimated</strong> from the sequence using some curve fitting techniques.</p>

<p>What if the elements in the sequence are random numbers? For example, $x_3 = [1,0,1,1,1,1,0,0,1,\cdots]$. We can’t assume any deterministic function to <strong>represent</strong> the sequence. Therefore, We go with a probabilistic function (distribution). In this case, the sequence could be modeled as Bernoulli trails with probabilities of observing $1$ and $0$.</p>

<h2 id="likelihood-function">Likelihood Function</h2>
<p>In all the above cases, we <strong>assumed</strong> the underlying function that could have generated the sequences. Our assumption may turn out to be wrong as we observe more samples. If the elements of a sequence are random samples, then we call the <strong>assumed underlying</strong> distribution function as <strong>likelihood</strong> function. Each sample ,$x_i \in {0,1}$ , in the random sequence $X = [1,0,1,1,0]$ is drawn from the likelihood function</p>
<center>
$P(x_i) = p^{x_i}(1-p)^{1-x_i}$
</center>
<p>What we need to <strong>estimate</strong> is the value of $p$ (that is, the probability of $x_i=1$) from the observed samples. We can also view the observation as <strong>a single trail</strong> with 5 independent Bernoulli random variables. Therefore, we are also interested in the likelihood of the sequence itself. In other words, we are interested in the joint PDF of $(x_0,x_1,x_2,x_3,x_4)$</p>
<center>
$P(x_1,x_2,x_3,x_4,x_5) = ?$
</center>
<p>Before proceeding, we need to make a few assumptions about the random variables</p>

<ul>
  <li>Are the random variables independent?</li>
  <li>Are they coming from the same distribution?</li>
</ul>

<p>If the answer is yes to both questions, then we can write the joint pdf as a product of individual pdfs</p>
<center>
$P(x_0,x_1,x_2,x_3,x_4) = P(x_0)P(x_1)P(x_2)P(x_3)P(x_4) = \prod \limits_{i=0}^n P(x_i)$
</center>

<p>These assumptions can be clubbed together, then it is called <strong>Independent and Identically Distributed (IID)</strong>. If the first assumption does not hold, then we can write the joint PDFs as a product of conditional pdfs. That is,</p>
<center>
$P(x_0,x_1,x_2,x_3,x_4) = P(x_0)P(x_1|x_0)P(x_2|x_1,x_0)P(x_3|x_2,x_1,x_0)P(x_4|x_3,x_2,x_1,x_0)$
</center>

<p>Assuming IID for the random sequence $X = [1,0,1,1,0]$ (Note: The order of the samples in the sequence doesn’t matter because of IID), the joint PDF (likelihood function) that generated the sequence is given by</p>

<center>

$\bbox[5px, border: 2px solid green]{P(x_0,x_1,x_2,x_3,x_4) = \prod \limits_{i=0}^n P(x_i)= \prod \limits_{i=0}^n p^{x_i}(1-p)^{1-x_i}}$                
</center>

<h2 id="maximum-likelihood-function">Maximum Likelihood Function</h2>

<p>Carefully observe the interactive plot given below as we vary the value of $p$ from zero to one in the horizontal axis. The vertical axis is the probability of observing the random sequence $X$ (joint probability). There is exactly one value of $p$ in the interval $[0,1]$ that maximizes the likelihood of the sequence $X$.</p>

<p>The number of elements in the sequence is 5. Therefore, we can vary the value of $n$ from zero to four. Here, $n$ denotes the number of ones observed in the sequence of length 5. For $n=0$, the bias of the coin $p$ is $0$ (the coin never turns into a head). Therefore, the joint probability is 1. We can very the value of $n$ to 0,1,2,3,4,5. For $n=3$, then we can compute the joint PDF as $\prod \limits_{i=0}^n p^{x_i}(1-p)^{1-x_i}=p^3p^2$ .</p>

<iframe scrolling="no" title="MaximumLikelihoodBernouliTrials" src="https://www.geogebra.org/material/iframe/id/bubwuwr6/width/700/height/500/border/888888/sfsb/true/smb/false/stb/false/stbh/false/ai/false/asb/false/sri/false/rc/false/ld/false/sdz/true/ctl/false" width="700px" height="500px" style="border:0px;"> </iframe>

<p>Remember that ALL VALUES (except 0 and 1) of $p$ could have possibly generated the sequence, the value of $p$ which increases the likelihood is what we want. We can cast this as an optimization problem as follows</p>
<center>
$\hat{p} = \underset{p} {\mathrm{argmax}}\prod \limits_{i=0}^n p^{x_i}(1-p)^{1-x_i}$
</center>

<p>Note, we do not care about the maximum value of the function. We are looking for $\mathrm{argmax}$ (that is, the value of $p \in [0,1]$ for which the function attains its maximum).</p>

<p>Well, we can see from the plot that for the given sequence (where the number of ones $n=3$), the function attains its maximum at $\hat{p}=0.6$. That’s great. However, plotting is not possible in many scenarios. So, we need an approach that allows us to estimate  $\hat{p}$ from the given sequence itself.</p>

<h3 id="hey-calculus">Hey Calculus</h3>
<p>You might have already guessed it! We are talking about the minimum and maximum of the function. Taking a derivative of the function and setting it to zero solves the problem of finding $\hat{p}$. Note that we have a joint pdf that contains product terms. Applying $log$ allows us to convert the product terms into sum terms and taking the derivative becomes easier (moreover, $log$ is monotonous and also a concave function). Therefore, we can write</p>

<center>
$
\begin{align*}
\hat{p} &amp;= \prod_{i=1}^n p^{x_i}(1-p)^{(1-x_i)}\\
\ell(\hat{p}) &amp;= \log{p}\sum_{i=1}^n x_i + \log{(1-p)}\sum_{i=1}^n (1-x_i)\\
\dfrac{\partial\ell(\hat{p})}{\partial p} &amp;= \dfrac{\sum_{i=1}^n x_i}{p} - \dfrac{\sum_{i=1}^n (1-x_i)}{1-p} \overset{\text{set}}{=}0\\
\sum_{i=1}^n x_i - p\sum_{i=1}^n x_i &amp;= p\sum_{i=1}^n (1-x_i)\\
\end{align*}
$
</center>

<p>Finally, we have a simple estimation of $p$, as a function of elements in the sequence, that maximizes the likelihood. Since we are using $log$ function to find the $\mathrm{argmax}$, it is also called as <strong>Maximum Log Likelihood Estimation</strong></p>

<center>
$\bbox[5px, border: 2px solid green]{\hat{p} = \dfrac{1}{n}\sum_{i=1}^n x_i}$
</center>

<h3 id="change-of-likelihood-function">Change of likelihood function</h3>

<p>If the elements of the sequence $x_i \in \mathbb{R}$, then we have to use the appropriate density function,say, the Gaussian density function. We need to $\mathrm{argmax}$ of mean $\mu$ and the variance $\sigma$ of the distribution by following the exact steps that we followed for the Bernoulli distribution. Take the derivative of the joint PDF for the mean and variance, and set the result to zero.</p>

<h3 id="point-vs-interval-estimation">Point vs Interval estimation</h3>

<p>The quantities ($p,\mu,\sigma$) that we have computed are called the point estimation. There are is another type called <strong>Interval Estimation</strong> where the estimated value could vary within some interval.</p>

<h3 id="how-good-is-the-estimation">How good is the estimation?</h3>
<p>Comparison is inevitable! We do want to compare our estimated value $\hat{p}$ to the true value of $p$. We want to measure how close it is to the true value. Alas! We do not have the true value of $p$. Often, we can’t know the true value of the quantities we measure. Despite the limitations, we have two statistical performance measures called <strong>Bias</strong> and <strong>Variance</strong>.</p>

<h2 id="maximum-aposteriori-estimation-aka-bayesian-estimation">Maximum Aposteriori Estimation (aka, Bayesian Estimation)</h2>

<p>Assume you are investigating a murder. Initially, there are two suspects $A$ and $B$. The suspect $A$ is good at shooting and martial arts. The suspect $B$ is a cook. Suppose I ask you whom, of these two, do you believe the killer is?. Your initial belief points you to the suspect $A$. Suppose you have come to know that the cook had committed crimes for money in the past (additional information or evidence). Would you change your belief about the suspect $A$?</p>

<p>Mathematically, we can pose this question as follows</p>
<center>
$P(killer=A)=0.8$ (initially)
</center>
<p>After giving you a piece of evidence, then your belief changes</p>
<center>
$P(killer=A|evidance)=0.6$ 
</center>

<p>In a nutshell, we change our <strong>prior</strong> beliefs about something after seeing the <strong>evidence</strong>. This concept could be modeled using Bayes theorem,</p>

<center>
 $\bbox[5px, border: 2px solid green]{P(\theta|x) = \frac{P(x|\theta)}{P(x)} P(\theta)}$
</center>
<p>
where, $P(\theta)$ is our prior belief (unconditional distribution),$P(x|\theta)$ is a likelihood function (which is a function of $\theta$), $P(x)$ is a marginal distribution, $ P(\theta|x) $ is our change in belief (also called Aposterior). In this case, the parameter ($\theta$) that we are trying to estimate itself assumes some <b>parameterized</b> distributions like Beta and Gaussian distribution. If both the prior and posterior follow the same distribution (say, Gaussian), then they are called <b>conjugate prior</b>. 
</p>

<p>Falling back to the Bernoulli (coin tossing) experiment with $x=[1,0,1,1,0]$. In MLE, we estimated $\hat{p}$ as a single point. On the contrary, in Bayesian, we start with an initial belief for the parameter we estimate. We are gonna generalize this for any parameter. Therefore, instead of $\hat{p}$, let’s use $\theta$. Let’s start with some value for $\theta$ as given by the Beta distribution (why? Because we want to change our belief after seeing the evidence, we get the probability value from the parameterized function $f(p,\alpha,\beta)$, and <strong>optimize</strong> the parameters $(\alpha,\beta)$ according to the evidence)</p>

<center>
$f(p,\alpha,\beta)= \mathbf{B}(\alpha,\beta) p^{\alpha-1}(1-p)^{\beta-1}$
</center>

<p>In the above equation $\mathbf{B}(\alpha,\beta)$ is a Beta function (returns a normalization constant, so that the probability integrates to 1). You can play with the app below to see how the $\alpha$ and $\beta$ control the shape of the distribution,</p>

<iframe scrolling="no" title="BetaDistribution" src="https://www.geogebra.org/material/iframe/id/w8xyjzej/width/700/height/500/border/888888/sfsb/true/smb/false/stb/false/stbh/false/ai/false/asb/false/sri/false/rc/false/ld/false/sdz/true/ctl/false" width="700px" height="500px" style="border:0px;"> </iframe>

<p>Surprisingly (fortunately), the other terms resemble Bernoulli’s distribution with the number of heads $n_h=\alpha-1$ and the number of tails $n_t=\beta-1$. Note, however, both $\alpha,\beta \in \mathbb{R}^+$. How do we make sense of this <strong>prior</strong> distribution of $\theta$? Let’s try to understand it</p>

<p>
Suppose, for some reason, you <b>believe</b> that (before tossing the coin to observe the outcomes) the coin is highly biased toward <b>heads</b>! We can encode this belief into the parameter $\alpha$ and $\beta$. In this case, we set the parameter $\alpha$ to a high value than the $\beta$. Set $\alpha=4.3$ and $\beta=1.3$ in the applet and observe the shape. Does this look similar to the one we plotted in MLE section? It is because we are computing the joint pdf given the prior $P(x_0,x_1,\cdots,x_4|(\alpha=4.3,\beta=1.3))$
</p>

<p>

Now you toss the coin 5 times and observe the outcomes in each trial: $x=[0,1,0,1,0]$. This is a surprise! The trial produced more zeros than ones. We have to adjust our beliefs after seeing the evidence. How do we do that? We have to adjust the value of $\alpha$ and $\beta$. How much? That we need to decide as follows (we will drop the $p(x)$ in the denominator)
</p>
<p align="center">
$
\begin{align*}
P(\theta|x) &amp;= \frac{P(x|\theta)}{P(x)}P(\theta) \\
            &amp;\propto \bigg(\prod \limits_{i=1}^n p^{x_i} (1-p)^{1-x_i} \bigg) \cdot \bigg(p^{\alpha-1} (1-p)^{\beta-1} \bigg) \\
            &amp;\propto \prod \limits_{i=1}^n p^{x_i+\alpha-1} (1-p)^{1-x_i+\beta-1} \\
            &amp; \propto p^{\alpha+n_h-1} (1-p)^{\beta+n_t-1} \quad (\because n_h=\sum \limits_{i=1}^n x_i, n_t = \sum \limits_{i=1}^n (1-x_i) )

\end{align*}
$
</p>

<p>So, we can see that the Aposteriori belief is obtained by modifying a prior by adding $n_h,n_t$ that are functions of data (evidence) to the parameter $(\alpha,\beta)$. Since the given sequence has more zeros (tails), it increases the value of $\beta$ by 3 and the value of $\alpha$ by 2. Therefore, the updated value of $\alpha=6.3$ and the updated value of $\beta=4.3$. This shifts the peak of the distribution to the left!</p>]]></content><author><name>Arun Prakash A</name></author><category term="ML" /><category term="Math" /><summary type="html"><![CDATA[Introduction The concept of estimation of an unknown quantity from the given observations has been a fascinating area of study for many centuries. However, it remains elusive for many beginners. Let’s start with a concept that we are already familiar with. Here is a sequence $x_1=[1,3,5,7,9,11, \times,\cdots,]$. What could be the value of the sequence at index 6 (denoted by $\times$)? You may extend the sequence by adding +2 to the previous value. The answer could be thirteen. How about the value at the index 1000? Well, you can keep extending the sequence or you can come up with the mathematical relation that describes the sequence as a function of index $n$. We could write $x_1(n)=2n+1$. Once we find the functional representation of a sequence from the observation, we may throw away the observations. Similarly, consider one more sequence of values $x_2 = [1,0.90,0.81,0.74,0.67, \times, \cdots]$. What could be the value at $\times$?. We can see that the element values in the sequence are decreasing slowly. Therefore, I assume (by chance) that the underlying function that (likely) generated the sequence is of the form $e^{-ct}, \quad t=0,1,2,3,\cdots$. However, I do not know the value of $c$. The value of $c$ can be estimated from the sequence using some curve fitting techniques. What if the elements in the sequence are random numbers? For example, $x_3 = [1,0,1,1,1,1,0,0,1,\cdots]$. We can’t assume any deterministic function to represent the sequence. Therefore, We go with a probabilistic function (distribution). In this case, the sequence could be modeled as Bernoulli trails with probabilities of observing $1$ and $0$. Likelihood Function In all the above cases, we assumed the underlying function that could have generated the sequences. Our assumption may turn out to be wrong as we observe more samples. If the elements of a sequence are random samples, then we call the assumed underlying distribution function as likelihood function. Each sample ,$x_i \in {0,1}$ , in the random sequence $X = [1,0,1,1,0]$ is drawn from the likelihood function $P(x_i) = p^{x_i}(1-p)^{1-x_i}$ What we need to estimate is the value of $p$ (that is, the probability of $x_i=1$) from the observed samples. We can also view the observation as a single trail with 5 independent Bernoulli random variables. Therefore, we are also interested in the likelihood of the sequence itself. In other words, we are interested in the joint PDF of $(x_0,x_1,x_2,x_3,x_4)$ $P(x_1,x_2,x_3,x_4,x_5) = ?$ Before proceeding, we need to make a few assumptions about the random variables Are the random variables independent? Are they coming from the same distribution? If the answer is yes to both questions, then we can write the joint pdf as a product of individual pdfs $P(x_0,x_1,x_2,x_3,x_4) = P(x_0)P(x_1)P(x_2)P(x_3)P(x_4) = \prod \limits_{i=0}^n P(x_i)$ These assumptions can be clubbed together, then it is called Independent and Identically Distributed (IID). If the first assumption does not hold, then we can write the joint PDFs as a product of conditional pdfs. That is, $P(x_0,x_1,x_2,x_3,x_4) = P(x_0)P(x_1|x_0)P(x_2|x_1,x_0)P(x_3|x_2,x_1,x_0)P(x_4|x_3,x_2,x_1,x_0)$ Assuming IID for the random sequence $X = [1,0,1,1,0]$ (Note: The order of the samples in the sequence doesn’t matter because of IID), the joint PDF (likelihood function) that generated the sequence is given by $\bbox[5px, border: 2px solid green]{P(x_0,x_1,x_2,x_3,x_4) = \prod \limits_{i=0}^n P(x_i)= \prod \limits_{i=0}^n p^{x_i}(1-p)^{1-x_i}}$ Maximum Likelihood Function Carefully observe the interactive plot given below as we vary the value of $p$ from zero to one in the horizontal axis. The vertical axis is the probability of observing the random sequence $X$ (joint probability). There is exactly one value of $p$ in the interval $[0,1]$ that maximizes the likelihood of the sequence $X$. The number of elements in the sequence is 5. Therefore, we can vary the value of $n$ from zero to four. Here, $n$ denotes the number of ones observed in the sequence of length 5. For $n=0$, the bias of the coin $p$ is $0$ (the coin never turns into a head). Therefore, the joint probability is 1. We can very the value of $n$ to 0,1,2,3,4,5. For $n=3$, then we can compute the joint PDF as $\prod \limits_{i=0}^n p^{x_i}(1-p)^{1-x_i}=p^3p^2$ . Remember that ALL VALUES (except 0 and 1) of $p$ could have possibly generated the sequence, the value of $p$ which increases the likelihood is what we want. We can cast this as an optimization problem as follows $\hat{p} = \underset{p} {\mathrm{argmax}}\prod \limits_{i=0}^n p^{x_i}(1-p)^{1-x_i}$ Note, we do not care about the maximum value of the function. We are looking for $\mathrm{argmax}$ (that is, the value of $p \in [0,1]$ for which the function attains its maximum). Well, we can see from the plot that for the given sequence (where the number of ones $n=3$), the function attains its maximum at $\hat{p}=0.6$. That’s great. However, plotting is not possible in many scenarios. So, we need an approach that allows us to estimate $\hat{p}$ from the given sequence itself. Hey Calculus You might have already guessed it! We are talking about the minimum and maximum of the function. Taking a derivative of the function and setting it to zero solves the problem of finding $\hat{p}$. Note that we have a joint pdf that contains product terms. Applying $log$ allows us to convert the product terms into sum terms and taking the derivative becomes easier (moreover, $log$ is monotonous and also a concave function). Therefore, we can write $ \begin{align*} \hat{p} &amp;= \prod_{i=1}^n p^{x_i}(1-p)^{(1-x_i)}\\ \ell(\hat{p}) &amp;= \log{p}\sum_{i=1}^n x_i + \log{(1-p)}\sum_{i=1}^n (1-x_i)\\ \dfrac{\partial\ell(\hat{p})}{\partial p} &amp;= \dfrac{\sum_{i=1}^n x_i}{p} - \dfrac{\sum_{i=1}^n (1-x_i)}{1-p} \overset{\text{set}}{=}0\\ \sum_{i=1}^n x_i - p\sum_{i=1}^n x_i &amp;= p\sum_{i=1}^n (1-x_i)\\ \end{align*} $ Finally, we have a simple estimation of $p$, as a function of elements in the sequence, that maximizes the likelihood. Since we are using $log$ function to find the $\mathrm{argmax}$, it is also called as Maximum Log Likelihood Estimation $\bbox[5px, border: 2px solid green]{\hat{p} = \dfrac{1}{n}\sum_{i=1}^n x_i}$ Change of likelihood function If the elements of the sequence $x_i \in \mathbb{R}$, then we have to use the appropriate density function,say, the Gaussian density function. We need to $\mathrm{argmax}$ of mean $\mu$ and the variance $\sigma$ of the distribution by following the exact steps that we followed for the Bernoulli distribution. Take the derivative of the joint PDF for the mean and variance, and set the result to zero. Point vs Interval estimation The quantities ($p,\mu,\sigma$) that we have computed are called the point estimation. There are is another type called Interval Estimation where the estimated value could vary within some interval. How good is the estimation? Comparison is inevitable! We do want to compare our estimated value $\hat{p}$ to the true value of $p$. We want to measure how close it is to the true value. Alas! We do not have the true value of $p$. Often, we can’t know the true value of the quantities we measure. Despite the limitations, we have two statistical performance measures called Bias and Variance. Maximum Aposteriori Estimation (aka, Bayesian Estimation) Assume you are investigating a murder. Initially, there are two suspects $A$ and $B$. The suspect $A$ is good at shooting and martial arts. The suspect $B$ is a cook. Suppose I ask you whom, of these two, do you believe the killer is?. Your initial belief points you to the suspect $A$. Suppose you have come to know that the cook had committed crimes for money in the past (additional information or evidence). Would you change your belief about the suspect $A$? Mathematically, we can pose this question as follows $P(killer=A)=0.8$ (initially) After giving you a piece of evidence, then your belief changes $P(killer=A|evidance)=0.6$ In a nutshell, we change our prior beliefs about something after seeing the evidence. This concept could be modeled using Bayes theorem, $\bbox[5px, border: 2px solid green]{P(\theta|x) = \frac{P(x|\theta)}{P(x)} P(\theta)}$ where, $P(\theta)$ is our prior belief (unconditional distribution),$P(x|\theta)$ is a likelihood function (which is a function of $\theta$), $P(x)$ is a marginal distribution, $ P(\theta|x) $ is our change in belief (also called Aposterior). In this case, the parameter ($\theta$) that we are trying to estimate itself assumes some parameterized distributions like Beta and Gaussian distribution. If both the prior and posterior follow the same distribution (say, Gaussian), then they are called conjugate prior. Falling back to the Bernoulli (coin tossing) experiment with $x=[1,0,1,1,0]$. In MLE, we estimated $\hat{p}$ as a single point. On the contrary, in Bayesian, we start with an initial belief for the parameter we estimate. We are gonna generalize this for any parameter. Therefore, instead of $\hat{p}$, let’s use $\theta$. Let’s start with some value for $\theta$ as given by the Beta distribution (why? Because we want to change our belief after seeing the evidence, we get the probability value from the parameterized function $f(p,\alpha,\beta)$, and optimize the parameters $(\alpha,\beta)$ according to the evidence) $f(p,\alpha,\beta)= \mathbf{B}(\alpha,\beta) p^{\alpha-1}(1-p)^{\beta-1}$ In the above equation $\mathbf{B}(\alpha,\beta)$ is a Beta function (returns a normalization constant, so that the probability integrates to 1). You can play with the app below to see how the $\alpha$ and $\beta$ control the shape of the distribution, Surprisingly (fortunately), the other terms resemble Bernoulli’s distribution with the number of heads $n_h=\alpha-1$ and the number of tails $n_t=\beta-1$. Note, however, both $\alpha,\beta \in \mathbb{R}^+$. How do we make sense of this prior distribution of $\theta$? Let’s try to understand it Suppose, for some reason, you believe that (before tossing the coin to observe the outcomes) the coin is highly biased toward heads! We can encode this belief into the parameter $\alpha$ and $\beta$. In this case, we set the parameter $\alpha$ to a high value than the $\beta$. Set $\alpha=4.3$ and $\beta=1.3$ in the applet and observe the shape. Does this look similar to the one we plotted in MLE section? It is because we are computing the joint pdf given the prior $P(x_0,x_1,\cdots,x_4|(\alpha=4.3,\beta=1.3))$ Now you toss the coin 5 times and observe the outcomes in each trial: $x=[0,1,0,1,0]$. This is a surprise! The trial produced more zeros than ones. We have to adjust our beliefs after seeing the evidence. How do we do that? We have to adjust the value of $\alpha$ and $\beta$. How much? That we need to decide as follows (we will drop the $p(x)$ in the denominator) $ \begin{align*} P(\theta|x) &amp;= \frac{P(x|\theta)}{P(x)}P(\theta) \\ &amp;\propto \bigg(\prod \limits_{i=1}^n p^{x_i} (1-p)^{1-x_i} \bigg) \cdot \bigg(p^{\alpha-1} (1-p)^{\beta-1} \bigg) \\ &amp;\propto \prod \limits_{i=1}^n p^{x_i+\alpha-1} (1-p)^{1-x_i+\beta-1} \\ &amp; \propto p^{\alpha+n_h-1} (1-p)^{\beta+n_t-1} \quad (\because n_h=\sum \limits_{i=1}^n x_i, n_t = \sum \limits_{i=1}^n (1-x_i) ) \end{align*} $ So, we can see that the Aposteriori belief is obtained by modifying a prior by adding $n_h,n_t$ that are functions of data (evidence) to the parameter $(\alpha,\beta)$. Since the given sequence has more zeros (tails), it increases the value of $\beta$ by 3 and the value of $\alpha$ by 2. Therefore, the updated value of $\alpha=6.3$ and the updated value of $\beta=4.3$. This shifts the peak of the distribution to the left!]]></summary></entry><entry><title type="html">Representation Learning</title><link href="http://localhost:4000/2023/09/20/RepresentationLearning.html" rel="alternate" type="text/html" title="Representation Learning" /><published>2023-09-20T00:00:00+05:30</published><updated>2023-09-20T00:00:00+05:30</updated><id>http://localhost:4000/2023/09/20/RepresentationLearning</id><content type="html" xml:base="http://localhost:4000/2023/09/20/RepresentationLearning.html"><![CDATA[<h2 id="motivation">Motivation</h2>
<p>Let’s start with a simple question. How do you <strong>represent</strong> a number on a real line?. That’s straightforward. How do you <strong>represent</strong> a 2D point $\begin{bmatrix}x \ y \end{bmatrix}$ in a coordinate system?. Well, we use two real lines that are <strong>orthogonal</strong> to each other. Move $x$ unit on the axis and $y$ unit on the $y$ axis, then the location denotes the point. For latter convenience, we call $x$ and $y$ as the coefficients of the standard (unit) basis vectors $\mathbf{u_x}$ and $\mathbf{u_y}$.</p>

<p>Suppose you are given 5 points $x_1,x_2,\cdots,x_5$ as shown in the app below. Could you represent at least four of them with a single number(coefficient) per point?. Hmm, ya, there is a pattern! Four points are lying on the line (check the “show line” checkbox to see the line) and only one point is not on the line. In other words, the four points are linearly dependent. That is, the four points are lying on the lower dimensional subspace $\mathbb{R}$ of the original vector space $\mathbb{R}^2$.</p>

<p>It means that we can reach the four points by scaling the unit vector $\mathbf{w}$ by an appropriate constant $c$. In linear algebra terminology, we can say the vector $\mathbf{w}$ is a new basis vector. Here, we call $\mathbf{w}$ as the <strong>representation</strong> for these four points. Once we have the $\mathbf{w}$ and coefficients $c_1,c_2,c_3$ and $c_4$, we can represent all these points in the new basis. We are representing the original data points of dim=2, $x_i \in \mathbb{R}^2$, as points in dim=1, $c_i \in \mathbb{R}$. So, we may call this process as dimensionality reduction! Note however that the dim of basis vector $\mathbf{w}$ is still the same as dim of the original points $(\mathbb{R}^2)$. In general, the dimension of the representation( aka: eigen, principal, basis) vectors is always equal to the dim of the original points.</p>

<p>However, how do we represent the point $x_5$ using $c_5\mathbf{w}$? It is not possible as the point is not on the line. Suppose we relax the requirement that we need to <strong>exactly</strong> represent the point. Instead, we want to represent the point as close as possible. For that, we need to first <strong>project</strong> the point onto the line (subspace). The red arrow in the applet denotes the projected vector $\mathbb{p}$ and the point $x_5’$ is the projection of the point $x_5$ onto the line. The dotted segment represents the error between the projected point and the actual point.</p>

<p><em>Activity:</em> Check the <code class="language-plaintext highlighter-rouge">show Circle</code> check box in the app. Then move the point $x_5$ on the circle and observe the change in error value as you move along the circle. When does the error become high (that is, error=1)?</p>

<iframe scrolling="no" title="Projection-PCA" src="https://www.geogebra.org/material/iframe/id/rhce9wch/width/700/height/500/border/888888/sfsb/true/smb/false/stb/false/stbh/false/ai/false/asb/false/sri/false/rc/false/ld/false/sdz/true/ctl/false" width="700px" height="500px" style="border:0px;"> </iframe>

<p>If we have more points $x_6,x_7,\cdots$, then we project all those points onto the subspace spanned by $\mathbf{w}$ (the line) to calculate the appropriate coefficient values, $c_i$. Note that the vector $\mathbf{w}$ is a unit vector. Therefore, to represent any point $x_i$, the scaling factor $c_i$ is given by $c_i = \mathbf{x_i}^T\mathbf{w}$ and the project vector is</p>
<center>
 $\mathbb{p_i}=c_i\mathbb{w}$
 </center>

<h2 id="principled-approach">Principled Approach</h2>

<p>Well, in the above setup, we assumed we are given with $\mathbf{w}$ (or we could have calculated it, not a big deal). Suppose we receive a few more points as shown in the figure below,</p>
<p align="center">
  <img align="center" src="https://lh3.googleusercontent.com/d/1fiSjRrD8FN0Zb_Y3gBtxumk2Ob_LdrFn" />
</p>

<p>Now, many points are outside the given line. That is, if we sum the error for each data point, then the average error might be high. The error is the function of the line we choose (that is, $\mathbb{w}$). However, there could be a better line ($\mathbb{w}$) that minimizes the average error. How do we find it?</p>

<p>In general, we have $n$ data points and each data point $x_i \in \mathbb{R}^d$. We are interested in finding a set of $\mathbf{w_i} \in \mathbb{R}^d$ and the scalars $c_i \in \mathbb{R}$ such that it minimizes the average error as much as possible. Again, in linear algebra terminology, we are looking for new bases that span the low-dimensional subspace. By default,  the data points are represented in $\mathbb{R}^d$ with $d$ canonical (orthogonal) bases (coordinates). Our objective is to come up with $d’ \leq d$ number of bases (coordinates) that minimizes the following error</p>
<center>
$Error = \frac{1}{n} \sum \limits_{i=1}^n ||x_i-x_i'||^2 = \frac{1}{n}  \sum \limits_{i=1}^n ||x_i-c_iw||^2 = \frac{1}{n}  \sum \limits_{i=1}^n ||x_i-(x_i^Tw)w||^2 $
</center>

<p>So, in the above equation, the only way to reduce the error is to find the right $w$ (note that we dropped the suffix $i$ as we are just finding a single vector $w$ for simplicity). That is, the error is a function of $w$ not of $x$. We can rewrite the error function as follows</p>
<center>
$g(w) = \frac{1}{n} \sum \limits_{i=1}^n -(x_i^Tw)^2 = -\frac{1}{n} w^T \big[\sum \limits_{i=1}^n (x_ix_i^T)\big] w = -\frac{1}{n} w^TCw $
</center>

<p>Here $C$ is a <strong>covariance</strong> matrix (Assuming $x_i$s are from some distribution) of size $d \times d$. You may <a href="https://www.youtube.com/watch?v=mU6CzvuUM00">watch this video</a> for the derivation. So, we need to find the $\mathbf{w}$ that minimizes the error. An optimization problem!</p>

<p>We can get rid of the negative sign by posing this as a maximization problem. That is, maximize $\frac{1}{n} w^TCw$. It is important to note that it maximizes the <strong>variance</strong>, not the error. There are $d$ eigenvectors for the covariance matrix $C$, of these, it is the <strong>eigenvector</strong> corresponding to the <strong>maximum eigenvalue</strong> that maximizes the quantity $\frac{1}{n} w^TCw$.</p>

<h2 id="algorithm">Algorithm</h2>
<ol>
  <li>A dataset $X = {x_1,x_2,\cdots,x_n} \quad x_i \in \mathbb{R}^d $</li>
  <li>Compute the mean vector $\mu = \frac{1}{n} \sum \limits_{i=1}^{n}x_i$, $\mu \in \mathbb{R}^d$</li>
  <li>Subtract the mean from each data point. It centers the data points about the origin as we assume the existence of vector space to find $w_i$. The vector space necessarily includes the origin.</li>
  <li>Project the data points onto the $w_i$ for all $i$.</li>
</ol>

<h2 id="principal-components">Principal Components</h2>
<p>The eigenvectors $(w_i)$ of the covariance matrix $C$ are called the principal directions and the eigenvalues $(c_i)$ corresponding to the eigenvectors convey the amount of variance along that direction. The component of the data point $x_i$ projected on each principal direction is called the <strong>Principal Component</strong>.</p>
<center>
  $x_i = c_1w_1+c_2w_2+\cdots+c_dw_d, \quad c_i=x_i^Tw_i$
  </center>
<p>So we can say that any data point can be decomposed into its principal components (called Analysis) and synthesized back using the sum of principal components (called Synthesis).</p>

<p>Recall that, suppose we have a thousand data points each of size 100. To store all the data points, assuming one byte of memory to store an element, we need $1000 \times 100 \times 1 = 100 KB$ of memory. However, if we use the projected version of all the data points, then we need to store only $c_i, i=1 \cdots 1000$ and the eigenvector $w \in \mathbb{R}^{100}$, which requires $1000 \times 1 + 100 = 1 KB$ of memory. That is a 100 times reduction in memory (of course, at the cost of losing more information).</p>

<p>What if we consider the second-largest eigenvector? It adds new information from that direction at the cost of doubling the memory requirement from 1KB to 2KB. If we use all the eigenvectors, then we can exactly reconstruct the data points without losing any information (the error becomes zero). If the data points live in a lower dimensional subspace, then the error becomes zero with a few eigenvectors. That’s what exactly PCA does.</p>

<p>The application of PCA is numerous. It is used to remove the noise from the data points, de-correlate the correlated features, feature extraction and so on.</p>

<h2 id="just-a-baby-step">Just A baby step</h2>
<p>Principal Component Analysis (PCA) is just a baby step toward representation learning. The projection is on to the subspace spanned by the principal components which are lines and planes. Moreover, we expect the data points to have a high variance representation in the lower dimensional subspace for PCA to be helpful. It may not always be the case. What happens to the variance of principal components if the data points are distributed on a unit circle in $\mathbb{R}^2$? Then all the points are linearly <strong>independent</strong>. We need to use another technique called Independent Component Analysis (ICA). We may apply some kernel tricks on the data points (Kernel PCA), or even neural network-based approaches like AutoEncoders!</p>]]></content><author><name>Arun Prakash A</name></author><category term="ML" /><summary type="html"><![CDATA[Motivation Let’s start with a simple question. How do you represent a number on a real line?. That’s straightforward. How do you represent a 2D point $\begin{bmatrix}x \ y \end{bmatrix}$ in a coordinate system?. Well, we use two real lines that are orthogonal to each other. Move $x$ unit on the axis and $y$ unit on the $y$ axis, then the location denotes the point. For latter convenience, we call $x$ and $y$ as the coefficients of the standard (unit) basis vectors $\mathbf{u_x}$ and $\mathbf{u_y}$. Suppose you are given 5 points $x_1,x_2,\cdots,x_5$ as shown in the app below. Could you represent at least four of them with a single number(coefficient) per point?. Hmm, ya, there is a pattern! Four points are lying on the line (check the “show line” checkbox to see the line) and only one point is not on the line. In other words, the four points are linearly dependent. That is, the four points are lying on the lower dimensional subspace $\mathbb{R}$ of the original vector space $\mathbb{R}^2$. It means that we can reach the four points by scaling the unit vector $\mathbf{w}$ by an appropriate constant $c$. In linear algebra terminology, we can say the vector $\mathbf{w}$ is a new basis vector. Here, we call $\mathbf{w}$ as the representation for these four points. Once we have the $\mathbf{w}$ and coefficients $c_1,c_2,c_3$ and $c_4$, we can represent all these points in the new basis. We are representing the original data points of dim=2, $x_i \in \mathbb{R}^2$, as points in dim=1, $c_i \in \mathbb{R}$. So, we may call this process as dimensionality reduction! Note however that the dim of basis vector $\mathbf{w}$ is still the same as dim of the original points $(\mathbb{R}^2)$. In general, the dimension of the representation( aka: eigen, principal, basis) vectors is always equal to the dim of the original points. However, how do we represent the point $x_5$ using $c_5\mathbf{w}$? It is not possible as the point is not on the line. Suppose we relax the requirement that we need to exactly represent the point. Instead, we want to represent the point as close as possible. For that, we need to first project the point onto the line (subspace). The red arrow in the applet denotes the projected vector $\mathbb{p}$ and the point $x_5’$ is the projection of the point $x_5$ onto the line. The dotted segment represents the error between the projected point and the actual point. Activity: Check the show Circle check box in the app. Then move the point $x_5$ on the circle and observe the change in error value as you move along the circle. When does the error become high (that is, error=1)? If we have more points $x_6,x_7,\cdots$, then we project all those points onto the subspace spanned by $\mathbf{w}$ (the line) to calculate the appropriate coefficient values, $c_i$. Note that the vector $\mathbf{w}$ is a unit vector. Therefore, to represent any point $x_i$, the scaling factor $c_i$ is given by $c_i = \mathbf{x_i}^T\mathbf{w}$ and the project vector is $\mathbb{p_i}=c_i\mathbb{w}$ Principled Approach Well, in the above setup, we assumed we are given with $\mathbf{w}$ (or we could have calculated it, not a big deal). Suppose we receive a few more points as shown in the figure below, Now, many points are outside the given line. That is, if we sum the error for each data point, then the average error might be high. The error is the function of the line we choose (that is, $\mathbb{w}$). However, there could be a better line ($\mathbb{w}$) that minimizes the average error. How do we find it? In general, we have $n$ data points and each data point $x_i \in \mathbb{R}^d$. We are interested in finding a set of $\mathbf{w_i} \in \mathbb{R}^d$ and the scalars $c_i \in \mathbb{R}$ such that it minimizes the average error as much as possible. Again, in linear algebra terminology, we are looking for new bases that span the low-dimensional subspace. By default, the data points are represented in $\mathbb{R}^d$ with $d$ canonical (orthogonal) bases (coordinates). Our objective is to come up with $d’ \leq d$ number of bases (coordinates) that minimizes the following error $Error = \frac{1}{n} \sum \limits_{i=1}^n ||x_i-x_i'||^2 = \frac{1}{n} \sum \limits_{i=1}^n ||x_i-c_iw||^2 = \frac{1}{n} \sum \limits_{i=1}^n ||x_i-(x_i^Tw)w||^2 $ So, in the above equation, the only way to reduce the error is to find the right $w$ (note that we dropped the suffix $i$ as we are just finding a single vector $w$ for simplicity). That is, the error is a function of $w$ not of $x$. We can rewrite the error function as follows $g(w) = \frac{1}{n} \sum \limits_{i=1}^n -(x_i^Tw)^2 = -\frac{1}{n} w^T \big[\sum \limits_{i=1}^n (x_ix_i^T)\big] w = -\frac{1}{n} w^TCw $ Here $C$ is a covariance matrix (Assuming $x_i$s are from some distribution) of size $d \times d$. You may watch this video for the derivation. So, we need to find the $\mathbf{w}$ that minimizes the error. An optimization problem! We can get rid of the negative sign by posing this as a maximization problem. That is, maximize $\frac{1}{n} w^TCw$. It is important to note that it maximizes the variance, not the error. There are $d$ eigenvectors for the covariance matrix $C$, of these, it is the eigenvector corresponding to the maximum eigenvalue that maximizes the quantity $\frac{1}{n} w^TCw$. Algorithm A dataset $X = {x_1,x_2,\cdots,x_n} \quad x_i \in \mathbb{R}^d $ Compute the mean vector $\mu = \frac{1}{n} \sum \limits_{i=1}^{n}x_i$, $\mu \in \mathbb{R}^d$ Subtract the mean from each data point. It centers the data points about the origin as we assume the existence of vector space to find $w_i$. The vector space necessarily includes the origin. Project the data points onto the $w_i$ for all $i$. Principal Components The eigenvectors $(w_i)$ of the covariance matrix $C$ are called the principal directions and the eigenvalues $(c_i)$ corresponding to the eigenvectors convey the amount of variance along that direction. The component of the data point $x_i$ projected on each principal direction is called the Principal Component. $x_i = c_1w_1+c_2w_2+\cdots+c_dw_d, \quad c_i=x_i^Tw_i$ So we can say that any data point can be decomposed into its principal components (called Analysis) and synthesized back using the sum of principal components (called Synthesis). Recall that, suppose we have a thousand data points each of size 100. To store all the data points, assuming one byte of memory to store an element, we need $1000 \times 100 \times 1 = 100 KB$ of memory. However, if we use the projected version of all the data points, then we need to store only $c_i, i=1 \cdots 1000$ and the eigenvector $w \in \mathbb{R}^{100}$, which requires $1000 \times 1 + 100 = 1 KB$ of memory. That is a 100 times reduction in memory (of course, at the cost of losing more information). What if we consider the second-largest eigenvector? It adds new information from that direction at the cost of doubling the memory requirement from 1KB to 2KB. If we use all the eigenvectors, then we can exactly reconstruct the data points without losing any information (the error becomes zero). If the data points live in a lower dimensional subspace, then the error becomes zero with a few eigenvectors. That’s what exactly PCA does. The application of PCA is numerous. It is used to remove the noise from the data points, de-correlate the correlated features, feature extraction and so on. Just A baby step Principal Component Analysis (PCA) is just a baby step toward representation learning. The projection is on to the subspace spanned by the principal components which are lines and planes. Moreover, we expect the data points to have a high variance representation in the lower dimensional subspace for PCA to be helpful. It may not always be the case. What happens to the variance of principal components if the data points are distributed on a unit circle in $\mathbb{R}^2$? Then all the points are linearly independent. We need to use another technique called Independent Component Analysis (ICA). We may apply some kernel tricks on the data points (Kernel PCA), or even neural network-based approaches like AutoEncoders!]]></summary></entry></feed>